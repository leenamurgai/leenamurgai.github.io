<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Dr Leena Murgai">
  <meta name="dcterms.date" content="2022-06-28">
  <meta property="og:url" content="https://mitigatingbias.ml">
  <meta property="og:type" content="book">
  <meta property="og:title" content="Mitigating Bias in Machine Learning">
  <meta property="og:description" content="Mitigating Bias in Machine Learning discusses how practicing model developers might build fairer predictive systems, and avoid causing harm. Part I offers context (philosophical, legal, technical) and practical solutions. Part II discusses how we quantify different notions of fairness, where possible making connections with ideologies from other disciplines (discussed in part I). Part III analyses methods for mitigating bias, looking at the impact on the various metrics (discussed in part II).">
  <meta property="og:book:author" content="Leena Murgai">
  <meta property="og:image" content="https://raw.githubusercontent.com/leenamurgai/leenamurgai.github.io/main/profile/figures/SocialPreviewLandscape.png">
  <meta property="og:image:type" content="image/png">
  <meta property="og:image:width" content="1280">
  <meta property="og:image:height" content="640">
  <title>Mitigating Bias in Machine Learning</title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="tex2html/tufte/tufte.css">
  <link rel="stylesheet" href="tex2html/css/pandoc.css">
  <link rel="stylesheet" href="tex2html/css/navbar.css">
  <link rel="stylesheet" href="tex2html/css/tweak.css">
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<article>
<header>
<h1 class="title">Mitigating Bias in Machine Learning</h1>
<p class="byline">Dr Leena Murgai</p>
<p class="byline">28 June 2022</p>
</header>
<div class="TOC">
<nav id="TOC">
    <div class="shortthickbar"></div>
    <div class="shortthickbar"></div>
    <div class="shortthickbar"></div>
<ul>
<li><a href="#ch_Notation">Notation and conventions</a></li>
<li><a href="#part-i-introduction">Part I Introduction</a></li>
<li><a href="#ch_Background"><span class="toc-section-number">1</span> Context</a>
<ul>
<li><a href="#bias-in-machine-learning"><span class="toc-section-number">1.1</span> Bias in Machine Learning</a></li>
<li><a href="#sec_FairnessJustice"><span class="toc-section-number">1.2</span> A Philosophical Perspective</a></li>
<li><a href="#a-legal-perspective"><span class="toc-section-number">1.3</span> A Legal Perspective</a></li>
<li><a href="#sec_SimpsParadox"><span class="toc-section-number">1.4</span> A Technical Perspective</a></li>
<li><a href="#sec_harms"><span class="toc-section-number">1.5</span> What’s the Harm?</a></li>
<li><a href="#summary">Summary</a></li>
</ul></li>
<li><a href="#ch_EthicalDev"><span class="toc-section-number">2</span> Ethical development</a>
<ul>
<li><a href="#machine-learning-cycle"><span class="toc-section-number">2.1</span> Machine Learning Cycle</a></li>
<li><a href="#sec_ResponseDev"><span class="toc-section-number">2.2</span> Model development and deployment life cycle</a></li>
<li><a href="#sec_ProcessPolicy"><span class="toc-section-number">2.3</span> Responsible model development and deployment</a></li>
<li><a href="#common-causes-of-harm"><span class="toc-section-number">2.4</span> Common causes of harm</a></li>
<li><a href="#linking-common-causes-of-harm-to-the-workflow"><span class="toc-section-number">2.5</span> Linking common causes of harm to the workflow</a></li>
<li><a href="#summary-1">Summary</a></li>
</ul></li>
<li><a href="#part-ii-measuring-bias">Part II Measuring Bias</a></li>
<li><a href="#ch_GroupFairness"><span class="toc-section-number">3</span> Group Fairness</a>
<ul>
<li><a href="#sec_BalOut"><span class="toc-section-number">3.1</span> Comparing outcomes</a></li>
<li><a href="#sec_BalErr"><span class="toc-section-number">3.2</span> Comparing errors</a></li>
<li><a href="#sec_Impossible"><span class="toc-section-number">3.3</span> Incompatibility between fairness criteria</a></li>
<li><a href="#summary-2">Summary</a></li>
</ul></li>
<li><a href="#app_AIF360"><span class="toc-section-number">A</span> AIF360</a>
<ul>
<li><a href="#app_Install"><span class="toc-section-number">A.1</span> Installing AIF360</a></li>
<li><a href="#app_AIF360_GF"><span class="toc-section-number">A.2</span> Group fairness in AIF360</a></li>
</ul></li>
<li><a href="#app_Metrics"><span class="toc-section-number">B</span> Performance Metrics</a></li>
<li><a href="#app_ProbRules"><span class="toc-section-number">C</span> Rules of Probability</a></li>
<li><a href="#app_Solutions"><span class="toc-section-number">D</span> Solutions to Exercises</a>
<ul>
<li><a href="#sec_app_GFSolutions"><span class="toc-section-number">D.1</span> Chapter <span>3</span>: Group Fairness</a></li>
</ul></li>
<li><a href="#bibliography">References</a></li>
</ul>
</nav>
</div>
<div id="collapsiblemenu">
  <button class="collapsible">
    <div class="shortthickbar"></div>
    <div class="shortthickbar"></div>
    <div class="shortthickbar"></div>
  </button>
  <div class="content">
    <ul>
    <li><a href="#ch_Notation">Notation and conventions</a></li>
    <li><a href="#part-i-introduction">Part I Introduction</a></li>
    <li><a href="#ch_Background"><span class="toc-section-number">1</span> Context</a>
    <ul>
    <li><a href="#bias-in-machine-learning"><span class="toc-section-number">1.1</span> Bias in Machine Learning</a></li>
    <li><a href="#sec_FairnessJustice"><span class="toc-section-number">1.2</span> A Philosophical Perspective</a></li>
    <li><a href="#a-legal-perspective"><span class="toc-section-number">1.3</span> A Legal Perspective</a></li>
    <li><a href="#sec_SimpsParadox"><span class="toc-section-number">1.4</span> A Technical Perspective</a></li>
    <li><a href="#sec_harms"><span class="toc-section-number">1.5</span> What’s the Harm?</a></li>
    <li><a href="#summary">Summary</a></li>
    </ul></li>
    <li><a href="#ch_EthicalDev"><span class="toc-section-number">2</span> Ethical development</a>
    <ul>
    <li><a href="#machine-learning-cycle"><span class="toc-section-number">2.1</span> Machine Learning Cycle</a></li>
    <li><a href="#sec_ResponseDev"><span class="toc-section-number">2.2</span> Model development and deployment life cycle</a></li>
    <li><a href="#sec_ProcessPolicy"><span class="toc-section-number">2.3</span> Responsible model development and deployment</a></li>
    <li><a href="#common-causes-of-harm"><span class="toc-section-number">2.4</span> Common causes of harm</a></li>
    <li><a href="#linking-common-causes-of-harm-to-the-workflow"><span class="toc-section-number">2.5</span> Linking common causes of harm to the workflow</a></li>
    <li><a href="#summary-1">Summary</a></li>
    </ul></li>
    <li><a href="#part-ii-measuring-bias">Part II Measuring Bias</a></li>
    <li><a href="#ch_GroupFairness"><span class="toc-section-number">3</span> Group Fairness</a>
    <ul>
    <li><a href="#sec_BalOut"><span class="toc-section-number">3.1</span> Comparing outcomes</a></li>
    <li><a href="#sec_BalErr"><span class="toc-section-number">3.2</span> Comparing errors</a></li>
    <li><a href="#sec_Impossible"><span class="toc-section-number">3.3</span> Incompatibility between fairness criteria</a></li>
    <li><a href="#summary-2">Summary</a></li>
    </ul></li>
    <li><a href="#app_AIF360"><span class="toc-section-number">A</span> AIF360</a>
    <ul>
    <li><a href="#app_Install"><span class="toc-section-number">A.1</span> Installing AIF360</a></li>
    <li><a href="#app_AIF360_GF"><span class="toc-section-number">A.2</span> Group fairness in AIF360</a></li>
    </ul></li>
    <li><a href="#app_Metrics"><span class="toc-section-number">B</span> Performance Metrics</a></li>
    <li><a href="#app_ProbRules"><span class="toc-section-number">C</span> Rules of Probability</a></li>
    <li><a href="#app_Solutions"><span class="toc-section-number">D</span> Solutions to Exercises</a>
    <ul>
    <li><a href="#sec_app_GFSolutions"><span class="toc-section-number">D.1</span> Chapter <span>3</span>: Group Fairness</a></li>
    </ul></li>
    <li><a href="#bibliography">References</a></li>
    </ul>
  </div>
</div>
<section id="ch_Notation" class="level1 unnumbered">
<h1 class="unnumbered">Notation and conventions</h1>
<section id="mathematical-notation" class="level3 unnumbered">
<h3 class="unnumbered">Mathematical notation</h3>
<ul>
<li><p><span class="math inline">\(\mathbb{P}(A)\)</span> denotes probability of event <span class="math inline">\(A\)</span></p></li>
<li><p><span class="math inline">\(\mathbb{E}\)</span> denotes expectation</p></li>
<li><p><span class="math inline">\(\forall\)</span> means for all</p></li>
<li><p><span class="math inline">\(|\)</span> means such that</p></li>
<li><p><span class="math inline">\(\in\)</span> means a member of</p></li>
<li><p><span class="math inline">\(\Rightarrow\)</span> means implies</p></li>
<li><p><span class="math inline">\(\Leftrightarrow\)</span> means if and only if</p></li>
<li><p>square brackets are inclusive, round brackets are not, for example, <span class="math display">\[x \in [a, b) \quad \Leftrightarrow \quad a \leq x &lt; b\]</span></p></li>
<li><p><span class="math inline">\(\rightarrow\)</span> means tends to. We use a superscript <span class="math inline">\(+\)</span> or <span class="math inline">\(-\)</span> to indicate if the limiting value is approached from above or below respectively,</p>
<ul>
<li><p><span class="math inline">\(y(x)\rightarrow0^+\)</span> as <span class="math inline">\(x\rightarrow\infty\,\Rightarrow\,y(x)\)</span> tends to 0 from above as <span class="math inline">\(x\)</span> tends to infinity.</p></li>
<li><p><span class="math inline">\(y(x)\rightarrow0^-\)</span> as <span class="math inline">\(x\rightarrow\infty\,\Rightarrow\,y(x)\)</span> tends to 0 from below as <span class="math inline">\(x\)</span> tends to infinity.</p></li>
</ul></li>
</ul>
</section>
<section id="typographical-conventions" class="level3 unnumbered">
<h3 class="unnumbered">Typographical conventions</h3>
<p>In this book we mostly follow mathematical typographical conventions for variables. All variables are in italic. We use:</p>
<ul>
<li><p>lowercase letters for scalar variables, e.g. <span class="math inline">\(a\)</span>,</p></li>
<li><p>uppercase letters for random variables, e.g. <span class="math inline">\(X\)</span>.</p></li>
<li><p>lowercase bold typeface letters for (column) vectors, e.g. <span class="math inline">\(\boldsymbol{y}\)</span>,</p></li>
<li><p>uppercase bold typeface letters for matrices and vectors of random variables, e.g. <span class="math inline">\(\boldsymbol{X}\)</span>.</p></li>
</ul>
<p>Notice we have overloaded our notation slightly for matrices and random variables. If it is not clear from the context which we mean, it will be stated explicitly.</p>
<p>We may also use tensor notation where convenient. For the elements of a matrix <span class="math inline">\(\boldsymbol{X}\)</span> we use <span class="math inline">\(x_{ij}\)</span> where <span class="math inline">\(i\)</span> refers to the row and <span class="math inline">\(j\)</span> to the column. Similarly for the elements of a vector <span class="math inline">\(\boldsymbol{y}\)</span>, we use <span class="math inline">\(y_i\)</span> where <span class="math inline">\(i\)</span> refers to the row. For the transpose we use T in the superscript so that <span class="math inline">\(\boldsymbol{y}^T\)</span> would be a row vector. We also use <span class="math inline">\(\boldsymbol{x}_i\)</span> to denote the <span class="math inline">\(i\)</span>th row of the matrix <span class="math inline">\(\boldsymbol{X}\)</span>.</p>
</section>
<section id="data" class="level3 unnumbered">
<h3 class="unnumbered">Data</h3>
<p>We will use <span class="math inline">\(\boldsymbol{X}\)</span> to denote the (non-sensitive) feature matrix and <span class="math inline">\(\boldsymbol{Z}\)</span> to denote the sensitive feature matrix (features like gender and race for example). We use <span class="math inline">\(\boldsymbol{y}\)</span> to denote the target variable vector (a column vector with <span class="math inline">\(n\)</span> elements, each corresponding to an sample) and <span class="math inline">\(\boldsymbol{\hat{y}}(\boldsymbol{X},\boldsymbol{Z})\)</span> to denote the predicted target variable output by our model, which is a function of the features. We shall use <span class="math inline">\(\mathcal{X}\)</span> and <span class="math inline">\(\mathcal{Z}\)</span> to denote the set of possible values our feature vectors <span class="math inline">\(\boldsymbol{x}\)</span> and <span class="math inline">\(\boldsymbol{z}\)</span> can take respectively and <span class="math inline">\(\mathcal{Y}\)</span> to denote the set of all possible values our outcome <span class="math inline">\(y\)</span> can take. When more appropriate we will use set notation to denote our set of data points which we write as <span class="math inline">\((\boldsymbol{X}, \boldsymbol{Z}, Y)=\{\boldsymbol{x}_i, \boldsymbol{z}_i, y_i\}_{i=1}^n\)</span></p>
<p>We shall use the same notation for our target and model output for both discrete (classification) and continuous (regression) variables. In the case where the target variable is discrete and derived from a continuous classifier (that is, one where we find the classification by applying a threshold to an underlying score), we denote the underlying score as <span class="math inline">\(\boldsymbol{p}(\boldsymbol{X},\boldsymbol{Z})\)</span> (if <span class="math inline">\(y\)</span> is a binary variable) in which case we can write, <span class="math display">\[\hat{y}_i(\boldsymbol{X},\boldsymbol{Z})
= H\left( p_i(\boldsymbol{X},\boldsymbol{Z}) - \tau \right) \quad
  \forall \,\, i\]</span> where <span class="math inline">\(H(x)\)</span> is the Heaviside step function: <span class="math display">\[H(x) = \left\{
\begin{array}{rl}
1 &amp; \textrm{if} \quad x &gt; 0 \\
0 &amp; \textrm{otherwise}
\end{array}
\right.\]</span> and <span class="math inline">\(\tau\)</span> is the threshold.</p>
<p>Note that if there was a single sensitive feature (rather than multiple) we would use <span class="math inline">\(\boldsymbol{z}\)</span> (rather than <span class="math inline">\(\boldsymbol{Z}\)</span>) to denote it (since it would be a vector) and if the target was multi-class rather binary we would use <span class="math inline">\(\boldsymbol{P}\)</span> (rather than <span class="math inline">\(\boldsymbol{p}\)</span>) for the score since we would need a score for each class. If we have <span class="math inline">\(n\)</span> examples, <span class="math inline">\(m_x\)</span> non-sensitive features, and <span class="math inline">\(m_z\)</span> sensitive features then, <span class="math inline">\(\boldsymbol{X}\)</span> is an <span class="math inline">\(n \times m_x\)</span> matrix, <span class="math inline">\(\boldsymbol{Z}\)</span> is an <span class="math inline">\(n \times m_z\)</span> and <span class="math inline">\(\boldsymbol{y}\)</span> and <span class="math inline">\(\boldsymbol{p}\)</span> are vectors with <span class="math inline">\(n\)</span> elements. If <span class="math inline">\(\boldsymbol{y}\)</span> was a multi-class target variable with <span class="math inline">\(c\)</span> possible classes, then <span class="math inline">\(\boldsymbol{P}\)</span> would be an <span class="math inline">\(n \times c\)</span> matrix.</p>
<p>For binary sensitive and target variables <span class="math inline">\(\boldsymbol{z}\)</span> and <span class="math inline">\(\boldsymbol{y}\)</span>, we will set the advantaged group and advantageous target class to have the value one, the disadvantaged group and disadvantageous target class will then take the value zero.</p>
</section>
<section id="random-variables" class="level3 unnumbered">
<h3 class="unnumbered">Random variables</h3>
<p>Following the typographical conventions described above, we use <span class="math inline">\(\boldsymbol{X}\)</span>, <span class="math inline">\(\boldsymbol{Z}\)</span> (or <span class="math inline">\(Z\)</span> for a single sensitive feature), <span class="math inline">\(Y\)</span>, <span class="math inline">\(\hat{Y}\)</span> and <span class="math inline">\(P\)</span> (or <span class="math inline">\(\boldsymbol{P}\)</span> for multi-class), to denote the the random variables corresponding to our non-sensitive features, sensitive features, target variable, model predicted target variable and model probability function respectively.</p>
<section id="special-values" class="level4 unnumbered">
<h4 class="unnumbered">Special values</h4>
<p>We will occasionally use <span class="math inline">\(+\)</span> or <span class="math inline">\(-\)</span> in the subscript of a binary variable to respectively denote the advantaged or disadvantaged outcome (or class). For example,</p>
<ul>
<li><p><span class="math inline">\(Y = y_+\)</span> is the advantageous outcome</p></li>
<li><p><span class="math inline">\(Y = y_-\)</span> is the disadvantageous outcome</p></li>
<li><p><span class="math inline">\(Z = z_+\)</span> is the advantaged (privileged) class</p></li>
<li><p><span class="math inline">\(Z = z_-\)</span> is the disadvantaged (unprivileged) class</p></li>
</ul>
<p>For brevity and readability, we shall (on occasion) omit the random variable in the event descriptor of a probability term (if it is obvious which random variable we are referring to). For example, for a binary target variable we might write, <span class="math display">\[\mathbb{P}(Y=y_+) = \mathbb{P}(y_+).\]</span></p>
</section>
</section>
<section id="probability-density-functions" class="level3 unnumbered">
<h3 class="unnumbered">Probability density functions</h3>
<p>As a shorthand we will use <span class="math inline">\(f_X\)</span> to denote the probability density function for the random variable <span class="math inline">\(X\)</span>. Note then that for a discrete random variable <span class="math inline">\(X\)</span>, we can write, <span class="math display">\[\mathbb{P}(X=x) = f_{X}(x),\]</span> while for a continuous random variable we have <span class="math display">\[\mathbb{P}(a&lt;X&lt;b) = \int_a^b f_X(x) \, \mathrm{d}x.\]</span></p>
</section>
<section id="expectations" class="level3 unnumbered">
<h3 class="unnumbered">Expectations</h3>
<p>We denote the expectation as, <span class="math display">\[\mathbb{E}[g(X)] = \sum_{x\in\mathcal{X}} g(x)f_X(x) = \int_{x\in\mathcal{X}} g(x)f_X(x) \, \mathrm{d}x\]</span> where we take the expectation of a multivariate function, we will use a subscript to indicate the variable the expectation is taken over, e.g. <span class="math inline">\(\mathbb{E}_X[g(X,Y)]\)</span>.</p>
</section>
<section id="naming-conventions" class="level3 unnumbered">
<h3 class="unnumbered">Naming conventions</h3>
<ul>
<li><p><span class="math inline">\(n\)</span> for number of examples or data points</p></li>
<li><p><span class="math inline">\(d\)</span> for differences</p></li>
<li><p><span class="math inline">\(r\)</span> for rates</p></li>
</ul>
</section>
</section>
<section id="part-i-introduction" class="level1 unnumbered">
<h1 class="unnumbered">Part I Introduction</h1>
<p>Welcome to Mitigating Bias in Machine Learning. If you’ve made it here chances are you’ve worked with models and have some awareness of the problem of biased machine learning algorithms. You might be a student with a foundational course in machine learning under your belt, or a Data Scientist or Machine Learning Engineer, concerned about the impact your models might have on the world.</p>
<p>In this book we are going to learn and analyse a whole host of techniques for measuring and mitigating bias in machine learning models. We’re going to compare them, in order to understand their strengths and weaknesses. Mathematics is an important part of modelling, and we won’t shy away from it. Where possible, we will aim to take a mathematically rigorous approach to answering questions.</p>
<p>Mathematics, just like code, can contain bugs. In this book, each has been used to verify the other. The analysis in this book, was completed using Python. The Jupyter Notebooks are available on GitHub, for those who would like to see/use them. That said, this book is intended to be self contained, and does not contain code. We will focuses on the concepts, rather than the implementation.</p>
<p>Mitigating Bias in Machine Learning is ultimately about fairness. The goal of this book is to understand how we, as practicing model developers, might build fairer predictive systems and avoid causing harm (sometimes that might mean not building something at all). There are many facets to solving a problem like this, not all of them involve equations and code. The first two chapters (part I) are dedicated to discussing these.</p>
<p>In a sense, over the course of the book, we will zoom in on the problem. In chapter 1, we’ll look at it from a broader perspective (philosophical, political, legal, technical and social). In chapter two we take a more practical view on the problem of ethical development (how to build and organise the development of models, with a view to reducing ethical risk).</p>
<p>In part II we will talk about how we quantify different notions of fairness.</p>
<p>In part III, we will look at methods for mitigating bias through model interventions and anlyse their impact.</p>
<p>Let’s get started.</p>
</section>
<section id="ch_Background" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Context</h1>
<div class="chapsumm">
<p><strong>This chapter at a glance</strong></p>
<ul>
<li><p>Problems with machine learning in socipolitical domains</p></li>
<li><p>Contrasting socio-political theories of fairness in decision systems</p></li>
<li><p>The history, application and interpretation of anti-discrimination law</p></li>
<li><p>Association paradoxes and the difficulty in identifying bias</p></li>
<li><p>The different types of harm caused by biased systems</p></li>
</ul>
</div>
<p>The goal of this chapter is to shed light on the problem of bias in in machine learning, from a variety of different perspectives, in an effort to provide context. The word <em>bias</em> can mean many things but in this book, we use it interchangably with the term <em>unfairness</em>. We’ll talk about why later.</p>
<p>Perhaps the biggest challenge in developing <em>sociotechnical systems</em> is that they inevitably involve questions which are social, philosophical, political, and legal in nature; questions to which there is often no definitive answer but rather competing viewpoints and trade-offs to be made. As we’ll see, this does not change when we attempt to quantify the problem. There are many measures of fairness that each defined in a different way, and they cannot all be satisfied simultaneously. The problem of bias in sociotechnical systems is very much an interdisciplinary one and, in this chapter, we discuss them as such. We will make connections between concepts and language from the various subjects over the course of this book.</p>
<p>In this chapter we shall discuss some philosophical theories of fairness in sociopolitical systems and consider how they might relate to model training and fairness criteria. We’ll take a legal perspective, looking at anti-discrimination laws in the US as an example. We’ll discuss the history, practical application of them and tensions that exist in their interpretation. Data can be misleading. Correlation does not imply causation and so domain knowledge in building machine learning systems is important. We will discuss the technical difficulty in identifying bias in static data through illustrative examples of Simpson’s paradox. Finally, why is it important to consider the fairness of our models? What happens if we don’t? We’ll finish the chapter by discussing some of the different types of harm caused by biased machine learning systems - some of which are difficult if not impossible to quantify.</p>
<p>Let’s start by describing the types of problems we are interested in.</p>
<section id="bias-in-machine-learning" class="level2" data-number="1.1">
<h2 data-number="1.1"><span class="header-section-number">1.1</span> Bias in Machine Learning</h2>
<p>Machine learning can be described as the study of computer algorithms that improve with (or learn) experience. It can be broadly subdivided into the fields of supervised, unsupervised and reinforcement learning.</p>
<section id="supervised-learning" class="level5 unnumbered">
<h5 class="unnumbered">Supervised learning</h5>
<p>For supervised learning problems, the experience come in the form of labelled training data. Given a set of features <span class="math inline">\(X\)</span> and labels (or targets) <span class="math inline">\(Y\)</span>, we want to learn a function or mapping <span class="math inline">\(f\)</span>, such that <span class="math inline">\(Y = f(X)\)</span>, where <span class="math inline">\(f\)</span> generalizes to previously unseen data.</p>
</section>
<section id="unsupervised-learning" class="level5 unnumbered">
<h5 class="unnumbered">Unsupervised learning</h5>
<p>For unsupervised learning problems there are no labels <span class="math inline">\(Y\)</span>, only features <span class="math inline">\(X\)</span>. Instead we are interested in looking for patterns and structure in the data. For example, we might want to subdivide the data into clusters of points with similar (previously unknown) characteristics or we might want to reduce the dimensionality of the data (to be able to visualize it or simply to make a supervised learning algorithm more efficient). In other words, we are looking for a new feature <span class="math inline">\(Y\)</span> and the mapping <span class="math inline">\(f\)</span> from <span class="math inline">\(X\)</span> to <span class="math inline">\(Y\)</span>.</p>
</section>
<section id="reinforcement-learning" class="level5 unnumbered">
<h5 class="unnumbered">Reinforcement learning</h5>
<p>Reinforcement learning is concerned with the problem of optimally navigating a state space to reach a goal state. The problem is framed as an agent that takes actions, which result in rewards (or penalties). The task is then to maximize the cumulative reward. As with unsupervised learning, the agent is not given a set of examples of optimal actions in various states, but rather must learn them through trial and error. A key aspect of reinforcement learning is the existence of a trade-off between exploration (searching unexplored territory in the hope of finding a better choice) and exploitation (exploiting what has been learned so far).</p>
<p>In this we will focus on the first two categories (essentially algorithms that capture and or exploit patterns in data), primarily because these are the fields in which problems related to bias in machine learning are most pertinent (automation and prediction). As one would expect then, these are also the areas in which many of the technical developments in measuring and mitigating bias have been concentrated.</p>
<p>The idea that the kinds of technologies described above are <em>learning</em> is an interesting one. The analogy is clear, learning by example is certainly a way to learn. In less modern disciplines one might simply think of <em>training</em> a model as; solving an equation, interpolating data, or optimising model parameters. So where does the terminology come from? The term <em>machine learning</em> was coined by Arthur Samuel in the 1950’s when, at IBM, he developed an algorithm capable of playing draughts (checkers). By the mid 70’s his algorithm was competitive at amateur level. Though it was not called reinforcement learning at the time, the algorithm was one of the earliest implementations of such ideas. Samuel used the term <em>rote learning</em> to describe a memorisation technique he implemented where the machine remembered all the states it had visited and the corresponding reward function, in order to extend the search tree.</p>
</section>
<section id="what-is-a-model" class="level3" data-number="1.1.1">
<h3 data-number="1.1.1"><span class="header-section-number">1.1.1</span> What is a Model?</h3>
<p>Underlying every machine learning algorithm is a model (often several of them) and these have been around for millennia. Based on the discovery of palaeolithic tally sticks (animal bones carved with notches) it’s believed that humans have kept numerical records for over 40,000 years. The earliest mathematical models (from around 4,000 BC) were geometric and used to advance the fields of astronomy and architecture. By 2,000 BC, mathematical models were being used in an algorithmic manner to solve specific problems by at least three civilizations (Babylon, Egypt and India).</p>
<p>A model is a simplified representation of some real world phenomena. It is an expression of the relationship between things; a function or mapping which, given a set of input variables (features), returns a decision or prediction (target). A model can be determined with the help of data, but it need not be. It can simply express an opinion as to how things should be related.</p>
<p>If we have a model which represents a theoretical understanding of the world (under a series of simplifying assumptions) we can test it by measuring and comparing the results to reality. Based on the results we can assess how accurate our understanding of the world was and update our model accordingly. In this way, making simplifying assumptions can be a means to iteratively improve our understanding of the world. Models play an incredibly important role in the pursuit of knowledge. They have provided a mechanism to understand the world around us, and explain why things behave as they do; to prove that the earth could not be flat, explain why the stars move and shift in brightness as they do or, (somewhat) more recently in the case of my PhD, explain why supersonic flows behave uncharacteristically, when a shock wave encounters a vortex.</p>
<p>As the use of models has been adopted by industry, increasingly their purpose has been geared towards prediction and automation, as a way to monetize that knowledge. But the pursuit of profit inevitably creates conflicts of interests. If your goal is to learn more, finding out where your theory is wrong and fixing it is a core part of the game. In business, where the goal is to maximise profit and minimise cost, it need not be.</p>
<p>I recall a joke I heard at school describing how one could tell which field of science an experiment belonged to. If it changes colour, it’s biology; if it explodes, it’s chemistry and if it doesn’t work, it’s physics. Models of real world phenomena fail. They are, by their very nature, a reductive representation of an infinitely more complex real world system. Obtaining adequately rich and relevant data is a major limitation of machine learning models and yet they are increasingly being applied to problems where that kind of data simply doesn’t exist.</p>
</section>
<section id="sociotechnical-systems" class="level3" data-number="1.1.2">
<h3 data-number="1.1.2"><span class="header-section-number">1.1.2</span> Sociotechnical systems</h3>
<p>We use the term sociotechnical systems to describe systems that involve algorithms that manage people. They make decisions for us, determine what we see, direct us and more. But managing large numbers of people inevitably exerts a level of authority and control. For many people that includes deciding the hours they work. Recent years have seen the adoption of just-in-time scheduling algorithms by large retailers to manage staffing needs. To predict footfall, the algorithms take into account everything from weather patterns to sports events. The cost of this efficiency is passed onto employees. The number of hours allocated are optimised to fall short of qualifying for costly health insurance. Employees are subjected to haphazard schedules with little notice that prevent them from being able to prioritise anything other than work and eliminating the possibility of any opportunity that might enable them to advance beyond the low-wage work pool.</p>
<p>Progress in the field of deep learning combined with increased availability and decreased cost of computational resources has led to an explosion in data and model use. Automation seemingly offers a path to making our lives easier, improving the efficiency and efficacy of the many industries we transact with day to day; but there are growing and legitimate concerns over how the benefit (and cost) of these efficiencies are distributed. Machine learning is already being used to automate decisions in just about every aspect of modern life; deciding which adverts to show to whom, deciding which transactions might be fraud when we shop, deciding who is able to access to financial services such as loans and credit cards, determining our treatment when sick, filtering candidates for education and employment opportunities, in detesrmining which neighbourhoods to police and even in the criminal justice system to decide what level bail should be set at, or the length of a given sentence. At almost every major life event, going to university, getting a job, buying a house, getting sick, decisions are being made by machines.</p>
</section>
<section id="what-kind-of-bias" class="level3" data-number="1.1.3">
<h3 data-number="1.1.3"><span class="header-section-number">1.1.3</span> What Kind of Bias?</h3>
<p><em>Bias</em> can have numerous different meanings depending on the context even within the same subject. Let’s talk about the kinds of biases that are relevant here. The word bias is used to describe systematic errors in variable estimation (predictions) from data. If the goal is to create systems that work similarly well for all types of people, we certainly want to avoid these. In a social context, bias is spoken of as prejudice or discrimination based on characteristics that we as a society deem to be unacceptable or unfair. Systemic discrimination exists and it shows up in the data in numerous different ways, in historical decisions, who is in it (and who is not). Bias need not be conscious, in reality it starts at the very inception of technology, in deciding which problems are worth solving in the first place. Bias exists in how we measure the cost and benefit of new technologies. For sociotechnical systems, these are all deeply intertwined.</p>
<p>Ultimately, mitigating bias in our models is about fairness and in this book we shall use the terms interchangeably. Machine learning models are capable of not only of proliferating existing societal biases, but amplifying them and are easily deployed at scale. But how do we even define fairness? And from whose perspective do we mean fair? The law can provide <em>some</em> context here. Laws, in many cases, define <em>protected</em> characteristics and domains (we’ll talk more about these later). We can potentially use these as a guide and we certainly have a responsibility to be law abiding citizens. A common approach historically has been to ignore protected characteristics. There’s a few reasons for this. One reason is the false belief that, an algorithm cannot discriminate based on features not included in the data. This assumption is is easy to disprove with a counter example. A reasonably fool-proof way to systematically discriminate by race or rather ethnicity (without explicitly using it), is to discriminate by location/residence; that is, another variable that’s strongly correlated and serves as a proxy. The legality of this practice depends on the domain. In truth, you don’t need a feature, or a proxy, to discriminate based on it, you just need enough data, to be able to predict it. If it is predictable, the information there and the algorithm is likely using it. Another reason for ignoring protected features is avoiding legal liability (we’ll talk more about this when we take a legal perspective later in the chapter).</p>
<section id="example-amazon-prime-same-day-delivery-service" class="level4 unnumbered">
<h4 class="unnumbered">Example: Amazon Prime same day delivery service</h4>
<p>In 2016, analysis published by Bloomberg uncovered racial disparities in eligibility for Amazon’s same day delivery services for Prime customers<span class="sidenote-wrapper"><label for="sn-0" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-0" class="margin-toggle"/><span class="sidenote">To be clear, the same day delivery was free for eligible Amazon Prime customers on sales exceeding $35. Amazon Prime members pay a fixed annual subscription fee, thus the disparity is in the level of service provided for Prime customers who are eligible verses those that are not.<br />
<br />
</span></span><span class="citation" data-cites="AmazonSameDayPrime"><a href="#ref-AmazonSameDayPrime" role="doc-biblioref">[1]</a></span><span class="marginnote"><span id="ref-AmazonSameDayPrime" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[1] </span><span class="csl-right-inline">D. Ingold and S. Soper, <span>“Amazon doesn’t consider the race of its customers. Should it?”</span> <em>Bloomberg</em>, 2016.</span>
</span>
</span>. The study used census data to identify Black and White residents and plot the data points on city maps which simultaneously showed the areas that qualified for the Prime customer same day delivery. The disparities are glaring at a glance. In six major cities, New York, Boston, Atlanta, Chicago, Dallas, and Washington, DC where the service did not have broad coverage, it was mainly Black neighbourhoods that were ineligible. In the latter four cities, Black residents were about half as likely to live in neighbourhoods eligible for Amazon same-day delivery as White residents.</p>
<p>At the time Amazon’s process in determining which ZIP codes to serve was reportedly a cost benefit calculation that did not explicitly take race into account but for those who have seen redlining maps from the 1930’s is hard to not see the resemblance. Redlining was the (now illegal) practice of declining (or raising prices for) financial products to people based on the neighbourhood where they lived. Because neighbourhoods were racially segregated (a legacy that lives on today), public and private institutions were able to systematically exclude minority populations from the housing market and deny loans for house improvements without explicitly taking race into account. Between 1934 and 1962, the Federal Housing Administration distributed $120 billion in loans. Thanks to redlining, 98% of these went to White families.</p>
<p>Amazon is a private enterprise, and it is legally entitled to make decisions about where to offer services based on how profitable it is. Some might argue they have a right to be able to make those decisions. Amazon is not responsible for the injustices that created such racial disparities, but the reality is that such disparities in access to goods and services perpetuate it. If same-day delivery sounds like a luxury, it’s worth considering the context. The cities affected have a long histories of racial segregation and economic inequality resulting from systemic racism now deemed illegal. They are neighbourhoods which to this day are underserved by brick and mortar retailers, where residents are forced to travel further and pay more for household essentials. Now we are in the midst of a pandemic, where once delivery of household goods used to be a luxury, with so many forced to quarantine, suddenly it’s become far more valuable. What we consider to be a necessity changes over time, it depends on where one lives, their circumstances and more. Finally, consider the scale of Amazon’s operations, in 2016 one third of retail e-commerce spending in the US was with Amazon (that number has since risen to almost 50%).</p>
<div class="lookbox">
<p><strong>Rational Prejudice</strong></p>
<p>What do you think? Is it fair for Amazon to provide different service levels based on where customers live in this way?</p>
</div>
</section>
</section>
</section>
<section id="sec_FairnessJustice" class="level2" data-number="1.2">
<h2 data-number="1.2"><span class="header-section-number">1.2</span> A Philosophical Perspective</h2>
<p>Cathy O’Neil describes models as “opinions embedded in code”. Developing models is not an objective scientific process, it involves making a series subjective choices. One of the most fundamental ways in which we impose our opinion on a machine learning model is in deciding how we measure success. Which model is the <em>right</em> one so to speak? Let’s look at the process of training a model. We start with some parametric representation (a family of models), which you hope is sufficiently in complex to be able to reflect the relationships between the variables in the data. The goal in training is to determine which model (in our chosen family) is <em>best</em>, the <em>best</em> model being the one that minimises the expected error (assuming our data is representive of some objective ground truth).</p>
<p>If all we are interested in doing is understanding how well our model represents the world, this approach might be valid. We don’t concern ourselves with the direction of the error in training. Overestimation is as bad as underestimation, false positives are as bad as false negatives. In this case, understanding errors are a means to refine our model if it does not explain the particular phenomenon we are studying. But for sociotechnical systems, we’re not simply trying to understand the world, we are making decisions off the back of it; decisions which result in a benefit or harm to those subjected to them. The very purpose of codifying a decision policy is often to cheaply deploy it at scale. The more people it processes, the more financial value there is in codifying the decision process. Another, way to look such models instead then, is as a system for distributing benefits (or conversely harms) among a population. In this section we briefly discuss some more philosphical theories relevant to these types of problems. We start with utilitarianism which is perhaps the easiest theory to draw parallels with when it comes to training a model.</p>
<section id="utilitarianism" class="level3" data-number="1.2.1">
<h3 data-number="1.2.1"><span class="header-section-number">1.2.1</span> Utilitarianism</h3>
<p>Utilitarianism provides a framework for moral reasoning in decision making. Under this framework, the correct course of action, when faced with a dilemma, is the one that maximises the benefit for the greatest number of people. The doctrine demands that the benefits to all people are are counted equally. Variations of the theory have evolved over the years. Some differ in their notion of how benefits are understood. Others distinguish between the quality of various kinds of benefit. In a business context, one might consider it as financial benefit (and cost). Although, this in itself depends on one’s perspective. Some doctrines advocate that the impact of the action in isolation should be considered, while others ask what the impact would be if everyone in the population took the same actions.</p>
<p>There are some practical problems with utilitarianism as the sole guiding principle for decision making. How do we measure benefit? How do we navigate the complexities of placing a value on immeasurable and vastly different consequences? What is a life, time, money or particular emotion worth and how do we compare and aggregate them? How can one even be certain of the consequences? Longer term consequences are hard if not impossible to predict. Perhaps the most significant flaw in utilitarianism for moral reasoning, is the omission of justice as a consideration.</p>
<p>Utilitarian reasoning judges actions based solely on consequences, and aggregates them over a population. So, if an action that unjustly harms a minority group happens to be the one that maximises the aggregate benefit over a population, it is nevertheless the correct action to take. Under utilitarianism, theft or infidelity might be morally justified, if those it would harm are none the wiser. Or punishing an innocent person for a crime they did not commit could be justified, if it served to quell unrest among a population. For this reason it is widely accepted that utilitarianism is insufficient as a framework for decision making.</p>
<p>Utilitarianism is a flavour of consequentialism, a branch of ethical theory that holds that consequences are the yard stick against which we must judge the morality of our actions. In contrast deontological ethics judges the morality of actions against a set of rules that define our duties or obligations towards others. Here it is not the consequences of our actions that matter but rather intent.</p>
<p>The conception of utilitarianism is attributed to British philosopher Jeremy Bentham who authored the first major book on the topic <em>An Introduction to the Principles of Morals and Legislation</em> in 1780. In it Bentham argues that, it is the pursuit of pleasure and avoidance of pain alone that motivate individuals to act. Given this he saw utilitarianism as a principle by which to govern. Broadly speaking, the role of government, in his view, was to assign rewards or punishments to actions, in proportion to the happiness or suffering they produced among the governed. At the time, the idea that the well-being of all people should be counted equally, and that that morality of actions should be judged accordingly was revolutionary. Bentham was a progressive in his time, he advocated for women’s rights (to vote, hold office and divorce), decriminalisation of homosexual acts, prison reform and the abolition of slavery and more. He argued many of his beliefs as a simple economic calculation of how much happiness they would produce. Importantly, he didn’t claim that all people were equal, but rather only that their happiness mattered equally.</p>
<p>Times have changed. Over the last century, as civil rights have advanced, the weaknesses of utilitarianism in practice have been exposed time and time again. Utilitarian reasoning has increasingly been seen as hindering social progress, rather than advancing it. For example, utilitarian arguments were used by Whites in apartheid South Africa, who claimed that all South Africans were better-off under White rule, and that a mixed government would lead to social decline as it had in other African nations. Utilitarian reasoning has been used widely by capitalist nations in the form of trickle-down economics. The theory being that the benefits of tax-breaks for the wealthy drive economic growth and ‘trickle-down’ to the rest of the population. But evidence suggests that trickle-down economic policies in more recent decades have done more damage than good, increasing national debt and fueling income inequality. Utilitarian principles have also been tested in the debate over torture, capturing a rather callous conviction, one where the ‘means justify the ends’.</p>
<p>Historian and author, Yuval Noah Harari has eloquently abstracted this problem. He argues that historically, decentralization of power and efficiency have aligned; so much so, that many of us cannot think of democracy as being capable of failing, to more totalitarian regimes. But in this new age, data is power. We can train enormous models, that require vast amounts of data, to process people en masse, organise and sort them. And importantly, one does not have to have a perfect system in order to have an impact because of the scale on which they can be deployed. The question Yuval poses is, <em>might the benefits of centralised data, offer a great enough advantage, to tip the balance of efficiency, in favour of more centralised models of power?</em></p>
</section>
<section id="justice-as-fairness" class="level3" data-number="1.2.2">
<h3 data-number="1.2.2"><span class="header-section-number">1.2.2</span> Justice as Fairness</h3>
<p>In his theory Justice As Fairness<span class="citation" data-cites="JusticeFairness"><a href="#ref-JusticeFairness" role="doc-biblioref">[2]</a></span><span class="marginnote"><span id="ref-JusticeFairness" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[2] </span><span class="csl-right-inline">J. Rawls, <em>Justice as fairness: A restatement</em>. Cambridge, Mass.: Harvard University Press, 2001.</span>
</span>
</span>, John Rawls takes a different approach. He describes an idealised democratic framework, based on liberal principles and explains how unified laws can be applied (in a free society made up of people with disparate world views) to create a stable sociopolitical system. One where citizens would not only freely co-operate, but further advocate. He described a political conception of justice which would:</p>
<ol>
<li><p>grant all citizens a set of basic rights and liberties</p></li>
<li><p>give special priority to the aforementioned rights and liberties over demands to further the general good, e.g. increasing the national wealth</p></li>
<li><p>assure all citizens sufficient means to make use of their freedoms.</p></li>
</ol>
<p>The special priority given to the basic rights and liberties in the political conception of justice contrasts with a utilitarian doctrine. Here constraints are placed on how benefits can be distributed among the population and a strategy for determining some minimum.</p>
<section id="principles-of-justice-as-fairness" class="level4 unnumbered">
<h4 class="unnumbered">Principles of Justice as Fairness</h4>
<ol>
<li><p><strong>Liberty principle:</strong> Each person has the same indefeasible claim to a fully adequate scheme of equal basic liberties, which is compatible with the same scheme of liberties for all;</p></li>
<li><p><strong>Equality principle:</strong> Social and economic inequalities are to satisfy two conditions:</p>
<ol>
<li><p><strong>Fair equality of opportunity:</strong> The offices and positions to which they are attached are open to all, under conditions of fair equality of opportunity;</p></li>
<li><p><strong>Difference (maximin) principle</strong> They must be of the greatest benefit to the least-advantaged members of society.</p></li>
</ol></li>
</ol>
<p>The principles of Justice as Fairness are ordered by priority so that fulfilment of the liberty principle takes precedence over the equality principles and fair equality of opportunity takes precedence over the difference principle.</p>
<p>The first principle grants basic rights and liberties to all citizens which are prioritised above all else and cannot be traded for other societal benefits. It’s worth spending a moment thinking about what those rights and liberties look like. They are the the basic needs that are important for people to be free, to have choices and the means to pursue their aspirations. Today many of what Rawls considered to be basic rights and liberties are allocated algorithmically; education, employment, housing, healthcare, consistent treatment under the law to name a few.</p>
<p>The second principle requires positions to be allocated meritocratically, with all similarly talented (with respect to the skills and competencies required for the position) individuals having the same chance of attaining such positions i.e. that allocation of such positions should be independent of social class or background. We will return to the concept of <em>equality of opportunity</em> in chapter <a href="#ch_GroupFairness" data-reference-type="ref" data-reference="ch_GroupFairness">3</a> when discussing <em>Group Fairness</em>.</p>
<p>The third principle acts to prevent redistribution of social and economic currency from the rich to the poor by requiring that inequalities are of maximal benefit to the least advantaged in a society, also described as the maximin principle. In this principle, Rawls does not take the simplistic view that inequality and fairness are mutually exclusive but rather concisely articulates when the existence of inequality becomes unfair. We shall return to maximin principle when we look at the use of <em>inequality indices</em> to measure algorithmic unfairness in a later chapter.</p>
</section>
</section>
</section>
<section id="a-legal-perspective" class="level2" data-number="1.3">
<h2 data-number="1.3"><span class="header-section-number">1.3</span> A Legal Perspective</h2>
<p>It’s important to remember that anti-discrimination laws are the result of long-standing and systemic discrimination against oppressed people. Their existance is a product of history; subjugation, genocide, civil war, mass displacement of entire communities, racial hierarchies and segregation, supremist policies (exclusive access to publicly funded initiatives), voter suppression and more. The law provides an important historical record of what we as a society deem fair and unfair, but without history there is no context. The law does not define the bechmark for fairness. Laws vary by durisdiction and change over time and in particular they often do not adequately recognise or address issues related to discrimination that are known and accepted by the sciences (social, mathematical, medical,...).</p>
<p>In this section we’ll look at the history, practical application and interpretation of the law in the US (acknowledging the narrow scope of our discussion) Finally, we’ll take a brief look at what might be on the legislative horizon for predictive algorithms, based on more recent global developments.</p>
<section id="a-brief-history-of-anti-discrimination-law-in-the-us" class="level3" data-number="1.3.1">
<h3 data-number="1.3.1"><span class="header-section-number">1.3.1</span> A Brief History of Anti-discrimination Law in the US</h3>
<p>Anti-discrimination laws in the US rest on the 14th amendment to the constitution which grants citizens <em>equal protections of the law</em>. Class action law suit Brown v Board (of Education of Topeka, Kansas) was a landmark case which in 1954, legally ended racial segregation in the US. Justices ruled unanimously that racial segregation of children in public schools was unconstitutional, establishing the precedent that “separate-but-equal” was, in fact, not equal at all. Though Brown v Board did not end segregation in practice, resistance to it in the south fuelled the civil rights movement. In the years that followed the NAACP (National Association for the Advancement of Coloured People) challenged segregation laws. In 1955, Rosa parks refusing to give up her seat on a bus in Montgomery (Alabama) led to sit ins and boycotts, many of them led by Martin Luther King Jr. The resulting Civil rights act of 1964 eventually brought an end to “Jim Crow” laws which barred Blacks from sharing buses, schools and other public facilities with Whites.</p>
<p>After the violent attack by Alabama state troopers on participants of a peaceful march from Selma to Montgomery was televised, The Voting Rights Act of 1965 was passed. It overcame many barriers (including literacy tests), at state and local level, used to prevent Black people from voting. Before this incidents of voting officials asking Black voters to “recite the entire Constitution or explain the most complex provisions of state laws”<span class="citation" data-cites="LBJ"><a href="#ref-LBJ" role="doc-biblioref">[3]</a></span><span class="marginnote"><span id="ref-LBJ" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[3] </span><span class="csl-right-inline">P. L. B. Johnson, <span>“Speech to a joint session of congress on march 15, 1965,”</span> <em>Public Papers of the Presidents of the United States</em>, vol. I, entry 107, pp. 281–287, 1965.</span>
</span>
</span> in the south were common place.</p>
<p>In the years following the second world war, there were many attempts to pass an Equal Pay Act. Initial efforts were led by unions who feared men’s salaries would be undercut by women who were paid less for doing their jobs during the war. By 1960, women made up 37% of the work force but earned on average 59 cents for each dollar earned by men. The Equal Pay Act was eventually passed in 1963 in a bill which endorsed “equal pay for equal work”. Laws for gender equality were strengthened the following year by the Civil Rights Act of 1964.</p>
<p>Throughout the 1800’s the American federal government displaced Native American communities to facilitate White settlement. In 1830 the Indian Removal Act was passed in order to relocate hundreds of thousands of Native Americans. Over the following two decades, thousands of those forced to march hundreds of miles west on the perilous “Trail of Tears” died. By the middle on the century, the term “manifest destiny” was popularlised to describe the belief that White settlement in North America was ordained by God. In 1887, the Dawes Act laid the groundwork for the seizing and redistribution of reservation lands from Native to White Americans. Between 1945 and 1968 the federal government terminated recognition of more than 100 tribal nations placing them under state jurisdiction. Once again Native Americans were relocated, this time from reservations to urban centres.</p>
<p>In addition to displacing people of colour, the federal government also enacted policies that reduced barriers to home ownership almost exclusively for White citizens - subsidizing the development of prosperous "White Caucasian" tenant/owner only suburbs, guaranteeing mortgages and enabling access to job opportunities by building highway systems for White commuters, often through communities of colour, simultaneously devaluing the properties in them. Even government initiatives aimed at helping veterans of World War II to obtain home loans accommodated Jim Crow laws allowing exclusion of Black people. In the wake of the Vietnam war, just days after the assassination of Martin Luther King J, the Fair Housing Act of 1968 was passed, prohibiting discrimination concerning the sale, rental and financing of housing based on race, religion, national origin or sex.</p>
<p>The Civil Rights Act of 1964 acted as a catalyst for many other civil rights movements, including those protecting people with disabilities. The Rehabilitation Act (1973) removed architectural, structural and transportation barriers and set up affirmative action programs. The Individuals with Disabilities Education Act (IDEA 1975) required free, appropriate public education in the least restrictive environment possible for children with disabilities. The Air Carrier Access Act (1988) which prohibited discrimination on the basis of disability in air travel and ensured equal access to air transportation services. The Fair Housing Amendments Act (1988) prohibited discrimination in housing against people with disabilities.</p>
<p>Title IX of the education amendments of 1972 prohibits federally funded educational institutions from discriminating against students or employees based on sex. The law ensured that schools (elementary to university level) that were recipients of federal funding (nearly all schools) provided fair and equal treatment of the sexes in all areas, including athletics. Before this few opportunities existed for female athletes. The National Collegiate Athletic Association (NCAA) offered no athletic scholarships for women and held no championships for women’s teams. Since then the number of female college athletes has grown five fold. The amendment is credited with decreasing dropout rates and increasing the numbers of women gaining college degrees.</p>
<p>The Equal Credit Opportunity Act was passed in 1974 when discrimination against women applying for credit in the US was rife. It was common practice for mortgage lenders to discount incomes of women that were of ’child bearing’ age or simply deny credit to them. Two years later the law was amended to prohibit lending discrimination based on race, color, religion, national origin, age, the receipt of public assistance income, or exercising one’s rights under consumer protection laws.</p>
<p>In 1978, congress passed the Pregnancy Discrimination Act in response to two Supreme Court cases that ruled that excluding pregnancy related disabilities from disability benefit coverage was not gender based discrimination, and did not violate the equal protection clause.</p>
<p>Table <a href="#tbl:RegDom" data-reference-type="ref" data-reference="tbl:RegDom">1.1</a> shows a (far from exhaustive) summary of regulated domains with corresponding US legislation. Note that legislation in these domains extend to marketing and advertising not just the final decision.</p>
<div id="tbl:RegDom">
<table>
<caption>Table 1.1: Regulated domains in the private sector under US federal law.</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Domain</th>
<th style="text-align: left;">Legislation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Finance</td>
<td style="text-align: left;">Equal Credit Opportunity Act</td>
</tr>
<tr class="even">
<td style="text-align: left;" rowspan="3">Education</td>
<td style="text-align: left;">Civil Rights Act (1964)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Education Amendment (1972)</td>
</tr>
<tr class="even">
<td style="text-align: left;">IDEA (1975)</td>
</tr>
<tr class="odd">
<td style="text-align: left;" rowspan="2">Employment</td>
<td style="text-align: left;">Equal Pay Act(1963)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Civil Rights Act (1964)</td>
</tr>
<tr class="odd">
<td style="text-align: left;" rowspan="2">Housing</td>
<td style="text-align: left;">Fair Housing Act (1968)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Fair Housing Amendments Act (1988)</td>
</tr>
<tr class="odd">
<td style="text-align: left;" rowspan="3">Transport</td>
<td style="text-align: left;">Urban Mass Transit Act (1970)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Rehabilitation Act (1973)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Air Carrier Access Act (1988)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Public accommodation<sup>a</sup></td>
<td style="text-align: left;">Civil Rights Act (1964)</td>
</tr>
</tbody>
</table>
</div>
<div class="tablenotes">
<p><sup>a</sup>Prevents refusal of customers.</p>
</div>
<p>Table <a href="#tbl:ProtChar" data-reference-type="ref" data-reference="tbl:ProtChar">1.2</a> provides a list of protected characteristics under US federal law with corresponding legislation (again not exhaustive).</p>
<div id="tbl:ProtChar">
<table>
<caption>Table 1.2: Protected characteristics under US Federal Law.</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Protected Characteristic</th>
<th style="text-align: left;">Legislation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Race</td>
<td style="text-align: left;">Civil Rights Act (1964)</td>
</tr>
<tr class="even">
<td style="text-align: left;" rowspan="3">Sex</td>
<td style="text-align: left;">Equal Pay Act (1963)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Civil Rights Act (1964)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Pregnancy Discrimination Act (1978)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Religion</td>
<td style="text-align: left;">Civil Rights Act (1964)</td>
</tr>
<tr class="even">
<td style="text-align: left;">National Origin</td>
<td style="text-align: left;">Civil Rights Act (1964)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Citizenship</td>
<td style="text-align: left;">Immigration Reform &amp; Control Act</td>
</tr>
<tr class="even">
<td style="text-align: left;">Age</td>
<td style="text-align: left;">Age Discrimination in Employment Act (1967)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Familial status</td>
<td style="text-align: left;">Civil Rights Act (1968)</td>
</tr>
<tr class="even">
<td style="text-align: left;" rowspan="2">Disability status</td>
<td style="text-align: left;">Rehabilitation Act of 1973</td>
</tr>
<tr class="odd">
<td style="text-align: left;">American with Disabilities Act of 1990</td>
</tr>
<tr class="even">
<td style="text-align: left;" rowspan="2">Veteran status</td>
<td style="text-align: left;">Veterans’ Readjustment Assistance Act 1974</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Uniformed Services Employment &amp; Reemployment Rights Act</td>
</tr>
<tr class="even">
<td style="text-align: left;">Genetic Information</td>
<td style="text-align: left;">Civil Rights Act(1964)</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="sec_AppLaw" class="level3" data-number="1.3.2">
<h3 data-number="1.3.2"><span class="header-section-number">1.3.2</span> Application and Interpretation of the Law</h3>
<p>To get an idea of how anti-discrimination laws are be applied in practice and how they might translate to algorithmic decision making, we look at Title VII of the Civil rights act of 1964 in the context of employment discrimination<span class="citation" data-cites="BarocasSelbst"><a href="#ref-BarocasSelbst" role="doc-biblioref">[4]</a></span><span class="marginnote"><span id="ref-BarocasSelbst" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[4] </span><span class="csl-right-inline">S. Barocas and A. D. Selbst, <span>“Big data’s disparate impact,”</span> <em>Calif Law Rev.</em>, vol. 104, pp. 671–732, 2016.</span>
</span>
</span>. Legal liability for discrimination against protected classes can be established as disparate treatment and/or disparate impact. Disparate treatment (also described as direct discrimination in Europe) refers to both differing treatment of individuals based on protected characteristics, and intent to discriminate. Disparate impact (or indirect discrimination in Europe) does not consider intent but addresses policies and practices that disproportionately impact protected classes.</p>
<section id="disparate-treatment" class="level4 unnumbered">
<h4 class="unnumbered">Disparate Treatment</h4>
<p>Disparate treatment effectively prohibits rational prejudice (backed by data showing the protected feature to be correlated) as well as denial of opportunities based on protected characteristics. For an algorithm, it effectively prevents the use of protected characteristics as inputs. It’s noteworthy that in the case of disparate treatment, the actual impact of using the protected features on the outcome is irrelevant; so even if a company could show that the target variable produced by their model had zero correlation with the protected characteristic, the company would still be liable for disparate treatment. This fact is somewhat bizarre given that not using the protected feature in the algorithm provides no guarantee that the algorithm is not biased in relation to it. Indeed an organisation could very well use their data to predict the protected characteristic.</p>
<p>In an effort to avoid disparate treatment liability, many organisations do not even collect data relating to protected characteristics, leaving them unable to accurately measure, let alone address, bias in their algorithms, even if they might want to<span class="sidenote-wrapper"><label for="sn-1" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-1" class="margin-toggle"/><span class="sidenote">In fact, I met a data scientist at a conference, who was working for a financial institution, that said her team was trying to predict sensitive features such as race and gender in order to measure bias in their algorithms.<br />
<br />
</span></span>. In summary, disparate treatment as applied today does not resolve the problem of unconscious discrimination against disadvantaged classes through their use of machine learning algorithms. Further it acts as a deterrent to ethically minded companies that might want to measure the biases in their algorithms.</p>
<div class="lookbox">
<p><strong>Disparate treatment</strong></p>
<p>Suppose a company predicts the sensitive feature and uses this as an input to its model. Should this be considered disparate treatment?</p>
</div>
<p>What about the case where the employer implements an algorithm, finds out that it has a disparate impact, and uses it anyway? Doesn’t that become disparate treatment? No it doesn’t and in fact, somewhat surprisingly, deciding not to apply it upon noting the disparate impact could result in a disparate treatment claim in the opposite direction<span class="citation" data-cites="FireFighters"><a href="#ref-FireFighters" role="doc-biblioref">[5]</a></span><span class="marginnote"><span id="ref-FireFighters" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[5] </span><span class="csl-right-inline"><span>“<span class="nocase">Ricci v. DeStefano, 557 U.S. 557</span>.”</span> 2009.</span>
</span>
</span>. We’ll return to this later. Okay, so what about disparate impact?</p>
</section>
<section id="disparate-impact" class="level4 unnumbered">
<h4 class="unnumbered">Disparate Impact</h4>
<p>In order to establish a violation, it is not enough to simply show that there is a disparate impact, but it must also be shown either that there is no business justification for it, or if there is, that the employer refuses to use another, less discriminatory, means of achieving the desired result. So how much of an impact is enough to warrant a disparate impact claim? There are no rules here only guidelines. The Uniform Guidelines on Employment selection procedures from the Equal Employment Opportunity Commission (EEOC) provides a guideline that if the selection rate from one protected group is less than four fifths of that from another, it will generally be regarded as evidence of adverse impact, though it also states that the threshold would depend on the circumstances.</p>
<p>Assuming the disparate impact is demonstrated, the issue becomes proving business justification. The requirement for business justification has softened in favour of the employer over the years; treated as “business necessity”<span class="citation" data-cites="BusinessNecessity"><a href="#ref-BusinessNecessity" role="doc-biblioref">[6]</a></span><span class="marginnote"><span id="ref-BusinessNecessity" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[6] </span><span class="csl-right-inline"><span>“<span class="nocase">Griggs v. Duke Power Co., 401 U.S. 424</span>.”</span> 1971.</span>
</span>
</span> earlier on and later interpreted as “business justification”<span class="citation" data-cites="BusinessJustification"><a href="#ref-BusinessJustification" role="doc-biblioref">[7]</a></span><span class="marginnote"><span id="ref-BusinessJustification" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[7] </span><span class="csl-right-inline"><span>“<span class="nocase">Wards Cove Packing Co. v. Atonio, 490 U.S. 642</span>.”</span> 1989.</span>
</span>
</span>. Today, it’s generally accepted that business justification lies somewhere between the extremes of “job-relatedness” and “business necessity”. As a concrete example of disparate impact and taking the extreme of job-relatedness - the EEOC along with several federal courts have determined that discrimination on the sole basis of a criminal record to be a violation under disparate impact unless the particular conviction is related to the role, because Non-White applicants are more likely to have a criminal conviction.</p>
<p>For a machine learning algorithm, business justification boils down to the question of job-relatedness of the target variable. If the target variable is improperly chosen, a disparate impact violation can be established. In practice however the courts will accept most plausible explanations of job-relatedness since not accepting it would set a precedent that it is determined discriminatory. Assuming the target variable to be proven job-related then, there is no requirement to validate the model’s ability to predict said trait, only a guideline which sets a low bar (a statistical significance test showing that the target variable correlates with the trait) and which the court is free to ignore.</p>
<p>Assuming business justification is proven by the employer, the final burden then falls on the plaintiff to show that the employer refused to use a less discriminatory “alternative employment practice”. If the less discriminatory alternative would incur additional cost (as is likely) would this be considered refusing? Likely not.</p>
<p>While on the surface, disparate impact might seem like a solution, the current framework of a weak business justification (in terms of a plausible target variable) and the employer refusing an alternative employment practice with no requirement to validate the model offers little resolve. Clearly there is need for reform.</p>
</section>
<section id="anti-classification-versus-anti-subordination" class="level4 unnumbered">
<h4 class="unnumbered">Anti-classification versus Anti-subordination</h4>
<p>Just as the meaning of fairness is subjective so is the interpretation of anti-discrimination laws. At one extreme, anti-classification holds the weaker interpretation, that the law is intended to prevent classification of people based on protected characteristics. At the other extreme, anti-subordination defines the stronger stance, that anti-discrimination laws exist to prevent social hierarchies, class or caste systems based on protected features and, that it should actively work to eliminate them where they exist. An important ideological difference between the two schools of thought is in the application of positive discrimination policies. Under anti-subordination principles, one might advocate for affirmative action as a means to bridge gaps in access to employment, housing, education and other such pursuits, that are a direct result of historical systemic discrimination against particular groups. A strict interpretation of the anti-classification principle would prohibit such actions. Both anti-classification and anti-subordination ideologies have been argued and upheld in landmark cases.</p>
<p>In 2003, the Supreme Court held that a student admissions process that favours “under-represented minority groups” does not violate the Fourteenth Amendment<span class="citation" data-cites="UnderRepStudents"><a href="#ref-UnderRepStudents" role="doc-biblioref">[8]</a></span><span class="marginnote"><span id="ref-UnderRepStudents" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[8] </span><span class="csl-right-inline"><span>“<span class="nocase">Grutter v. Bollinger, 539 U.S. 306</span>.”</span> 2003.</span>
</span>
</span>, provided it evaluated applicants holistically at an individual level. The same year, the New Haven Fire Department administered a two part test in order to fill 15 openings. Examinations were governed in part by the City of New Haven. Under the city charter, civil service positions must be filled by one of the top three scoring individuals. 118 (White, Black and Hispanic) fire fighters took the exams. Of the resulting 19 candidates who scored highest on the tests and could the considered for the positions, none were Black. After heated public debate and under threat of legal action either way, the city threw out the test results. This action was later determined to be a disparate treatment violation. In 2009, the court ruled that disparate treatment could not be used to avoid disparate impact without sufficient evidence of liability of the latter<span class="citation" data-cites="FireFighters"><a href="#ref-FireFighters" role="doc-biblioref">[5]</a></span>. This landmark case was the first example of conflict between the two doctrines of disparate impact and disparate treatment or anti-classification and anti-subordination.</p>
<p>Disparate treatment seems to align well with anti-classification principles, seeking to prevent intentional discrimination based on protected characteristics. In the case of disparate impact, things are less clear. Is it a secondary ‘line of defence’ designed to weed out well masked intentional discrimination? Or is its intention to address inequity that exists as a direct result of historical injustice? One can draw parallels here with the ‘business necessity’ versus ‘business justification’ requirements discussed earlier.</p>
</section>
</section>
<section id="future-legislation" class="level3" data-number="1.3.3">
<h3 data-number="1.3.3"><span class="header-section-number">1.3.3</span> Future Legislation</h3>
<p>In May 2018, the European Union (EU) brought into action the General Data Protection (GDPR) a legal framework around the protection of personal data of EU citizens. The framework is divided into binding and non-binding recitals. The regulation sets provisions for processing of data in relation to decision making, described as ‘profiling’ under recital 71<span class="citation" data-cites="GDPR"><a href="#ref-GDPR" role="doc-biblioref">[9]</a></span><span class="marginnote"><span id="ref-GDPR" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[9] </span><span class="csl-right-inline"><span>“<span>General Data Protection Regulation (GDPR): (EU) 2016/679 Recital 71</span>.”</span> 2016.</span>
</span>
</span>. Though currently non-binding, it provides an indication of what’s to come. The recital talks specifically about having the right not to be subject to decisions based solely on automated processing. It specifically talks about credit applications, e-recruiting and any system which analyses or predicts aspects of a persons performance at work, economic situation, health, personal preferences or interests, reliability or behaviour, location or movements. The recital also talks about requirements around using “appropriate mathematical or statistical procedures” to prevent “discriminatory effects on natural persons on the basis of racial or ethnic origin, political opinion, religion or beliefs, trade union membership, genetic or health status or sexual orientation”. More recently in 2021, the EU has proposed taking a risk based approach to the question of which technologies should be regulated, dividing it into four categories. Unacceptable risk, high risk, limited risk, minimal risk<span class="citation" data-cites="ECPR"><a href="#ref-ECPR" role="doc-biblioref">[10]</a></span><span class="marginnote"><span id="ref-ECPR" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[10] </span><span class="csl-right-inline"><span>“<span class="nocase">Europe fit for the Digital Age: Commission proposes new rules and actions for excellence and trust in Artificial Intelligence</span>.”</span> 2021.</span>
</span>
</span>. While things may change as the proposed law is debated but once agreed, it’s not unlikely that it will serve as a prototype for legislation in the U.S. (and other countries around the world), as did GDPR.</p>
<p>In April 2019, the <a href="https://www.congress.gov/bill/116th-congress/house-bill/2231">Algorithmic Accountability Act</a> was proposed to the US Senate. The bill requires specified commercial entities to conduct impact assessments of automated decision systems and specifically states that assessments must include evaluations and risk assessment in relation to “accuracy, fairness, bias, discrimination, privacy, and security” not just for the model output but for the training data. The bill has cosponsors in 22 states and has been referred to the Committee on Commerce, Science, and Transportation for review. These examples are clear indications that the issues of fairness and bias in automated decision making systems are on the radar of regulators.</p>
</section>
</section>
<section id="sec_SimpsParadox" class="level2" data-number="1.4">
<h2 data-number="1.4"><span class="header-section-number">1.4</span> A Technical Perspective</h2>
<p>The problem of distinguishing correlation from causation is an important one in identifying bias. Using illustrative examples of Simpson’s paradox, we demonstrate the danger of assuming causal relationships based on observational data.</p>
<section id="simpsons-paradox" class="level3" data-number="1.4.1">
<h3 data-number="1.4.1"><span class="header-section-number">1.4.1</span> Simpson’s Paradox</h3>
<p>In 1973, University of California, Berkeley received approximately 15,000 applications for the fall quarter<span class="citation" data-cites="Berkeley"><a href="#ref-Berkeley" role="doc-biblioref">[11]</a></span><span class="marginnote"><span id="ref-Berkeley" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[11] </span><span class="csl-right-inline">P. J. Bickel, E. A. Hammel, and J. W. O’Connell, <span>“Sex bias in graduate admissions: Data from berkeley,”</span> <em>Science</em>, vol. 187, Issue 4175, pp. 398–404, 1975.</span>
</span>
</span>. At the time it was made up of 101 departments. 12,763 applications reached the decision stage. Of these 8442 were male and 4321 were female. The acceptance rates for the applicants were 44% and 35% respectively (see Table <a href="#tbl:BerkAdm1" data-reference-type="ref" data-reference="tbl:BerkAdm1">1.3</a>).</p>
<div id="tbl:BerkAdm1">
<table>
<caption>Table 1.3: Graduate admissions data from Berkeley (fall 1973).</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Gender</th>
<th style="text-align: right;">Admitted</th>
<th style="text-align: right;">Rejected</th>
<th style="text-align: right;">Total</th>
<th style="text-align: right;">Acceptance Rate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Male</td>
<td style="text-align: right;">3738</td>
<td style="text-align: right;">4704</td>
<td style="text-align: right;">8442</td>
<td style="text-align: right;">44.3%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Female</td>
<td style="text-align: right;">1494</td>
<td style="text-align: right;">2827</td>
<td style="text-align: right;">4321</td>
<td style="text-align: right;">34.6%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Aggregate</td>
<td style="text-align: right;">5232</td>
<td style="text-align: right;">7531</td>
<td style="text-align: right;">12763</td>
<td style="text-align: right;">41.0%</td>
</tr>
</tbody>
</table>
</div>
<p>With a whopping 10% difference in acceptance rates, it seems a likely case of discrimination against women. Indeed, a <span class="math inline">\(\chi^2\)</span> hypothesis test for independence between the variables (gender and application acceptance) reveals that the probability of observing such a result or worse, assuming they are independent, is <span class="math inline">\(6\times10^{-26}\)</span>. A strong indication that they are not independent and therefore evidence of bias in favour of male applicants. Since admissions are determined by the individual departments, it’s worth trying to understand which departments might be responsible. We focus on the data for the six largest departments, shown in Table <a href="#tbl:BerkAdm2" data-reference-type="ref" data-reference="tbl:BerkAdm2">1.4</a>. Here again we see a similar pattern. There appears to be bias in favour of male applicants, and a <span class="math inline">\(\chi^2\)</span> test shows that the probability of seeing this result under the assumption of independence is <span class="math inline">\(1\times10^{-21}\)</span>. It looks like we have quickly narrowed down our search.</p>
<div id="tbl:BerkAdm2">
<table>
<caption>Table 1.4: Graduate admissions data from Berkeley (fall 1973) for the six largest departments.</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Gender</th>
<th style="text-align: right;">Admitted</th>
<th style="text-align: right;">Rejected</th>
<th style="text-align: right;">Total</th>
<th style="text-align: right;">Acceptance Rate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Male</td>
<td style="text-align: right;">1198</td>
<td style="text-align: right;">1493</td>
<td style="text-align: right;">2691</td>
<td style="text-align: right;">44.5%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Female</td>
<td style="text-align: right;">557</td>
<td style="text-align: right;">1278</td>
<td style="text-align: right;">1835</td>
<td style="text-align: right;">30.4%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Aggregate</td>
<td style="text-align: right;">1755</td>
<td style="text-align: right;">2771</td>
<td style="text-align: right;">4526</td>
<td style="text-align: right;">38.8%</td>
</tr>
</tbody>
</table>
</div>
<p>Figure <a href="#fig:SimpsParAccByDept" data-reference-type="ref" data-reference="fig:SimpsParAccByDept">1.1</a> shows the acceptance rates for each department by gender, in decreasing order of acceptance rates. Performing <span class="math inline">\(\chi^2\)</span> tests for each department reveals the only department where there is strong evidence of bias is A, but the bias is in favour of female applicants. The probability of observing the data for department A, under the assumption of independence, is <span class="math inline">\(5\times10^{-5}\)</span>.</p>
<figure>
<img src="01_Context/figures/Fig_BerkeleyAccByDept.png" id="fig:SimpsParAccByDept" style="width:85.0%" alt="Figure 1.1: Acceptance rate distributions by department for male and female applicants." /><figcaption aria-hidden="true">Figure 1.1: Acceptance rate distributions by department for male and female applicants.</figcaption>
</figure>
<p>So what’s going on? Figure <a href="#fig:SimpsParAppByDept" data-reference-type="ref" data-reference="fig:SimpsParAppByDept">1.2</a> shows the application distributions for male and female applicants for each of the six departments. From the plots we are able to see a pattern. Female applicants are more often applying for departments with a lower acceptance rate.</p>
<figure>
<img src="01_Context/figures/Fig_BerkeleyAppByDept.png" id="fig:SimpsParAppByDept" style="width:85.0%" alt="Figure 1.2: Application distributions by department for male and female applicants." /><figcaption aria-hidden="true">Figure 1.2: Application distributions by department for male and female applicants.</figcaption>
</figure>
<p>In other words a larger proportion of the women are being filtered out overall, simply because they are applying to departments that are harder to get into.</p>
<p>This is a classic example of Simpson’s Paradox (also known as the reversal paradox and Yule-Simpson effect). We have an observable relationship between two categorical variables (in this case gender and acceptance) which disappears or reverses, after controlling for one or more other variables (in this case department). Simpson’s Paradox is a special case of so called association paradoxes (where the variables are categorical, and the relationship changes qualitatively), but the same rules also apply to continuous variables. The <em>marginal</em> (unconditional) measure of association (e.g. correlation) between two variables need not be bounded by the <em>partial</em> (conditional) measures of association (after controlling for one or more variables). Although Edward Hugh Simpson famously wrote about the paradox in 1951, it was not discovered by him. In fact, it was reported by George Udny Yule as early as 1903. The association paradox for continuous variables was demonstrated by Karl Pearson in 1899.</p>
<p>Let’s discuss another quick example. A 1996 follow-up study on the effects of smoking recorded the mortality rate for the participants over a 20 year period. They found higher mortality rates among the non-smokers, 31.4% compared to 23.9% which, in itself, might imply a considerable protective affect from smoking. Clearly there’s something fishy going on. Disaggregating the data by age group showed that the mortality rates were higher for smokers in all but one of them. Looking at the age distribution of the populations of smokers and non-smokers, it’s apparent that the age distribution of the non-smoking group is more positively skewed, and so they are older on average. This concords with the rationale that non-smokers live longer - hence the difference in age distributions of the participants.</p>
<figure>
<img src="01_Context/figures/Fig_SimpParaReg.png" id="fig:SimpsPara" style="width:98.0%" alt="Figure 1.3: Visualisation of Simpsons Paradox. Wikipedia." /><figcaption aria-hidden="true">Figure 1.3: Visualisation of Simpsons Paradox. <a href="https://en.wikipedia.org/wiki/Simpson%27s_paradox">Wikipedia</a>.</figcaption>
</figure>
</section>
<section id="causality" class="level3" data-number="1.4.2">
<h3 data-number="1.4.2"><span class="header-section-number">1.4.2</span> Causality</h3>
<p>In both the above examples, it appears that the salient information is found in the disaggregated data (we’ll come back to this later). In both cases it is the disaggregated data that enables us to understand the <em>true nature</em> of the relationship between the variables of interest. As we shall see in this section, this need not be the case. To show this, we discuss two examples. In each case, the data is identical but the variables is not. The examples are those Simpson gave in his original 1951 paper<span class="citation" data-cites="Simpson"><a href="#ref-Simpson" role="doc-biblioref">[12]</a></span><span class="marginnote"><span id="ref-Simpson" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[12] </span><span class="csl-right-inline">E. Simpson, <span>“The interpretation of interaction in contingency tables,”</span> <em>Journal of the Royal Statistical Society</em>, vol. Series B, 13, pp. 238–241, 1951.</span>
</span>
</span>.</p>
<p>Suppose we have three binary variables, <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span> and <span class="math inline">\(C\)</span>, and we are interested in understanding the relationship between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> given a set of 52 data points. A summary of the data showing the association between variables <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are shown in Table <a href="#tbl:SimpPara" data-reference-type="ref" data-reference="tbl:SimpPara">1.5</a>, first for all the data points and then stratified (separated) by the value of <span class="math inline">\(C\)</span> (note the first table is the sum of the latter two). The first table indicates that <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are unconditionally independent (since changing the value of one variable does not change the distribution of the other). The next two tables suggest <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are conditionally dependent given <span class="math inline">\(C\)</span>.</p>
<div id="tbl:SimpPara">
<table>
<caption>Table 1.5: Data summary showing the association between variables <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, first for all the data points and then stratified by the value of <span class="math inline">\(C\)</span>.</caption>
<thead>
<tr class="header">
<th style="text-align: center;" colspan="5"></th>
<th style="text-align: center;" colspan="4"><span style="color: SteelBlue">Stained?</span> / <span style="color: FireBrick">Male?</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;" colspan="5"></td>
<td style="text-align: center;" colspan="2"><span class="math inline">\(C=1\)</span></td>
<td style="text-align: center;" colspan="2"><span class="math inline">\(C=0\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;" rowspan="2"><span style="color: SteelBlue">Black?</span>/ <span style="color: FireBrick">Died?</span></td>
<td style="text-align: center;" colspan="2"><span style="color: SteelBlue">Plain?</span>/ <span style="color: FireBrick">Treated?</span></td>
<td style="text-align: center;" rowspan="5"></td>
<td style="text-align: center;" rowspan="2"><span style="color: SteelBlue">Black?</span>/ <span style="color: FireBrick">Died?</span></td>
<td style="text-align: center;" colspan="4"><span style="color: SteelBlue">Plain?</span>/ <span style="color: FireBrick">Treated?</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(A=1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(A=0\)</span></td>
<td style="text-align: center;"><span class="math inline">\(A=1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(A=0\)</span></td>
<td style="text-align: center;"><span class="math inline">\(A=1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(A=0\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(B=1\)</span></td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;"><span class="math inline">\(B=1\)</span></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">3</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(B=0\)</span></td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;"><span class="math inline">\(B=0\)</span></td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(\mathbb{P}(B|A)\)</span></td>
<td style="text-align: center;">50%</td>
<td style="text-align: center;">50%</td>
<td style="text-align: center;"><span class="math inline">\(\mathbb{P}(B|A,C)\)</span></td>
<td style="text-align: center;">38%</td>
<td style="text-align: center;">43%</td>
<td style="text-align: center;">56%</td>
<td style="text-align: center;">60%</td>
</tr>
</tbody>
</table>
</div>
<div class="tablenotes">
<p><sup>a</sup>Each cell of the table shows the number of examples in the dataset satisfying the conditions given in the corresponding row and column headers.</p>
</div>
<section id="question" class="level5 unnumbered">
<h5 class="unnumbered">Question:</h5>
<p>Which distribution gives us the most relevant understanding of the association between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, the marginal (i.e. unconditional) <span class="math inline">\(\mathbb{P}(A,B)\)</span> or conditional distribution <span class="math inline">\(\mathbb{P}(A,B|C)\)</span>? To show that causal relationships matter, we consider two different examples.</p>
</section>
<section id="example-a-pack-of-cards-colliding-variable" class="level4 unnumbered">
<h4 class="unnumbered"><span style="color: SteelBlue">Example a) Pack of Cards (Colliding Variable)</span></h4>
<p>Suppose the population is a pack of cards. It so happens that baby Milen has been messing about with the cards and made some dirty in the process. Let’s summarise our variables,</p>
<ul>
<li><p><span class="math inline">\(A\)</span> tells us the character of the card, either plain (<span class="math inline">\(A=1\)</span>) or royal (King, Queen, Jack; <span class="math inline">\(A=0\)</span>).</p></li>
<li><p><span class="math inline">\(B\)</span> tells us the colour of the card, either black (<span class="math inline">\(B=1\)</span>) or red (<span class="math inline">\(B=0\)</span>).</p></li>
<li><p><span class="math inline">\(C\)</span> tells us if the card is dirty (<span class="math inline">\(C=1\)</span>) or clean (<span class="math inline">\(C=0\)</span>).</p></li>
</ul>
<p>In this case, the aggregated data showing <span class="math inline">\(\mathbb{P}(A,B)\)</span> is relevant since the cleanliness of the cards <span class="math inline">\(C\)</span> has no bearing on the association between the character <span class="math inline">\(A\)</span> and colour <span class="math inline">\(B\)</span> of the cards.</p>
</section>
<section id="example-b-treatment-effect-on-mortality-rate-confounding-variable" class="level4 unnumbered">
<h4 class="unnumbered"><span style="color: FireBrick">Example b) Treatment Effect on Mortality Rate (Confounding Variable)</span></h4>
<p>Next, suppose that the data relates to the results of medical trials for a drug on a potentially lethal illness. This time,</p>
<ul>
<li><p><span class="math inline">\(A\)</span> tells us if the subject was treated (<span class="math inline">\(A=1\)</span>) or not (<span class="math inline">\(A=0\)</span>).</p></li>
<li><p><span class="math inline">\(B\)</span> tells us if the subject died (<span class="math inline">\(B=1\)</span>) or recovered (<span class="math inline">\(B=0\)</span>).</p></li>
<li><p><span class="math inline">\(C\)</span> tells us the gender of the subject, either male (<span class="math inline">\(C=1\)</span>) or female (<span class="math inline">\(C=0\)</span>).</p></li>
</ul>
<p>In this case the disaggregated data shows the more relevant association, <span class="math inline">\(\mathbb{P}(A,B|C)\)</span>. From it, we can see that female patients are more likely to die than males overall; 56 and 60% versus 38 and 43%, depending on if they were treated or not. In both cases we see that treatment with the drug <span class="math inline">\(A\)</span> reduces the mortality rate for both male and female participants, and the effect is obscured by aggregating the data over gender <span class="math inline">\(C\)</span>.</p>
</section>
<section id="back-to-causality" class="level4 unnumbered">
<h4 class="unnumbered">Back to Causality</h4>
<p>The key difference between these examples is the causal relationship between the variables rather than the statistical structure of the data. In the first example with the playing cards, the variable <span class="math inline">\(C\)</span> is a <em>colliding</em> variable, in the second example looking at patient mortality, it is a <em>confounding</em> variable. Figure <a href="#fig:CollConfProg" data-reference-type="ref" data-reference="fig:CollConfProg">1.4</a> a) and b) show the causal relationships between the variables in the two cases.</p>
<figure class="fullwidth">
<img src="01_Context/figures/Fig_CollConfProg.png" id="fig:CollConfProg" alt="Figure 1.4: Causal diagrams for A, B and C when C is a colliding, confounding and prognostic variable." /><figcaption aria-hidden="true">Figure 1.4: Causal diagrams for <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span> and <span class="math inline">\(C\)</span> when <span class="math inline">\(C\)</span> is a colliding, confounding and prognostic variable.</figcaption>
</figure>
<p>The causal diagram in Figure <a href="#fig:CollConfProg" data-reference-type="ref" data-reference="fig:CollConfProg">1.4</a> a) shows the variables <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span> and <span class="math inline">\(C\)</span> for the first example. The arrows exist both from card character and colour to cleanliness because apparently, baby Milen had a preference for royal cards over plain and red cards over black. Conditioning on a collider <span class="math inline">\(C\)</span> generates an association (e.g. correlation) between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, even if they are unconditionally independent. This common effect is often observed as <em>selection</em> or <em>representation bias</em>. Representation bias can induce correlation between variables, even where there is none. For decision systems, this can lead to feedback loops that increase the extremity of the representation bias in future data. We’ll come back to this in chapter <a href="#ch_EthicalDev" data-reference-type="ref" data-reference="ch_EthicalDev">2</a>, when we talk about common causes of bias.</p>
<p>The causal diagram in Figure <a href="#fig:CollConfProg" data-reference-type="ref" data-reference="fig:CollConfProg">1.4</a> b) shows the variables <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span> and <span class="math inline">\(C\)</span> for the second example. The arrows exist from <span class="math inline">\(gender\)</span> to treatment because men were less likely to be treated, and from gender to death because men were also less likely to die. The arrow from <span class="math inline">\(A\)</span> to <span class="math inline">\(B\)</span> represents the effect of treatment on mortality which is observable only by conditioning on gender. Note that there are two sources of association in opposite directions between variables <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> (treatment and death); a positive association, because men were less likely to be treated; and a negative association, because male patients are less likely to die. The two effects cancel each other out when the data is aggregated.</p>
<p>We see through the discussion of these two examples, that statistical reasoning is not sufficient to be able to determine which of the distributions (marginal or conditional) are relevant. Note that the above conclusions in relation to colliding and confounding variables does not generalize to complex time varying problems.</p>
<p>Before moving on from causality, we return to the example we discussed at the very start of this section. According to our analysis of the Berkeley admissions data, we concluded that the disaggregated data contained the <em>salient</em> information explaining the disparity in acceptance rates for male and female applicants. The problem is, we have only shown that application rates to be one of many possible <em>causes</em> of the differing acceptance rates (we cannot see outside of our data). In addition, we have not proven <em>gender discrimanation</em>, not to be the cause. What we have evidenced, is the existance of disparities in both acceptance rates and application rates across sex. One problem is that <em>gender discrimanation</em> is not a measurable thing in itself. It’s complicated. It is made up of many components, most of which are not contained in the data. Beliefs, personal preferences, behaviours, actions, and more. A valid question we cannot answer is, <em>why do the application rates differ by sex?</em> How do we know that this is itself, is not a result of gender discrimnation. Perhaps some departments are less welcoming of women than others or, perhaps some are just much more welcoming of men than women? So how would we know if gender discrimination is at play here? We need to ask the right questions to collect the right data.</p>
</section>
</section>
<section id="sec_collapsibility" class="level3" data-number="1.4.3">
<h3 data-number="1.4.3"><span class="header-section-number">1.4.3</span> Collapsibility</h3>
<p>We have demonstrated that correlation does not imply causation in the manifestation of Simpson’s Paradox. But there is second factor that can have an impact; and that is the nature of the measure of association in question.</p>
<section id="example-c-treatment-effect-on-mortality-rate-prognostic-variable" class="level4 unnumbered">
<h4 class="unnumbered"><span style="color: SeaGreen">Example c) Treatment Effect on Mortality Rate (Prognostic Variable)</span></h4>
<p>Suppose that in the study of the efficacy of the treatment (in Example 2 above), we remedy the selection bias so that male and female patients are equally likely to be treated. We remove the causal relationship between variables <span class="math inline">\(A\)</span> and <span class="math inline">\(C\)</span> (treatment and gender). In this case, the variable <span class="math inline">\(C\)</span> becomes <em>prognostic</em> rather than confounding. See Figure <a href="#fig:CollConfProg" data-reference-type="ref" data-reference="fig:CollConfProg">1.4</a> c). In this case the decision as to which distributions (marginal or conditional) are most relevant would depend only on the target population in question. In the absence of the confounding variable in our study one might reasonably expect the marginal measure of association to be bounded by the partial measures of association. Such intuition is correct only if the measure of association is <em>collapsible</em> (that is, it can be expressed as the weighted average of the partial measures), not otherwise. Some examples of collapsible measures of association are the risk ratio and risk difference. The odds ratio however is not collapsible. If you don’t know what these are, don’t worry, we’ll return to them in chapter <a href="#ch_GroupFairness" data-reference-type="ref" data-reference="ch_GroupFairness">3</a>.</p>
</section>
</section>
</section>
<section id="sec_harms" class="level2" data-number="1.5">
<h2 data-number="1.5"><span class="header-section-number">1.5</span> What’s the Harm?</h2>
<p>In this section we discuss the recent and broader societal concerns related to machine learning technologies.</p>
<section id="the-illusion-of-objectivity" class="level3" data-number="1.5.1">
<h3 data-number="1.5.1"><span class="header-section-number">1.5.1</span> The Illusion of Objectivity</h3>
<p>One of the most concerning things about the machine learning revolution, is perception that these algorithms are somehow objective (unlike humans), and are therefore a better substitute for human judgement. This viewpoint is not just a belief of laymen but an idea that is also projected from within the machine learning community. There are often financial incentives to exaggerate the efficacy of such systems.</p>
<section id="automation-bias" class="level4 unnumbered">
<h4 class="unnumbered">Automation Bias</h4>
<p>The tendency for people to favour decisions made by automated systems despite contradictory information from non-automated sources, or <em>automation bias</em>, is a growing problem as we integrate more and more machines in our decision making processes especially in infrastructure - healthcare, transportation, communication, power plants and more.</p>
<p>It is important to be clear that in general, machine learning systems are not objective. Data is produced by a necessarily subjective set of decisions (how and who to sample, how to group events or characteristics, which features to collect). Modelling also involves making choices about how to process the data, what class of model to use and perhaps most importantly how sucess is determined. Finally, even if our model is calibrated to the data well, it says nothing about the distribution of errors across the population. The consistency of algorithms in decision making compared to humans (who individually make decisions on a case by case basis) is often described as a benefit<span class="sidenote-wrapper"><label for="sn-2" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-2" class="margin-toggle"/><span class="sidenote">One must not confuse consistency with objectivity. For algorithms, consistency also means consistently making the same errors.<br />
<br />
</span></span>, but it’s their very consistency that makes them dangerous - capable of discriminating systematically and at scale.</p>
<section id="example-compas" class="level5 unnumbered">
<h5 class="unnumbered">Example: COMPAS</h5>
<p>(Correctional Offender Management Profiling for Alternative Sanctions) is a “case management system for criminal justice practitioners”. The system, produces recidivism risk scores. It has been used in New York, California and Florida, but most extensively in Wisconsin since 2012, at a variety of stages in the criminal justice, from sentencing to parole. The <a href="https://assets.documentcloud.org/documents/2840784/Practitioner-s-Guide-to-COMPAS-Core.pdf">documentation</a> for the software describes it as an “objective statistical risk assessment tool”.</p>
<p>In 2013, Paul Zilly was convicted of stealing a push lawnmower and some tools in Barron County, Wisconsin. The prosecutor recommended a year in county jail and follow-up supervision that could help Zilly with “staying on the right path.” His lawyer agreed to a plea deal. But Judge James Babler upon seeing Zilly’s COMPAS risk scores overturned the plea deal that had been agreed on by the prosecution and defense, and imposed two years in state prison and three years of supervision. At an appeals hearing later that year, Babler said “Had I not had the COMPAS, I believe it would likely be that I would have given one year, six months”<span class="citation" data-cites="ProPub1"><a href="#ref-ProPub1" role="doc-biblioref">[13]</a></span><span class="marginnote"><span id="ref-ProPub1" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[13] </span><span class="csl-right-inline">J. Angwin, J. Larson, S. Mattu, and L. Kirchner, <span>“Machine bias,”</span> <em>ProPublica</em>, 2016.</span>
</span>
</span>. In other words the judge believed the risk scoring system to hold more insight that the prosecutor who had personally interacted with the defendant.</p>
</section>
</section>
<section id="the-ethics-of-classification" class="level4 unnumbered">
<h4 class="unnumbered">The Ethics of Classification</h4>
<p>The appeal of classification is clear. It creates a sense of order and understanding. It enables us to formulate problems neatly and solve them. An email is spam or it’s not; an x-ray shows tuberculosis or it doesn’t; a treatment was effective or it wasn’t. It can make finding things more efficient in a library or online. There are lots of useful applications of classification.</p>
<p>We tend to think of taxonomies as objective categorisations, but often they are not. They are snapshots in time, representative of the culture and biases of the creators. The very act of creating a taxonomy, can give life by existance to some individuals, while erasing others. Classifying people inevitably has the effect of reducing them to labels; labels that can result in people being treated as members of a group, rather than individuals; labels that can linger for much longer than they should (something it’s easy to forget when creating them). The Dewey Decimal System for example, was developed in the late 1800’s and widely adopted in the 1930’s to classify books. Until 2015, it categorised homosexuality as a mental derangement.</p>
</section>
<section id="classification-of-people" class="level4 unnumbered">
<h4 class="unnumbered">Classification of People</h4>
<p>From the 1930’s until the second world war, machine classification systems were used by Nazi Germany to process census data in order to identify and locate Jews, determine what property and businesses they owned, find anything of value that could be seized and finally to send them to their deaths in concentration camps. Classification systems have often been entangled with political and social struggle across the world. In Aparteid South Africa, they were been used extensively in many parts of the world to enforce social and racial hierarchies that determined everything from where people could live and work to whom they could marry. In 2019 it was estimated that some half a million Uyghurs (and other minority Muslims) are being held in internment camps in China without charge for the purposes of countering extremism and promoting social integration.</p>
<p>Recent papers on detecting criminality”<span class="citation" data-cites="CriminalFace"><a href="#ref-CriminalFace" role="doc-biblioref">[14]</a></span><span class="marginnote"><span id="ref-CriminalFace" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[14] </span><span class="csl-right-inline">X. Wu and X. Zhang, <span>“Automated inference on criminality using face images.”</span> 2017.Available: <a href="https://arxiv.org/abs/1611.04135">https://arxiv.org/abs/1611.04135</a></span>
</span>
</span> and sexuality<span class="citation" data-cites="SexualityFace"><a href="#ref-SexualityFace" role="doc-biblioref">[15]</a></span><span class="marginnote"><span id="ref-SexualityFace" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[15] </span><span class="csl-right-inline">Y. Wang and M. Kosinski, <span>“Deep neural networks are more accurate than humans at detecting sexual orientation from facial images,”</span> <em>Journal of Personality and Social Psychology</em>, 2018.</span>
</span>
</span> and ethnicity<span class="citation" data-cites="EthnicityFace"><a href="#ref-EthnicityFace" role="doc-biblioref">[16]</a></span><span class="marginnote"><span id="ref-EthnicityFace" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[16] </span><span class="csl-right-inline">C. Wang, Q. Zhang, W. Liu, Y. Liu, and L. Miao, <span>“Facial feature discovery for ethnicity recognition,”</span> <em>Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</em>, 2018.</span>
</span>
</span> from facial images have sparked controversy in the academic community. The latter in particular looks for facial features that identify among others, Chinese Uyghurs. Physiognomy (judging character from the physical features of a persons face) and phrenology (judging a persons level of intelligence from the shape and dimensions of their cranium) have historically been used as pseudo-scientific tools of oppressors, to prove the inferiority races and justify subordination and genocide. it is not without merit then to ask if some technologies should be built at all. Machine gaydar might be a fun application to mess about with friends for some, but in the 70 countries where homosexuality is still illegal (some of which enforce the death penalty) it is something rather different.</p>
</section>
</section>
<section id="personalisation-and-the-filter-bubble" class="level3" data-number="1.5.2">
<h3 data-number="1.5.2"><span class="header-section-number">1.5.2</span> Personalisation and the Filter Bubble</h3>
<p>Many believed the internet would breath new life into democracy. The decreased cost and increased accessibility of information would result in greater decentralization of power and flatter social structures. In this new era, people would be able to connect, share ideas and organise grass roots movements at a such a scale that would enable a step change in the rate of social progress. Some of these ideas have been realised to an extent but the increased ability to create and distribute content and corresponding volume of data has created new problems. The amount of information available to us through the internet is overwhelming. Email, blog posts, Twitter, Facebook, Instagram, Linked In, What’s App, You Tube, Netfix, TikTok and more. Today there are seemingly endless ways and places for us to communicate and share information. This barrage of information has resulted in what has been described as the attention crash. There is simply too much information for us to attend to all of it meaningfully. The mechanisms through which we can acquire new information that demands our attention too have expanded. We carry our smart phones everywhere we go and sleep beside them. There is hardly a waking moment, when we are unplugged and inaccessible. The demands on our attention and focus have never been greater. Media producers themselves have adapted their content in order to accommodate our new shortened attention spans.</p>
<p>With so much information available it’s easy to see the appeal of automatic filtering and curation. And of course, how good would said system really be if it didn’t take into account our personal tastes and preferences? So what’s the problem?! Over the last decade, personalisation has become entrenched in the systems we interact with day to day. Targeted advertising was just the beginning. Now it’s not just the trainers you browsed once that follow you around the web until you buy them, it’s everything. Since 2009, Google has returned personalised results every time someone queries their search engine, so two people who enter the same text don’t get the same result. In 2021 You Tube had more than two billion logged-in monthly users. Three quaters of adults in the US use it (more than facebook and Instagram) and 80% of U.S. parents of children under 11 watch it. It is the second most visited site in the world, after Google with visitors checking on average just under 9 pages, and spending 42 minutes per day there. In 2018, 70% of the videos people watched on You Tube were recommended. Some 40% of Americans under thirty get their news through social networking sites such as twitter and Facebook but this may be happening without you even knowing. Since 2010, it’s not the Washington Post that decides which news story you see in the prime real estate that is the top right hand corner of their home page, it’s Facebook - the same goes for the New York Times. So the kinds of algorithms that once determined what we spent our money on now determine our very perception of the world around us. The only question is, what are they optimising for?</p>
<p>Ignoring, for a moment, the fact that having the power to shape people’s perception of the world, in just a few powerful hands is in itself a problem. A question worth pondering on is what kind of citizens people who only ever see things they ‘like’, or feel the impulse to ’comment’ on (or indeed any other proxy for interest/engagement/attention) would make. As Eli Pariser put it in his book The Filter Bubble, “what one seems to like may not be what one actually wants, let alone what one needs to know to be an informed member of their community or country”. The internet has made the world smaller and with it we’ve seen great benefits. But the idea that, because anyone (regardless of their background) could be our neighbour, people would find common ground has not been realised to the extent people hoped. In some senses personalisation does the exact opposite. It risks us all living in a world full of mirrors, where we only ever hear the voices of people who see the world as we do, being deprived of differing perspectives. Of course we have always lived in our own filter bubble in some respects but the thing that has changed is that now we don’t make the choice and often don’t even know when we are in it. We don’t know when or how decisions are made about what we should see. We are more alone in our bubbles than we have ever been before.</p>
<p>Social capital is created by the interpersonal bonds we build in shared identity, values, trust and reciprocity. It encourages people to collaborate in order to solve common problems for the common good. There are two kinds of social capital, bonding and bridging. Bonding capital is acquired through development of connections in groups that have high levels of similarity in demographics and attitudes - the kind you might build by, say socialising with collegues from work. Bridging capital is created when people from different backgrounds (race, religion, class) connect - something that might happen at a town hall meeting say. The problem with personalisation is that by construction it reduces opportunities to see the world through the eyes of people who don’t necessarily look like us. It reduces bridging capital and that exactly the kind of social capital we need to solve wider problems that extend beyond our own narrow or short term self interests.</p>
</section>
<section id="disinformation" class="level3" data-number="1.5.3">
<h3 data-number="1.5.3"><span class="header-section-number">1.5.3</span> Disinformation</h3>
<p>In June 2016, it was announced that Britain would be leaving the EU. 33.5 million people voted in the referendum of which 51.9% voted to leave. The decision that will impact the UK for, not just a term, but generations to come, rested on less than 2% of voters. Ebbw Vale is a small town in Wales where 62% of the electorate (the largest majority in the country) voted to leave. The town has a history in steel and coal dating back to the late 1700’s. By the 1930’s the Ebbw Vale Steelworks was the largest in Europe by volume. In the 1960’s it employed some 14,500 people. But, towards the end of the 1900’s, after the collapse of the UK steel industry, the town suffered one of the highest unemployment rates in Britain. What was strange about the overwhelming support to leave was that Ebbw Vale was perhaps one of the largest recipients of EU development funding in the UK. A £350m regeneration project funded by the EU replaced the industrial wasteland left behind when the steelworks closed in 2002 with The Works (a housing, retail and office space, wetlands, learning campus and more). A further £33.5 in funding from the European Social Fund paid for a new college and apprenticeships, to help young people learn a trade. An additional £30 million for a new railway line, £80 million for road improvements and shortly before the vote a further £12.2 million for other upgrades and improvements were all from the EU.</p>
<p>When journalist Carole Cadwalladr returned to the small town where she had grown up to report on why residents had voted so overwhelmingly in favour of leaving the EU, she was no less confused. It was clear how much the town had benefited from being part of the EU. The new road, train station, college, leisure centre and enterprise zones (flagged an EU tier 1 area, eligible for the highest level of grant aid in the UK), everywhere she went she saw signs with proudly displayed EU flags saying so. So she wandered around town asking people and was no less perplexed by their answers. Time and time again people complained about immigration and foreigners. They wanted to take back control. But the immigrants were nowhere to be found, because Ebbw Vale had one of the lowest rates of immigration in the country. So how did this happen? How did a town with hundreds of millions of pounds of EU funding vote to leave the EU because of immigrants that didn’t exist? In her emotive TED talk<span class="citation" data-cites="CarCadTED"><a href="#ref-CarCadTED" role="doc-biblioref">[17]</a></span><span class="marginnote"><span id="ref-CarCadTED" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[17] </span><span class="csl-right-inline">C. Cadwalladr, <em>Facebook’s role in <span>Brexit</span> - and the threat to democracy</em>. TED, 2019.</span>
</span>
</span>, Carole shows images of some the adverts on Facebook, people were targeted with as part of the leave campaign (see Figure <a href="#fig:Brexit" data-reference-type="ref" data-reference="fig:Brexit">1.5</a>). They were all centred around a lie - that Turkey was joining the EU.</p>
<figure>
<img src="01_Context/figures/Fig_Brexit.png" id="fig:Brexit" alt="Figure 1.5: Targeted disinformation adverts shown on Facebook[17]." /><figcaption aria-hidden="true">Figure 1.5: Targeted disinformation adverts shown on Facebook<sup><span class="citation" data-cites="CarCadTED"><a href="#ref-CarCadTED" role="doc-biblioref">[17]</a></span></sup>.</figcaption>
</figure>
<p>Most people in the UK saw adverts on buses and billboards with false claims, for example that the National Health Service (NHS) would have an extra £350 million a week, if we left the EU. Although many believed them, those adverts circulated in the open for everyone to see, giving the mainstream media at the opportunity to debunk them. The same cannot be said for the adverts in Figure <a href="#fig:Brexit" data-reference-type="ref" data-reference="fig:Brexit">1.5</a>. They were targeted towards specific individuals, as part of an evolving stream of information displayed in their Facebook ‘news’ feed. The leave campaign paid Cambridge Analytica (a company that had illegally gained access to the data of 87 million Facebook users) to identify individuals that could be manipulated into voting leave. In the UK, spending on elections in the is limited by law as a means to ensure fair elections. After a nine month investigation, the UK’s Electoral Commission confirmed these spending limits had been breached by the leave campaign. There are ongoing criminal investigations into where the funds for the campaign originate (overseas funding of election campaigns is also illegal) but evidence suggests ties with Russia. Brexit was the precursor to the Trump administration winning the US election just a few months later that year. The same people and companies used the same strategies. It’s become clear that current legislation protecting democracy is inadequate. Facebook, was able to profit from politically motivated money without recognizing any responsibility in ensuring the transactions were legal. Five years later, the full extent of the disinformation campaign on Facebook has yet to be understood. Who was shown what and when, how people were targeted, what other lies were told, who paid for the adverts or where the money came from.</p>
<p>Since then deep learning technology has advanced to the point of being able to pose as human in important ways that risk enabling disinformation not just through targeted advertising but machines impersonating humans. GANs can fabricate facial images, videos (deepfakes) and audio. Advancements in language models (Open AIs GPT-2 and more recently GPT-3) are capable of creating lengthy human like prose given just a few prompts. Deep learning now provides all the tools to fabricate human identities and target dissemination of false information at scale. There are growing concerns that in the future, bots will drown out actual human voices. As for the current state of play, it’s difficult to know the exact numbers but in 2017, researchers estimated that between 9 and 15% of all twitter accounts were bots<span class="citation" data-cites="FakeTwitter"><a href="#ref-FakeTwitter" role="doc-biblioref">[18]</a></span><span class="marginnote"><span id="ref-FakeTwitter" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[18] </span><span class="csl-right-inline">O. Varol, E. Ferrara, C. A. Davis, F. Menczer, and A. Flammini, <span>“Online human-bot interactions: Detection, estimation, and characterization.”</span> 2017.Available: <a href="https://arxiv.org/abs/1703.03107">https://arxiv.org/abs/1703.03107</a></span>
</span>
</span>. In 2020 a study by researchers at Carnegie Mellon University reported that 45% of the 200 million tweets they analysed discussing coronavirus came from accounts that behaved like bots<span class="citation" data-cites="FakeCovid"><a href="#ref-FakeCovid" role="doc-biblioref">[19]</a></span><span class="marginnote"><span id="ref-FakeCovid" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[19] </span><span class="csl-right-inline">B. Allyn, <span>“Researchers: Nearly half of accounts tweeting about coronavirus are likely bots,”</span> <em>NPR</em>, May 2020.</span>
</span>
</span>. For Facebook, things are less clear as we must rely on their own reporting. In mid-2019, Facebook estimated that only 5% of its 2.4 billion monthly active users were fake though its reporting raised some questions<span class="citation" data-cites="FakeFB"><a href="#ref-FakeFB" role="doc-biblioref">[20]</a></span><span class="marginnote"><span id="ref-FakeFB" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[20] </span><span class="csl-right-inline">J. Nicas, <span>“Does facebook really know how many fake accounts it has?”</span> <em>The New York Times</em>, 2019.</span>
</span>
</span>.</p>
</section>
<section id="harms-of-representation" class="level3" data-number="1.5.4">
<h3 data-number="1.5.4"><span class="header-section-number">1.5.4</span> Harms of Representation</h3>
<p>The interventions we’ll talk about in most of this book are designed to measure and mitigate harms of allocation in machine learning systems.</p>
<section id="harms-of-allocation" class="level4 unnumbered">
<h4 class="unnumbered">Harms of Allocation</h4>
<p>An allocative harm happens when a system allocates or withholds an opportunity or resource. Systems that approve or deny credit allocate financial resources; systems that decide who should and should not see adverts for high paying jobs allocate employment opportunities and systems that determine who will make a good tenant allocate housing resources. Harms of allocation happen as a result of discrete decisions at a given point in time, the immediate impact of which can be quantified. This makes it possible to challenge the justice and fairness of specific determinations and outcomes.</p>
<p>Increasingly however, machine learning systems are affecting us, not just through allocation, but are shaping our view of the world and society at large by deciding what we do and don’t see. These harms are far more difficult to quantify.</p>
</section>
<section id="harms-of-representation-1" class="level4 unnumbered">
<h4 class="unnumbered">Harms of Representation</h4>
<p>Harms of representation occur when systems enforce the subordination of groups through characterizations that affect the perception of them. In contrast to harms of allocation, harms of representation have long-term effects on attitudes and beliefs. They create identities and labels for humans, societies and their cultures. Harms of representation don’t just affect our perception of each other, they affect how we see ourselves. They are difficult to formalise and in turn difficult to quantify but the effect is real.</p>
<div class="lookbox">
<p><strong>The Surgeon’s Dilemma</strong></p>
<p>A father and his son are involved in a horrific car crash and the man died at the scene. But when the child arrived at the hospital and was rushed into the operating theatre, the surgeon pulled away and said: “I can’t operate on this boy, he’s my son”. How can this be?</p>
</div>
<p>Did you figure it out? How long did it take? There is, of course, no reason why the surgeon couldn’t be the boy’s mother. If it took you a while to figure out, or came to a different conclusion, you’re not alone. More than half the people presented with this riddle do, and that includes women. The point of this riddle is to demonstrate the existence of unconscious bias. Representational harms are insidious. They silently fix ideas in peoples subconscious about what people of a particular gender, nationality, faith, race, occupation and more, are like. They draw boundaries between people and affect our perception of world. Below we describe five different harms of representation:</p>
</section>
<section id="stereotyping" class="level4 unnumbered">
<h4 class="unnumbered">Stereotyping</h4>
<p>Stereotyping occurs through excessively generalised portrayals of groups. In 2016, the Oxford English Dictionary was publicly criticised<span class="citation" data-cites="SexistOED"><a href="#ref-SexistOED" role="doc-biblioref">[21]</a></span><span class="marginnote"><span id="ref-SexistOED" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[21] </span><span class="csl-right-inline">E. O’Toole, <span>“A dictionary entry citing <span>‘rabid feminist’</span> doesn’t just reflect prejudice, it reinforces it,”</span> <em>The Guardian</em>, 2016.</span>
</span>
</span> for employing the phrase “rabid feminist” as a usage example for the word rabid. The dictionary included similarly sexist common usages for other words like shrill, nagging and bossy. But even before this, historical linguists observed that words referring to women undergo pejoration (when the meaning of a word deteriorates over time) far more often than those referring to men<span class="citation" data-cites="Pejoration"><a href="#ref-Pejoration" role="doc-biblioref">[22]</a></span><span class="marginnote"><span id="ref-Pejoration" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[22] </span><span class="csl-right-inline">D. Shariatmadari, <span>“Eight words that reveal the sexism at the heart of the english language,”</span> <em>The Guardian</em>, 2016.</span>
</span>
</span>. Consider words like mistress (once simply the female equivalent of master, now used to describe a woman in an illicit relationship with a married man); madam (once simply the female equivalent of sir, now also used to describe a woman who runs a brothel); hussy (once a neutral term for the head of a household, now used to describe an immoral or ill-behaved woman); and governess (female equivalent of governor, later used to describe a woman responsible for the care of children).</p>
<p>Unsurprisingly then, gender stereotyping is known to be a problem in natural language processing systems. In 2016 Bolukbasi et al. showed that word embeddings exhibited familiar gender biases in relation to occupations<span class="citation" data-cites="WomanHomemaker"><a href="#ref-WomanHomemaker" role="doc-biblioref">[23]</a></span><span class="marginnote"><span id="ref-WomanHomemaker" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[23] </span><span class="csl-right-inline">T. Bolukbasi, K.-W. Chang, J. Zou, V. Saligrama, and A. Kalai, <span>“Man is to computer programmer as woman is to homemaker? Debiasing word embeddings.”</span> 2016.Available: <a href="https://arxiv.org/abs/1607.06520">https://arxiv.org/abs/1607.06520</a></span>
</span>
</span>. By performing arithmetic on word vectors, they were able to uncover relationships such as <span class="math display">\[\overrightarrow{\textrm{man}} - \overrightarrow{\textrm{woman}} \approx \overrightarrow{\textrm{computer programmer}} - \overrightarrow{\textrm{homemaker}}.\]</span></p>
<p>In 2017 Caliskan et al. found that Google Translate contained similar gender biases.<span class="citation" data-cites="BiasSemantics"><a href="#ref-BiasSemantics" role="doc-biblioref">[24]</a></span><span class="marginnote"><span id="ref-BiasSemantics" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[24] </span><span class="csl-right-inline">A. Caliskan, J. J. Bryson, and A. Narayanan, <span>“Semantics derived automatically from language corpora contain human-like biases,”</span> <em>Science</em>, vol. 356, pp. 183–186, 2017.</span>
</span>
</span> In their research they found that “translations to English from many gender-neutral languages such as Finnish, Estonian, Hungarian, Persian, and Turkish led to gender-stereotyped sentences”. So for example when they translated Turkish sentences with genderless pronouns: “O bir doktor. O bir hemişre.”, the resulting English sentences were: “He is a doctor. She is a nurse.” They performed these types of tests for 50 occupations and found that the stereotypical gender association of the word almost perfectly predicted the resulting pronoun in the English translation.</p>
</section>
<section id="recognition" class="level4 unnumbered">
<h4 class="unnumbered">Recognition</h4>
<p>Harms of recognition happen when groups of people are in some senses erased by a system through failure to recognise. In her <a href="https://www.ted.com/talks/joy_buolamwini_how_i_m_fighting_bias_in_algorithms/transcript?language=en">TED Talk</a>, Joy Buolamwini, talks about how as an undergraduate studying computer science she worked on social robots. One of her projects involved creating a robot which could play peek-a-boo, but she found that her robot (which used third party software for facial recognition) could not see her. She was forced to borrow her roommate’s face to complete the project. After her work auditing several popular gender classification packages from IBM, Microsoft and Face++ in the project <a href="http://gendershades.org/overview.html">Gender Shades</a><span class="citation" data-cites="GenderShades"><a href="#ref-GenderShades" role="doc-biblioref">[25]</a></span><span class="marginnote"><span id="ref-GenderShades" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[25] </span><span class="csl-right-inline">J. Buolamwini and T. Gerbru, <em>Gender shades: Intersectional accuracy disparities in commercial gender classification</em>, vol. 81. Proceedings of Machine Learning Research, 2018, pp. 1–15.</span>
</span>
</span> in 2017 and seeing the failure of these technologies on the faces of some of the most recognizable Black women of her time, including Oprah Winfrey, Michelle Obama, and Serena Williams, she was prompted to echo the words of Sojourner Truth in asking “<a href="https://medium.com/@Joy.Buolamwini/when-ai-fails-on-oprah-serena-williams-and-michelle-obama-its-time-to-face-truth-bf7c2c8a4119">Ain’t I a Woman?</a>”. Harms of recognition are failures in seeing humanity in people.</p>
</section>
<section id="denigration" class="level4 unnumbered">
<h4 class="unnumbered">Denigration</h4>
<p>In 2015, much to the horror of many people, it was reported that <a href="https://www.bbc.com/news/technology-33347866">Google Photos had labelled a photo of a Black couple as Gorillas</a>. It’s hard to find the right words to describe just how offensive an error this is. It demonstrated how a machine, carrying out a seemingly benign task of labelling photos, could deliver an attack on a person’s human dignity.</p>
<p>In 2020, an ethical audit of several large computer vision datasets<span class="citation" data-cites="TinyImages"><a href="#ref-TinyImages" role="doc-biblioref">[26]</a></span><span class="marginnote"><span id="ref-TinyImages" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[26] </span><span class="csl-right-inline">V. U. Prabhu and A. Birhane, <span>“Large image datasets: A pyrrhic win for computer vision?”</span> 2020.Available: <a href="https://arxiv.org/abs/2006.16923">https://arxiv.org/abs/2006.16923</a></span>
</span>
</span>, revealed some disturbing results. TinyImages (a dataset of 79 million 32 x 32 pixel colour photos compiled in 2006, by MIT’s Computer Science and Artificial Intelligence Lab for image recognition tasks) contained racist, misogynistic and demeaning labels with corresponding images. Figure <a href="#fig:TinyImages" data-reference-type="ref" data-reference="fig:TinyImages">1.6</a> shows a subset of the data found in TinyImages.</p>
<figure>
<img src="01_Context/figures/Fig_TinyImages.png" id="fig:TinyImages" alt="Figure 1.6: Subset of data in TinyImages exemplifying toxicity in both the images and labels[26]." /><figcaption aria-hidden="true">Figure 1.6: Subset of data in TinyImages exemplifying toxicity in both the images and labels<span class="citation" data-cites="TinyImages"><a href="#ref-TinyImages" role="doc-biblioref">[26]</a></span>.</figcaption>
</figure>
<p>The problem, unfortunately, does not end here. Many of the datasets used to train and benchmark, not just computer vision but natural language processing tasks, are related. Tiny Images was compiled by searching the internet for images associated with words in WordNet (a machine readable, lexical database, organised by meaning, developed at Princeton), which is where TinyImages inherited its labels from. ImageNet (widely considered to be a turning point in computer vision capabilities) is also based on WordNet and, Cifar-10 and Cifar-100 were derived from TinyImages.</p>
<p>Vision and language datasets are enormous. The time, effort and consideration in collecting the data that forms the foundation of these technologies (compared to that which has gone into advancing the models built on them), is questionable to say the least. Furthermore a dataset can have impact beyond the applications trained on it, because datasets often don’t just die, they evolve. This calls into question the technologies that are in use today, capable of creating persistent representations of our world, and trained on datasets so large they are difficult and expensive to audit.</p>
<p>And there’s plenty of evidence to suggest that this is a problem. For example, in 2013, a study found that Google searches were more likely to return personalised advertisements that were suggestive of arrest records for Black names<span class="citation" data-cites="LatanyaSweeney"><a href="#ref-LatanyaSweeney" role="doc-biblioref">[27]</a></span><span class="marginnote"><span id="ref-LatanyaSweeney" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[27] </span><span class="csl-right-inline">L. Sweeney, <span>“Discrimination in online ad delivery,”</span> <em>SSRN</em>, 2013.</span>
</span>
</span> than White<span class="sidenote-wrapper"><label for="sn-3" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-3" class="margin-toggle"/><span class="sidenote">Suggestive of an arrest record in the sense that they claim to have arrest records specifically for the name that you searched, regardless of whether they do in reality have them.<br />
<br />
</span></span> This doesn’t just result in allocative harms for people applying for jobs for example, it’s denigrating. <a href="https://www.vice.com/en_us/article/j5jmj8/google-artificial-intelligence-bias">Google’s Natural Language API for sentiment analysis is also known to have problems</a>. In 2017, it was assigning negative sentiment to sentences such as “I’m a jew” and “I’m a homosexual” and “I’m black”; neutral sentiment to the phrase “white power” and positive sentiment to the sentences “I’m christian” and “I’m sikh”.</p>
</section>
<section id="under-representation" class="level4 unnumbered">
<h4 class="unnumbered">Under-representation</h4>
<p>In 2015, the New York Times reported, that “<a href="https://www.nytimes.com/2015/03/03/upshot/fewer-women-run-big-companies-than-men-named-john.html">Fewer women run big companies than men named John</a>”, despite this Google’s image search still managed to under-represent women in search results for the word “CEO”. Does this really matter? What difference would an alternate set of search results make? A study the same year found that “people rate search results higher when they are consistent with stereotypes for a career, and shifting the representation of gender in image search results can shift people’s perceptions about real-world distributions.”<span class="citation" data-cites="OccupationImageSearch"><a href="#ref-OccupationImageSearch" role="doc-biblioref">[28]</a></span><span class="marginnote"><span id="ref-OccupationImageSearch" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[28] </span><span class="csl-right-inline">M. Kay, C. Matuszek, and S. A. Munson, <span>“Unequal representation and gender stereotypes in image search results for occupations,”</span> <em>ACM</em>, 2015.</span>
</span>
</span>.</p>
</section>
<section id="ex-nomination" class="level4 unnumbered">
<h4 class="unnumbered">Ex-nomination</h4>
<p>Ex-nomination occurs through invisible means and affects people’s views of the norms within societies. It tends to happen through mechanisms which amplify the presence of some groups and suppress the presence of others. The cultures, beliefs, politics of ex-nominated groups over time become the default. The most obvious example is the ex-nomination of Whiteness and White culture in western society, which might sound like a bizarre statement - what is White culture? But such is the effect of ex-nomination, you can’t describe it, because it is just the norm and everything else is not. Richard Dyer in his book White examines the reproduction and preservation of whiteness in visual media over five centuries, from the depiction of the crucifixion to modern day film. It’s perhaps should not come as a surprise then, when facial recognition software can’t see black faces; or when gender recognition software fails more often than not for black women; or that a generative model that improves the resolution of images, converted a pixelated picture of Barack Obama, into a high-resolution image of a white man.</p>
<p>The ex-nomination of White culture is evident in our language too, in terminology like whitelist and white lie. If you look up white in dictionary and or thesaurus and you’ll find words like innocent and pure, light, transparent, immaculate, neutral. Doing the same for the word black on the other hand, returns very different associations, dirty, soiled, evil, wicked, black magic, black arts, black mark, black humour, blacklist and black is often used as a prefix in describing disastrous events. A similar assessment can be made for gender with women being under-represented in image data and feminine versions of words more often undergoing pejoration (when the meaning or status of a word deteriorates over time).</p>
<p>Members of ex-nominated groups experience a kind of privilege that it is easy to be unaware of. It is a power that comes from being the norm. They have advantages that are not earned, outside of their financial standing or effort, that the ‘equivalent’ person outside the ex-nominated group would not. Their hair type, skin tone, accent, food preferences and more are catered to by every store, product, service and system and it cost less to access them; they see themselves represented in the media and are more often represented in a positive light; they are not subject to profiling or stereotypes; they are more likely to be treated as individuals rather than as representative of (or as exceptions to) a group; they are more often humanised - more likely to be be given the benefit of the doubt, treated with compassion and kindness and thus recover from mistakes; they are less likely to be suspected of crimes; more likely to be trusted financially; they have greater access to opportunities, resources and power and are able to climb financial, social and professional ladders faster. The advantages enjoyed by ex-nominated groups accumulate over time and compound over generations.</p>
</section>
</section>
</section>
<section id="summary" class="level2 unnumbered">
<h2 class="unnumbered">Summary</h2>
<section id="machine-learning" class="level3 unnumbered">
<h3 class="unnumbered">Machine learning</h3>
<ul>
<li><p>In this book we will use algorithm and model interchangeably. A model can be determined using data, but it need not be. It can simply express an opinion on the relationship between variables. In practice the implementation is an algorithm either way. More precisely, a model is a function or mapping; given a set of input variables (features) it returns a decision or prediction (target).</p></li>
<li><p>A model is a simplified representation of some real world phenomena. When trained on historic data they best capture the dense part of the distribution. Therefore, when faced with rare or unprecedented events they tend to perform poorly. Obtaining adequately rich and relevant data is a major limitation for most machine learning models.</p></li>
<li><p>At almost every major life event, going to university, getting a job, buying a house, getting sick, decisions are increasingly being made by machines. By construction, these models encode existing societal biases. They not only proliferate but are capable of amplifying them and are easily deployed at scale. Understanding the shortcomings of these models and ensuring such technologies are deployed responsibly are essential if we are to safeguard social progress.</p></li>
</ul>
</section>
<section id="discrimination-bias-fairness-and-ethics" class="level3 unnumbered">
<h3 class="unnumbered">Discrimination, bias, fairness and ethics</h3>
<ul>
<li><p>Building machine learning systems ethically is not about finding the perfect answer every time but rather expanding our perspectives on the technology we develop. It’s looking for the cracks before deploying systems, preventing the foreseeable failures and doing the best we can on the ones we didn’t see coming.</p></li>
<li><p>According to uilitarian doctrine, the correct course of action (when faced with a dilemma) is the one that maximises the benefit for the greatest number of people. The doctrine demands that the benefits to all people are are counted equally.</p></li>
<li><p>The standard, approach to training a model, which is essentially to minimise the aggregate error on the training data, is loosely justified in a utilitarian sense, in that we optimise our decision process which maximises utility (minimises the probability of error) for the greatest number of people (we aim to optimise over the target population, in the sense that we aim to reduce the generalisation error).</p></li>
<li><p>Utilitarianism is a flavour of consequentialism, a branch of ethical theory that holds that consequences are the yardstick against which we must judge the morality of our actions. In contrast deontological ethics judges the morality of actions against a set of rules that define our duties or obligations towards others. Here it is not the consequences of our actions that matter but rather intent.</p></li>
<li><p>There are some practical problems with utilitarianism but perhaps the most significant flaw in utilitarianism for moral reasoning is the omission of justice as a consideration.</p></li>
<li><p>Principles of Justice as Fairness:</p>
<ol>
<li><p><strong>Liberty principle:</strong> Each person has the same indefeasible claim to a fully adequate scheme of equal basic liberties, which is compatible with the same scheme of liberties for all;</p></li>
<li><p><strong>Equality principle:</strong> Social and economic inequalities are to satisfy two conditions:</p>
<ol>
<li><p><strong>Fair equality of opportunity:</strong> The offices and positions to which they are attached are open to all under conditions of fair equality of opportunity;</p></li>
<li><p><strong>Difference principle</strong> They must be of the greatest benefit to the least-advantaged members of society.</p></li>
</ol></li>
</ol>
<p>The principles of justice as fairness are ordered by priority so that fulfilment of the liberty principle takes precedence over the equality principles and fair equality of opportunity takes precedence over the difference principle. In contrast to utilitarianism, justice as fairness introduces a number of constraints that must be satisfied for a decision process to be fair. Applied to a machine learning one might interpret the liberty principle as a requirement of some minimum accuracy level (maximum probability of error) to be set for all members of the population, even if this means the algorithm is less accurate overall. Parallels can be drawn here in machine learning where there is a trade-off between fairness and utility of an algorithm.</p></li>
<li><p>Anti-discrimination laws were born out of long-standing, vast and systemic discrimination against historically oppressed and disadvantaged classes. Such discrimination has contributed to disparities in all measures of prosperity (health, wealth, housing, crime, incarceration) that persist today.</p></li>
<li><p>Legal liability for discrimination against protected classes may be established through both disparate treatment and disparate impact. Disparate treatment (also described as direct discrimination in Europe) refers to both formal differences in the treatment of individuals based on protected characteristics, and the intent to discriminate. Disparate impact (also described as indirect discrimination in Europe) does not consider intent but is concerned with policies and practices that disproportionately impact protected classes.</p></li>
<li><p>Just as the meaning of fairness is subjective, so too is the interpretation of anti-discrimination laws. Two conflicting interpretations are anti-classification and anti-subordination. Anti-classification is a weaker interpretation, that the law is intended to prevent classification of people based on protected characteristics. Anti-subordination is the stronger interpretation that anti-discrimination laws exist to prevent social hierarchies, class or caste systems based on protected features and, that it should actively work to eliminate them where they exist.</p></li>
</ul>
</section>
<section id="association-paradoxes" class="level3 unnumbered">
<h3 class="unnumbered">Association paradoxes</h3>
<ul>
<li><p>Identifying bias in data can be tricky. Data can be misleading. An association paradox is a phenomenon where an observable relationship between two variables disappears or reverses after controlling for one or more other variables. In order to know which associations (or distributions) are relevant, i.e. the marginal (unconditional) or partial associations (conditional distributions), one must understand the causal nature of the relationships. Association paradoxes can also occur for non-collapsible measures of association. Collapsible measures of association are those which can be expressed as the weighted average of the partial measures.</p></li>
</ul>
</section>
<section id="whats-the-harm" class="level3 unnumbered">
<h3 class="unnumbered">What’s the harm?</h3>
<ul>
<li><p>It is important to be clear that in general, machine learning systems are not objective. Data is produced by a necessarily subjective set of decisions. The consistency of algorithms in decision making compared to humans (who make decisions on a case by case basis) is often described as a benefit, but it’s their very consistency that makes them dangerous - capable of discriminating systematically and at scale.</p></li>
<li><p>Classification creates a sense of order and understanding. It enables us to find things more easily, formulate problems neatly and solve them. But classifying people inevitably has the effect of reducing people labels; labels that can result in people being treated as members of a group, rather than individuals.</p></li>
<li><p>Personalisation algorithms that shape our perception of the world in a way that covertly mirror our beliefs can have the effect of trading bridging for bonding capital, the former kind is important in solving global problems that require collective action, such as global warming.</p></li>
<li><p>Targeted political advertising and technologies that enable machines to impersonate humans are powerful tools that can be used as part of orchestrated campaigns of disinformation that manipulate perceptions at an individual level and yet at scale. They are capable of causing great harm to political and social institutions and pose a threat to security.</p></li>
<li><p>An allocative harm happens when a system allocates or withholds an opportunity or resource. Harms of representation occur when systems enforce the subordination of groups through characterizations that affect the perception of them. In contrast to harms of allocation, harms of representation have long-term effects on attitudes and beliefs. They create identities and labels for humans, societies and their cultures. Harms of representation affect our perception of each other and even ourselves. Harms of representation are difficult to quantify. Some types of harms of representation are, stereotyping, (failure of) recognition, denigration, under-representation and ex-nomination.</p></li>
</ul>
</section>
</section>
</section>
<section id="ch_EthicalDev" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Ethical development</h1>
<div class="chapsumm">
<p><strong>This chapter at a glance</strong></p>
<ul>
<li><p>The machine learning cycle - feedback from models to data</p></li>
<li><p>The machine learning development and deployment life cycle</p></li>
<li><p>A practical approach to ethical development and deployment</p></li>
<li><p>A taxonomy of common causes of bias</p></li>
</ul>
</div>
<p>In this chapter, we transition to a more systematic approach to understanding the problem of fairness in decisions making systems. In later chapters we will look at different measures of fairness and bias mitigation techniques but before we discuss and analyse these methods, we review some more practical aspects of responsible model development and deployment. None of the bias mitigation techniques that we will talk about in part three of this book will rectify a poorly formulated, discriminatory machine learning problem or remedy negligent deployment of a predictive algorithm. A model in itself is not the source of unfair or illegal discrimination, models are developed and deployed by people as part of a process. In order to address the problem of unfairness we need to look at the whole system, not just the data or the model.</p>
<p>We’ll start by looking at the machine learning cycle and discuss the importance of how a model is used in the feedback effect it has on data. Where models can be harmful we should expect to have processes in place that aim to avoid common, foreseeable or catastrophic failures. We’ll discuss how to take a proactive rather than reactive approach to managing risks associated with models. We’ll discuss where in the machine learning model development cycle bias metrics and modelling interventions fit. Finally, we’ll classify the most common causes of bias, identifying the parts of the workflow to which they relate.</p>
<p>Our goal is to present problems and interventions schematically, creating a set of references for building, reviewing, deploying and monitoring machine learning solutions that aim to avoid the common pitfalls that result in unfair models. We take a high enough view that the discussion remains applicable to many machine learning applications. The specifics of the framework, can be tailored for a particular use case. Indeed the goal is for the resources in this chapter can be used as a starting point for data science teams that want to develop their own set of standards. Together we will progress towards thinking critically about the whole machine learning cycle, development, validation, deployment and monitoring of machine learning systems. By the end of this chapter we will have a clearer picture of what due diligence in model development and deployment might look like from a practical perspective.</p>
<section id="machine-learning-cycle" class="level2" data-number="2.1">
<h2 data-number="2.1"><span class="header-section-number">2.1</span> Machine Learning Cycle</h2>
<figure>
<img src="02_EthicalDevelopment/figures/Fig_MLCycle.png" id="fig:MLCycle" style="width:70.0%" alt="Figure 2.1: The machine learning cycle" /><figcaption aria-hidden="true">Figure 2.1: The machine learning cycle</figcaption>
</figure>
<p>Machine learning systems can have longterm and compounding effects on the world around us. In this section we analyse the impact in a variety of different examples to breakdown the mechanisms that determine the nature and magnitude of the effect. In Figure <a href="#fig:MLCycle" data-reference-type="ref" data-reference="fig:MLCycle">2.1</a>, we present the machine learning cycle - a high-level depiction of the interaction between a machine learning solution and the real world. A machine learning system starts with a set of objectives. These can be achieved in a myriad of different ways. The translation of these objectives, into a tractable machine learning problem, consists of a series of subjective decisions; what data we collect to train a model on, what events we predict, what features we use, how we clean and process the data, how we evaluate the model and the decision policy are all choices. They determine the model we create, the actions we take and finally the resulting cycle of feedback on the data.</p>
<p>The most familiar parts of the cycle to most developers of machine learning solutions are on the right hand side; processing data, model selection, training and cross validation and prediction. Each action taken on the basis of our model prediction creates a new world state, which generates new data, which we collect and train our model on, and around it goes again. The actions we take based on our model predictions define how we use the model. The same model used in a different way can result in a very different feedback cycle.</p>
<p>Notice that the world state and data are distinct nodes in in the cycle. Most machine learning models rely on the assumption that the training data is accurate, rich and representative of the population, but this is often not the case. Data is a necessarily subjective representation of the world. The sample may be biased, contain an inadequate collection of features, subjective decisions around how to categorise features into groups, systematic errors or be tainted with prejudice decisions. We may not even be able to measure the true metric we wish to impact. Data collected for one purpose is often reused for another under the assumption that it represents the ground truth when it does not.</p>
<div class="lookbox">
<p><strong>Stale data</strong></p>
<p>Suppose a model trained on a dataset is used to make decisions but those decisions are never fed back into the model because the model is not retrained. Our model may continually impact the world state, but the resulting changes are not fed back into the model. What are the implicit assumptions one might be using here? Are there conditions under which this might be an acceptable practice? What are the factors that might affect the nature of the feedback. What makes this better, worse or no different to regular retraining?</p>
</div>
<section id="feedback-from-model-to-data" class="level3" data-number="2.1.1">
<h3 data-number="2.1.1"><span class="header-section-number">2.1.1</span> Feedback from model to data</h3>
<p>In cases where the ground truth assignment (target variable choice) systematically disadvantages certain classes, actions taken based on predictions from models trained on the data can reinforce the bias and even amplify it. Similarly, decisions made on the basis of results derived from machine learning algorithms, trained on data that under or over-represents disadvantaged classes, can have feedback effects that further skew the representation of those classes in future data. The cycle of training on biased data (which justifies inaccurate beliefs), taking actions in kind, and further generating data that reinforces those biases can become a kind of self-fulfilling prophecy. The good news is that just as we can create pernicious cycles that exaggerate disparities, we can create virtuous ones that have the effect of reducing them. Let’s take two illustrative examples.</p>
<section id="predictive-policing" class="level4 unnumbered">
<h4 class="unnumbered">Predictive policing</h4>
<p>In the United States, predictive policing has been implemented by police departments in several states including California, Washington, South Carolina, Alabama, Arizona, Tennessee, New York and Illinois. Such algorithms use data on the time, location and nature of past crimes, in order to determine how and where to patrol and thus improve the efficiency with which policing resources are allocated. A major flaw with these algorithms pertains to the data used to train them. It is not of where crimes occurred, but rather where there have been previous arrests. A proxy target variable (arrests) is used in place of the desired target variable (crime). Racial disparities in policing in the US is a well publicised problem. Figure <a href="#fig:drugs" data-reference-type="ref" data-reference="fig:drugs">2.2</a> demonstrates this disparity for policing of drug related crimes. In 2015, an analysis by The Hamilton Project found that at the state level, Blacks were 6.5 times as Whites to be incarcerated for drug-related crimes<span class="citation" data-cites="HamProj"><a href="#ref-HamProj" role="doc-biblioref">[29]</a></span><span class="marginnote"><span id="ref-HamProj" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[29] </span><span class="csl-right-inline"><span>“Rates of drug use and sales, by race; rates of drug related criminal justice measures, by race.”</span> The Hamilton Project, 2015.</span>
</span>
</span> despite drug related crime being more prevalent among Whites. Taking actions based on predictions from an algorithm trained on arrest data will likely amplify existing disparities between under and over-policed neighbourhoods which correlate with race.</p>
<figure>
<img src="02_EthicalDevelopment/figures/Fig_RatesDrugUseSaleRace.png" id="fig:drugs" alt="Figure 2.2: Rates of drug use and sales compared to criminal justice measures by race[29]." /><figcaption aria-hidden="true">Figure 2.2: Rates of drug use and sales compared to criminal justice measures by race<span class="citation" data-cites="HamProj"><a href="#ref-HamProj" role="doc-biblioref">[29]</a></span>.</figcaption>
</figure>
</section>
<section id="car-insurance" class="level4 unnumbered">
<h4 class="unnumbered">Car insurance</h4>
<p>As a comparative example, let’s consider car insurance. It is well publicised that car insurance companies discriminate against young male drivers (despite age and gender being legally protected characteristics in the countries where these insurance companies operate) since statistically, they are at higher risk of being involved in accidents. Insurance companies act on risk predictions by determining the price of insurance at an individual level - the higher the risk, the more expensive the cost of insurance. What is the feedback effect of this on the data? Of course young men are disadvantaged by having to pay more, but one can see how this pricing structure acts as an incentive to drive safely. It is in the drivers interest to avoid having an accident that would result in an increase in their car insurance premiums. For a high risk driver in particular, an accident could potentially make it prohibitively expensive for them to drive. The feedback effect on the data would be to reduce the disparity in incidents of road traffic accidents among high and low risk individuals.</p>
<p>Along with the difference in the direction of the feedback effects in the examples given above, there is another important distinction to be made in terms of the magnitude of the feedback effect. This is related to how much control the institution making decisions based on the predictions, has over the data. In the predictive policing example the data is entirely controlled by the police department. They decide where to police and who to arrest, ultimately determining the places and people that do (and don’t) end up in the data. They produce the training data, in its entirety, as a result of their actions. Consequently, we would expect the feedback effect of acting on predictions based on the data to be strong and capable of dramatically shifting the distribution of data generated over time. Insurance companies by comparison, have far less influence over the data (consisting individuals involved in road traffic accidents). Though they can arguably encourage certain driving behaviours through pricing, they do not ultimately determine who is and who is not involved in a car accident. As such, feedback effects of risk-related pricing in car insurance are likely to be less strong in comparison.</p>
<div class="lookbox">
<p><strong>Risk related pricing and discrimination</strong></p>
<p>Do you think age and gender based discrimination in car insurance are fair? Why?</p>
</div>
</section>
</section>
<section id="model-use" class="level3" data-number="2.1.2">
<h3 data-number="2.1.2"><span class="header-section-number">2.1.2</span> Model use</h3>
<p>We’ve seen some examples illustrating how the strength and direction of feedback from models to (future) data can vary. In this section we’ll demonstrate how the same model can have a very different feedback cycle depending on how it is used (i.e. the actions that are taken based on its predictions). A crucial part of responsible model development and deployment then should be clearly defining and documenting the way in which a model is intended to be used and relevant tests and checks that were performed. In addition, considering potential usecases for which one might be tempted to use the model but for which it is not suitable and documenting them can prevent misuse. Setting out the specific use case is an important part of enabling effective and focused analysis and testing in order to understand both its strengths and weaknesses.</p>
<p>The idea that the use case for a product, tool or model should be well understood before release; that it should be validated and thoroughly tested for that use case and further that the potential harms caused (even for unintended uses) should be mitigated is not novel. In fact, many industries have safety standards set by a regulatory body that enshrine these ideas in law. The motor vehicle industry has a rich history of regulation aimed at reducing risk of death or serious injury from road traffic accidents that continues to evolve today. In the early days, protruding knobs and controls on the dash would impale people in collisions. It was not until the 1960s that seatbelts, collapsing steering columns and head restraints became a requirement. Safety testing and requirements have continued to expand to including rear brake lights, a variety of impact crash tests, ISOFIX child car seat anchors among others. There are many more such examples across different industries but it is perhaps more instructive to consider an example that involves the use of models.</p>
<p>Let’s look at an example in the banking industry. Derivatives are financial products in the form of a contract that result in payments to the holder contingent on future events. The details, such as payment amounts, dates and events that lead to them are outlined in the contract. The simplest kinds of derivatives are called vanilla options; if at expiry, the underlying asset is above (call option) or below (put option) a specified limit, the holder receives the difference. In order to price them one must model the behaviour of the underlying asset over time. As the events which result in payments become more elaborate, so does the modelling required to be able to price them, as does the certainty with which they can be priced. In derivatives markets, it is a well understood fact that valuation models are product specific. A model that is suitable for pricing a simple financial instrument will not necessarily be appropriate for pricing a more complex one. For this reason, regulated banks that trade derivatives must validate models specifically for the instruments they will be used to price and document their testing. Furthermore they must track their product inventory (along with the models being used to price them) in order to ensure that they are not using models to price products for which the are inappropriate. Model suitability is determined via an approval process, where approved models have been validated as part of a model review process to some standard of due diligence has been carried out for specified the usecase.</p>
<p>Though machine learning models are not currently regulated in this way, it’s easy to draw parallels when it comes to setting requirements around model suitability. But clear consideration of the use case for a machine learning model is not just about making sure that the model performs well for the intended use case. How a predictive model is used, ultimately determines the actions that are taken in kind, and thus the nature of the feedback it has on future data. Just as household appliances come with manuals and warnings against untested / inappropriate / dangerous uses, datasets and models could be required to be properly documented with descriptions, metrics, analysis around usecase specific performance and warnings.</p>
<p>It is worth noting that COMPAS<span class="citation" data-cites="ProPub2"><a href="#ref-ProPub2" role="doc-biblioref">[30]</a></span><span class="marginnote"><span id="ref-ProPub2" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[30] </span><span class="csl-right-inline">J. Larson, S. Mattu, L. Kirchner, and J. Angwin, <span>“How we analyzed the COMPAS recidivism algorithm,”</span> <em>ProPublica</em>, 2016.</span>
</span>
</span> was not developed to be used in sentencing. Tim Brennan (the co-founder of Northpointe and co-creator of its COMPAS risk scoring system) himself stated in a court testimony that they “wanted to stay away from the courts”. Documentation<span class="citation" data-cites="COMPASguide"><a href="#ref-COMPASguide" role="doc-biblioref">[31]</a></span><span class="marginnote"><span id="ref-COMPASguide" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[31] </span><span class="csl-right-inline">Northpointe, <em>Practitioners guide to COMPAS core</em>. 2015.</span>
</span>
</span> for the software (dated 2015 two years later) describes it as a risk and needs assessment and case management system. It talks about it being used “to inform decisions regarding the placement, supervision and case management of offenders” and probation officers using the recidivism risk scales to “triage their case loads”. There is no mention of its use in sentencing. Is it reasonable to assume that a model, developed as a case management tool for probation officers could be used to advise judges with regards to sentencing? Napa County, California, uses a similar risk scoring system in the courts. There a Superior Court Judge who trains other judges in evidence-based sentencing cautions colleagues in their interpretation of the scores. He outlines a concrete example of where the model falls short. “A guy who has molested a small child every day for a year could still come out as a low risk because he probably has a job. Meanwhile, a drunk guy will look high risk because he’s homeless. These risk factors don’t tell you whether the guy ought to go to prison or not; the risk factors tell you more about what the probation conditions ought to be.”<span class="citation" data-cites="ProPub2"><a href="#ref-ProPub2" role="doc-biblioref">[30]</a></span></p>
<p>Propublica’s review of COMPAS looked at recidivism risk for more than 10,000 criminal defendants in Broward County, Florida<span class="citation" data-cites="ProPub3"><a href="#ref-ProPub3" role="doc-biblioref">[32]</a></span><span class="marginnote"><span id="ref-ProPub3" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[32] </span><span class="csl-right-inline">J. Larson, <span>“ProPublica analysis of data from broward county, fla.”</span> ProPublica, 2016.</span>
</span>
</span>. Their analysis found the distributions of risk scores for Black and White defendants to be markedly different, with White defendants being more likely to be scored low-risk - see Figure <a href="#fig:COMPAS" data-reference-type="ref" data-reference="fig:COMPAS">2.3</a>.</p>
<figure>
<img src="02_EthicalDevelopment/figures/Fig_Propublica.png" id="fig:COMPAS" style="width:85.0%" alt="Figure 2.3: Comparison of recidivism risk scores for White and Black defendants[32]" /><figcaption aria-hidden="true">Figure 2.3: Comparison of recidivism risk scores for White and Black defendants<sup><span class="citation" data-cites="ProPub3"><a href="#ref-ProPub3" role="doc-biblioref">[32]</a></span></sup></figcaption>
</figure>
<p>Comparing predicted recidivism rates for over 7,000 of the defendants with the rate that actually occurred over a two-year period, they found the accuracy of the algorithm in predicting recidivism for Black and White defendants to be similar (59% for White and 63% for Black defendants), however the errors revealed a different pattern. They found that Blacks were almost twice as likely as Whites to be labelled as higher risk but not actually re-offend . The errors for White defendants were in the opposite direction; while being more likely to be labelled as low-risk, they more often went on to commit further crimes. See Table <a href="#tbl:COMPAS" data-reference-type="ref" data-reference="tbl:COMPAS">2.1</a>.</p>
<div id="tbl:COMPAS">
<table>
<caption>Table 2.1: COMPAS comparison of risk score errors for White versus Black defendants</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Error type</th>
<th style="text-align: right;">White</th>
<th style="text-align: right;">Black</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Labelled Higher Risk, But Didn’t Re-Offend</td>
<td style="text-align: right;">23.5%</td>
<td style="text-align: right;">44.9%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Labelled Lower Risk, But Did Re-Offend</td>
<td style="text-align: right;">47.7%</td>
<td style="text-align: right;">28.0%</td>
</tr>
</tbody>
</table>
</div>
<p>How might different use cases for the model affect the feedback cycle? Let’s consider some different usecases.</p>
<p>In the courts, the COMPAS recidivism risk score has been used by judges as an aid in determining sentence length - the higher the risk, the longer the sentence. Of course being incarcerated limits ones ability to reoffend but unless the sentence is life, release is inevitable. What impact does a longer sentence have on recidivism? Current research suggests that “The longer and harsher the prison sentence – in terms of less freedom, choice and opportunity for safe, meaningful relationships – the more likely that prisoners’ personalities will be changed in ways that make their reintegration difficult and that increase their risk of re-offending”<span class="citation" data-cites="BBCFPrison"><a href="#ref-BBCFPrison" role="doc-biblioref">[33]</a></span><span class="marginnote"><span id="ref-BBCFPrison" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[33] </span><span class="csl-right-inline">C. Jarrett, <span>“How prison changes people,”</span> <em>BBC Future</em>, May 2018.</span>
</span>
</span>. Now in addition to this consider that as a Black defendent, you are more likely to be incorrectly flagged as high risk. If there was no racial disparity in recidivism rates in the data, we could expect the imbalance in errors to create one. What about crime rates - how do longer sentences impact those? Research shows that it is the certainty, rather than severity of punishment that acts as a deterrent to crime<span class="citation" data-cites="Nagin"><a href="#ref-Nagin" role="doc-biblioref">[34]</a></span><span class="marginnote"><span id="ref-Nagin" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[34] </span><span class="csl-right-inline">D. S. Nagin, <span>“Deterrence in the twenty-first century: A review of the evidence,”</span> <em>Crime and Justice</em>, vol. 42, May 2018.</span>
</span>
</span>. Long-term sentences are particularly ineffective for drug crimes as drug sellers are easily replaced in the community<span class="citation" data-cites="TheSentProj"><a href="#ref-TheSentProj" role="doc-biblioref">[35]</a></span><span class="marginnote"><span id="ref-TheSentProj" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[35] </span><span class="csl-right-inline">M. Mauer, <span>“Long-term sentences: Time to reconsider the scale of punishment,”</span> <em>The Sentencing Project</em>, 2018.</span>
</span>
</span>. On balance, excessive incarceration has negative consequences for public safety because finite resources spent on prison are diverted from policing, drug treatment, preschool programs, or other interventions that might produce crime-reducing benefits.</p>
<div class="lookbox">
<p><strong>Reducing incarceration rates</strong></p>
<p>The US has the highest rate of incarceration in the world, at 0.7% of the population<span class="citation" data-cites="PrisPolInit"><a href="#ref-PrisPolInit" role="doc-biblioref">[36]</a></span><span class="marginnote"><span id="ref-PrisPolInit" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[36] </span><span class="csl-right-inline">P. Wagner and W. Sawyer, <span>“States of incarceration: The global context,”</span> <em>Prison Policy Initiative</em>, 2018.</span>
</span>
</span>. It’s higher than countries with authoritarian governments, those that have recently been locked in civil war and those with murder rates more than twice that in the US. Comparing with countries that have stable democratic governments, the incarceration rate in the US is more than 5 times that of its closest peer - the UK. The US spends $57 billion a year on housing more than 2.2 million people in prison<span class="citation" data-cites="BBCFLongSent"><a href="#ref-BBCFLongSent" role="doc-biblioref">[37]</a></span><span class="marginnote"><span id="ref-BBCFLongSent" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[37] </span><span class="csl-right-inline">B. Lufkin, <span>“The myth behind long prison sentences,”</span> <em>BBC Future</em>, May 2018.</span>
</span>
</span>, almost half of which are private companies that spend significant sums on lobbying the federal government for policies that would further increase incarceration. Some have advocated for the use of risk scores in sentencing in order to reduce the rate of incarceration, the idea being that if the risk scores are low then defendants can be spared prison time. What might the feedback effect be for this usecase? What is the impact of the imbalance in error rates? What assumptions are you making to reach this conclusion?</p>
</div>
<p>Alternatively, suppose the software was used as a way to distribute limited rehabilitation resources, allocating them to those defendants that that were deemed to be at the highest risk of re-offending (and thus the most in need of intervention). Assuming the model to be accurate and that rehabilitation decreased the risk of reoffending, we can expect that using this model would serve to reduce existing disparities in recidivism rates between individuals. What about the imbalance in errors? Black defendents would more often erroneously be allocated rehabilitation resources and white defendents erroneously denied.</p>
<p>We have made numerous assumptions in our analysis of the feedback above; rehabilitation consistently reduces the risk of recidivism (regardless of the crime), that the relationship between sentence length and recidivism risk is monotonic and increasing. That two years is a long enough time horizon to consider. Without getting into the weeds, the point here is simply that the same model can have a very different feedback cycle if used in a different way. How a model is used is important and its <em>performance</em> cannot be evaluated in isolation from its usecase. A question to ask is, does the action taken on the back of the model serve to push extremes to the centre, or push them further apart? The relationships you have to understand to answer the question, will depend on the specifics of the problem.</p>
</section>
</section>
<section id="sec_ResponseDev" class="level2" data-number="2.2">
<h2 data-number="2.2"><span class="header-section-number">2.2</span> Model development and deployment life cycle</h2>
<p>In this section we cover the more practical aspects of ethical model development and deployment. We take a take a higher level view of the process by which machine learning systems are created and identify the stages at which we can build in safety considerations. We take inspiration from model risk management in finance where models are ubiquitous. In banking, processes and policies with regard to development, testing, documentation, review, monitoring and reporting of model related valuation risk, have been developed over decades, alongside regulation. Many of the ideas we discuss in this chapter were developed and implemented after the 2008 credit crisis in an effort to improve controls around valuation model risk for derivative products (more on this later).</p>
<p>Before we think about identifying and categorising common causes of harm in machine learning applications, it will be helpful to outline the workflow through which machine learning models might be developed and deployed responsibly. Figure <a href="#fig:Workflow" data-reference-type="ref" data-reference="fig:Workflow">2.4</a> does exactly this.</p>
<figure class="fullwidth">
<img src="02_EthicalDevelopment/figures/Fig_Workflow.png" id="fig:Workflow" alt="Figure 2.4: Fairness aware machine learning system development, deployment and management workflow." /><figcaption aria-hidden="true">Figure 2.4: Fairness aware machine learning system development, deployment and management workflow.</figcaption>
</figure>
<section id="model-governance-standards" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1"><span class="header-section-number">2.2.1</span> Model governance standards</h3>
<p>At the top, overarching the entire workflow, we have the model governance standards. These essentially outline the processes, roles and responsibilities that constitute the development, deployment and management of the machine learning system. It defines and documents a set of standards for the activities that constitute each stage of the depicted workflow. More on this later.</p>
</section>
<section id="problem-formulation" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2"><span class="header-section-number">2.2.2</span> Problem formulation</h3>
<p>Below this, the life cycle of a machine learning system starts in top left corner with the formulation of the problem. This segment of the development process includes setting objectives, gathering data, analysing and processing it, determining a target variable, relevant features and metrics that indicate success (and failure) of the model (in training, evaluating and monitoring the deployed model). This process should include consulting with experts in the problem domain. The goal here is to understand the problem, data and impact of potential solutions for all the stakeholders. The arrows show that the problem formulation process is an iterative one where ideally domain experts, data collection and processing all inform each other in the creation of a tractable machine learning problem.</p>
<p>An assessment should be made with regards to how appropriate the data is for the model use case. Understanding the provenance of the data (who collected it, how it was collected and for what purpose) is important. Is it representative of the population the model built on it intends to serve? Exploratory data analysis (EDA) should include understanding if there is bias and or discrimination in the data. In particular understanding how is the target variable distributed for different subgroups of the population and what the nature of the resulting machine learning cycle might be for the intended and unintended use cases. Is there strong correlation between protected features and other variables?</p>
<p>Problem formulation should also consider the proposed materiality of the associated risk. What’s the worst that can happen? How might the model be misused of misinterpreted? Would a disclaimer (what this model doesn’t tell you...) be appropriate? How many individuals would be exposed to the model? Is the model within risk appetite (as defined in the model governance standards)? Having a way to understand and compare the risks posed by different models/applications is useful in ensuring the appropriate amount of resource and scrutiny is applied at all stages of the development, deployment and maintenance life cycle.</p>
</section>
<section id="model-development" class="level3" data-number="2.2.3">
<h3 data-number="2.2.3"><span class="header-section-number">2.2.3</span> Model development</h3>
<p>Once the problem is well understood and represented in the data the next broad segment is developing a model. This includes splitting the data into training and testing sets, evaluating the model against its objectives and consequently refining the data, model, evaluation metrics or other aspects. The splitting of data may be more complex, depending on the cross validation approach, but for simplicity we omit specific details in Figure <a href="#fig:Workflow" data-reference-type="ref" data-reference="fig:Workflow">2.4</a>. Part of model development and validation process should be to understand the model’s limitations - where predictions might be unreliable, what it can and cannot be used for. The process of testing and analysing model output for performance should include analysis for discrimination and fairness. How are predictions and errors distributed for different subgroups of the population? How does the model output distribution differ from the training data? Again, model development is an iterative process and the data, metrics, training objectives, post-processing steps and more will evolve as the developers’ understanding of the problem improves.</p>
</section>
<section id="model-owners" class="level3" data-number="2.2.4">
<h3 data-number="2.2.4"><span class="header-section-number">2.2.4</span> Model owners</h3>
<p>For applications deemed ready for deployment, the documentation for the data and model analysis and implementation is submitted to the model owners for review. So who are these model owners? There are often many people involved in the development and deployment of a machine learning system (one would hope, at least two in general) and the model governance standards should specify which of them plays what role in deciding when a solution is ready to be deployed. Each of the model owners will have different (potentially conflicting) concerns. Model owners represent the different stakeholders of the risk associated with the model and collectively they are accountable, though for potentially differing aspects of it. These might include for example,</p>
<ul>
<li><p><strong>Product owners</strong> that will use the system to make decisions.</p></li>
<li><p><strong>Domain experts</strong> that may have had input in the development of the solution (legal, domain or application specific council) and/or may be responsible for dealing with cases for which the model is deemed inappropriate (a radiologist for a pneumonia detector for example).</p></li>
<li><p><strong>Model developers</strong> that were involved in the construction of the model from collecting the data to building the model.</p></li>
<li><p><strong>Independent model validators</strong> that provide adversarial challenge around the modelling and implementation.</p></li>
<li><p><strong>Engineers</strong> that might be responsible for ensuring that infrastructure (for example, data collection, storage, post-deployment monitoring and reporting) requirements can be met.</p></li>
</ul>
</section>
<section id="approval-process" class="level3" data-number="2.2.5">
<h3 data-number="2.2.5"><span class="header-section-number">2.2.5</span> Approval process</h3>
<p>Together model owners determine if the model is approved for deployment or not. For the sake of brevity, and to emphasize the right of the model owners to reject proposed solutions, we describe the situation where the model is not approved, as it being rejected. In reality, rejecting a model need not mean that it is scrapped. Model owners may for example require further analysis or other changes to be made before it is resubmitted for approval. In any organisation, ideally the values, mission and objectives are well enough understood by the members, that a solution being scrapped at the last hurdle would be a rare event. The kinds of issues that would result in rejection should generally be caught at an earlier stage of the model development workflow. Model owners will also be responsible for monitoring the model post-deployment, periodic re-review of the risks and failure postmortums that determine what changes are required when issues arise, including amendments to the model governance standards themselves. The model governance standards might be interpreted as a contract between the model owners that describes their commitments, individually and collectively in managing the risk.</p>
</section>
<section id="management-of-deployed-models" class="level3" data-number="2.2.6">
<h3 data-number="2.2.6"><span class="header-section-number">2.2.6</span> Management of deployed models</h3>
<p>Ensuring the necessary reporting mechanisms are in place so the decision system can be monitored both for validity and exposure, should be a predeployment requirement. This kind of risk tracking can be used as a control, if say limits can be defined which reflect risk appetite. Limits might be set based on how well understood the risks associated with a product (the longer a model is monitored, the more information we have about it) are and what mitigation strategies might be in place, for example.</p>
<p>Importantly the post-deployment cycle of Figure <a href="#fig:Workflow" data-reference-type="ref" data-reference="fig:Workflow">2.4</a> (like the machine learning cycle in Figure <a href="#fig:MLCycle" data-reference-type="ref" data-reference="fig:MLCycle">2.1</a>, at the start of the chapter) includes separate nodes for the model predictions and actions taken. Selbst et al.<span class="citation" data-cites="FairAbs"><a href="#ref-FairAbs" role="doc-biblioref">[38]</a></span><span class="marginnote"><span id="ref-FairAbs" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[38] </span><span class="csl-right-inline">A. D. Selbst, D. Boyd, S. A. Friedler, S. Venkatasubramanian, and J. Vertesi, <span>“Fairness and abstraction in sociotechnical systems,”</span> in <em>Proceedings of the conference on fairness, accountability, and transparency</em>, 2019, pp. 59–68. doi: <a href="https://doi.org/10.1145/3287560.3287598">10.1145/3287560.3287598</a>.</span>
</span>
</span>, describe five traps that one might fall into, even while attempting to create fair machine learning applications. In particular, they describe the <em>framing trap</em>, in which one might unwittingly ensure that an algorithm meets some narrow fairness criterion on outcomes or errors (over the <em>algorithmic frame</em>) but fail to consider its impact in the real world. For example, failing to be sufficiently transparent about the weaknesses of it which leads to it erroneously being prioritised over the judgement of human experts. Or we might fail to consider the longer term impacts on the sociopolitical landscape (over the <em>sociotechnical frame</em>) in determining something as complicated as fairness. If the actions taken off the back of the predictions include human judgement or interpretation, this should also be captured as part of monitoring the model. Are people using the model in ways that were not anticipated or is it having an adverse affect in some other way? Finally we include human experts in the loop again at the stage where predictions are acted upon. Human experts might for example be consulted in cases where the model is understood to produce less reliable predictions, or via an appeals process that is built into the decision system.</p>
<p>Processes and procedures for managing remedial work in the event of failures could be specified as part of the model governance standards. One of the issues with machine learning solutions is that when there are failures (say, a photo or sentence is labelled in an offensive way), the easiest response is an ad hoc rule based approach to ‘fixing’ the specific issue that occurred - the “if this, then do something else” solution, so to speak. But this kind of action isn’t sufficient to address the root of the problem. Remedial work will typically require more resource and planning to fix. A failure should prompt a re-review. Having a more robust process around dealing with failures when they occur, should mean that not only is action is taken in a timely manner, but also that meaningful changes are made as a result of them and that work is appropriately prioritised.</p>
<p>Failure post-mortems that focus on understanding the weaknesses of the model governance process (not the failure of individuals) could also be a means for improving them. Once in production, periodic re-reviews of the model are a means to catch risks that may have been missed the first time around. The frequency of re-reviews can depend on the risk level of the model/application in question if these are being tracked.</p>
</section>
<section id="measuring-fairness" class="level3" data-number="2.2.7">
<h3 data-number="2.2.7"><span class="header-section-number">2.2.7</span> Measuring fairness</h3>
<p>Bias and fairness metrics are essentially calculated on data. There are two stages at which we’ll be interested in measuring bias and or fairness in evaluating our machine learning system. The relevant nodes are coloured red in Figure <a href="#fig:Workflow" data-reference-type="ref" data-reference="fig:Workflow">2.4</a>.</p>
<ol>
<li><p><strong>Model input:</strong> The training data, during the <em>data evaluation</em> stage.</p></li>
<li><p><strong>Model output:</strong> The predictions produced by our model, that is the <em>model evaluation</em> stage.</p></li>
</ol>
<p>Our chosen fairness evaluation metrics calculated on the training data and model output will in general not be the same. By comparing the two, we can evaluate how well the model is replicating relationships in the data.</p>
</section>
<section id="bias-mitigation-techniques" class="level3" data-number="2.2.8">
<h3 data-number="2.2.8"><span class="header-section-number">2.2.8</span> Bias mitigation techniques</h3>
<p>There are three stages at which one can intervene in the development of machine learning model mapping to mitigate bias and they are categorised accordingly. Relevant nodes coloured green in Figure <a href="#fig:Workflow" data-reference-type="ref" data-reference="fig:Workflow">2.4</a>.</p>
<ol>
<li><p><strong>Pre-processing</strong> techniques modify the historical data on which the model is trained (at the <em>data pre-process</em> stage).</p></li>
<li><p><strong>In-processing</strong> techniques alter the training process or objective (at the <em>model training</em> stage).</p></li>
<li><p><strong>Post-processing</strong> techniques take a trained model/s and modify or combine the output (at the <em>model post-process</em> stage).</p></li>
</ol>
</section>
</section>
<section id="sec_ProcessPolicy" class="level2" data-number="2.3">
<h2 data-number="2.3"><span class="header-section-number">2.3</span> Responsible model development and deployment</h2>
<p>In this section we examine a fairness aware development, deployment and management policies for a sociotechnical system. For the most part, the ideas are similar to those concerned with effective model risk management; one that acknowledges that models are fallible and accordingly sets standards for development, deployment, monitoring and maintenance. The intention being, to prevent foreseeable failures and mitigate the associated risks. The main difference is that we consider ethical risk as a central component of the risks that must be managed. Of course utility is an important consideration in being fair (it’s hard to imagine a model that is no better than guessing, as being fair) but utility does not guarantee fairness. Viewing model evaluation through an ethical lens requires a more holistic assessment of the system, it’s purpose, reliability and impact; not just for the business, but for all those exposed to or affected by it and society at large.</p>
<p>We’ll address some of the problems that can’t be solved through the kinds of model mapping interventions we’ll talk about in this book. Another fair machine learning trap described by Selbst et al.<span class="citation" data-cites="FairAbs"><a href="#ref-FairAbs" role="doc-biblioref">[38]</a></span> is the <em>formalism trap</em>, in which one fails to account for the full meaning of complex social concepts, such as fairness, which can’t be formalised with mathematical equations. In chapter <a href="#ch_GroupFairness" data-reference-type="ref" data-reference="ch_GroupFairness">3</a> we’ll show that under such formalisms, a universally fair classifier is precluded by irreconcilable definitions. Fairness might more naturally be established <em>procedurally</em> (as often it is in law). Furthermore, social concepts are deeply <em>contextual</em>, and thus do not lend themselves well to abstraction (a core principal in mathematics which enables portability of solutions). Social concepts evolve over time, as cultural norms shift, therefore <em>contestability</em> is key, as it provides an avenue for change and challenge. These are qualities of a system rather than an equation and cannot be resolved through algorithmic interventions. They require people to do the right thing, and for organisations to define what they consider the right thing to be.</p>
<section id="policy" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1"><span class="header-section-number">2.3.1</span> Policy</h3>
<p>In industry, where innovation demands taking risks and time is money, how do we ensure the proper amount of care and attention is applied when creating products that have the potential for harm? Historically, the answer has been to impose rules that slow the process down, by requiring steps which prioritise safety over other concerns. In order to do this, one must first determine and define a safety standard. In Figure <a href="#fig:Workflow" data-reference-type="ref" data-reference="fig:Workflow">2.4</a>, overarching the whole process is a set of model governance standards. These essentially define that standard. They describe the process through which systems are developed and approved for deployment, and the standard to which systems are tested and evaluated.</p>
<p>In the financial sector, major banks (that are considered to be of systemic importance to a nations financial stability) are subjected to greater scrutiny by the central bank and regulators. An example of this might be requiring them to publish results of <a href="https://www.bankofengland.co.uk/stress-testing/2021/key-elements-of-the-2021-stress-test">solvency stress tests</a>. The currency might be social rather than financial for sociotechnical systems but the principal should be the same.</p>
<section id="prioritisation" class="level4 unnumbered">
<h4 class="unnumbered">Prioritisation</h4>
<p>Products which are of systemic importance to the sociopolitical landscape should have sufficient and appropriate resources (relative to those of the risk generating activities) to manage and mitigate their ethical risk. For applications that carry high risk of harm, risk functions should act as gatekeepers for model deployment and use.</p>
</section>
<section id="model-governance-standards-1" class="level4 unnumbered">
<h4 class="unnumbered">Model governance standards</h4>
<p>Though relatively new terminology in machine learning circles, the concept of model governance has existed for decades. For large financial institutions (which depend on vast numbers of proprietary models), operating and maintaining a model governance framework is a central part of model risk management and a regulatory requirement. The regulatory landscape of the financial sector is considerably more mature than that of other industries and the frameworks used to handle the associated risks have been developed and refined over time. It is therefore instructive to look at how such institutions manage their model risk and consider how these might be applied to sociotechnical systems.</p>
<p>So what does responsible and ethical machine learning development and deployment look like? In reality there is no one size fits all answer. As we’ve noted before, sociotechnical systems are context dependent. The answer can depend on a whole multitude of factors.</p>
<ul>
<li><p><strong>Domain:</strong> Different domains will have different legal and ethical concerns for example employment versus say social media.</p></li>
<li><p><strong>The number and complexity of the models being used by the business:</strong> A large organisation that uses or tests hundreds of models and composes them to make decisions and create new products (such as Microsoft) would benefit greatly from infrastructure and methodologies for measuring the materiality of the associated risks that would enable prioritisation of work related to mitigating them. In contrast, for a business based on a single model that automates a specific task (such as taging images), this would be less of a concern.</p></li>
<li><p><strong>Cost of errors:</strong> Where the stakes are high, for example self driving cars, pre-deployment testing will need to be extensive and prescribed in order to reduce the probability of making mistakes. Well defined and mandatory processes will play an important role - checklists, contingency planning, detailed logging for postmortems and more. For these types of applications we would want authority over model use to be distributed to risk functions which determine when the product is approved for deployment and have the power to decommission them. For a wake word detector (think "Hey Siri", "Okay Google" and "Alexa") a lower standard would be accepted by most.</p></li>
</ul>
<p>Given this, how does one approach the problem of responsible development? Step zero is to create a set of model governance standards, the purpose of which is to clearly define and communicate what responsible model development and deployment looks like for your specific application, use case, domain, business, principles and values.</p>
<p>What are the kinds of questions we might want our model governance standards to answer?</p>
<ul>
<li><p>Why is the work important? What kinds of events or uses of your models are you trying to avoid (or are outside of the organisation’s risk appetite)? What legislation is the company subject to? What are the consequences of failures? What are the values of the company that you want to protect?</p></li>
<li><p>Who is responsible? What are the roles that must be fullfilled to deploy monitor and manage the risks. Who are the stakeholders or model owners and what is their remit? Who is accountable?</p></li>
<li><p>What are model owners responsible for? What technology is covered by the standard. What kind of expertise are required to be able to report, understand and manage the risks? What are the questions each stakeholder must answer? What are the responsibilities of those experts at the various stages of the model development and deployment life cycle? What authority do they have in relation to determining if the model is fit for deployment? Who decides what?</p></li>
<li><p>How do you manage the risk? What are the rules, processes and requirements that ensure the companies values are maintained, people are treated fairly, the legal requirements are fulfilled and risks are appropriately managed? How do the stakeholders work together? For example some roles might need to be independent while others work alongside one another. What are the requirements around training data (documentation, review, storage, privacy, consent and such)? What are the requirements around modelling (documentation, testing, monitoring and such)? What are the processes around proposing, reviewing, testing, deploying, monitoring model related risks? For example, frequency of risk reviews, forums for discussion and monitoring. What are the processes and requirements in place for (specific foreseeable types of) failures? Are there stakeholder specific templates or check-lists that ensure particular questions get answered at specific points in the model development and deployment life cycle?</p></li>
</ul>
<p>The list of questions above is by no means exhaustive but a good starting point. Creating a set of model governance standards is about planning. Machine learning systems can be complicated and have many points of failure: problem formulation, data collection, data processing, modelling, implementation, interpretation. The only way to reduce the risk of failures is to be organised, deliberate and plan for them. Creating a set of standards does exactly that. Where the systems we build have real world consequences, the preparation, planning and process around development, review, analysis, deployment and monitoring of them should reflect that. Ensuring that the right questions get asked at the right time, knowing who is responsible for answering them and being prepared to address problems is a core part of developing and deploying models ethically.</p>
<p>Finally, we note that the benefits of having excellent model governance standards with well defined goals, processes, roles and responsibilities won’t be realised if in practice they are not followed. In large organisations, consistency can be a challenge. The role of internal audit is to provide objective feedback on the risks, systems, processes and compliance at an executive level. From a model governance perspective the role of auditors is to ensure that there are good processes in place and that the processes are being followed. Internal audit’s role is independent of the business up to the executive level. All functions within the business are required to cooperate with internal auditors and provide unfettered access to information requested. Internal audit does not contribute to the improvement of or compliance to processes directly. Their role is to , assess and report back to senior leadership. In a risk management context, internal audit are considered to be the <em>third line of defence</em>. We shall come to the first and second lines shortly.</p>
</section>
<section id="risk-assessment" class="level4 unnumbered">
<h4 class="unnumbered">Risk assessment</h4>
<p>In order to manage risk it must be identified. Any algorithm, no matter how simple, carries the risk of implementation errors or bugs and thus should at the very least be subject to unit testing and independent code review before being deployed. For organisations with more complicated risk profiles, an important component of managing risk is having a system to measure and track it. Having a way to compare risk level across products and or product classes, even if comparisons are coarse, enables some degree of risk appropriate prioritisation and resource allocation in managing them. Risk can be estimated in many different ways and exactly how it is measured will depend on the details of the application. Broadly speaking it should consider both the severity of the event and liklihood. What’s important is not the exact value but rather the ability to compare risks across products, applications or indeed any other lines along which a business is organised. Metrics that capture things like the scale on which the model is being used, utility, training data quality/representiveness, model complexity, potential for harm and more could potentially be used to coarsely judge the risk posed by different applications. Model governance standards can define risk bands or metrics if they are application specific enough.</p>
</section>
</section>
<section id="risk-controls" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2"><span class="header-section-number">2.3.2</span> Risk controls</h3>
<p>In this section we return to the workflow and see how the policies, discussed above, feed into the development, deployment and management of a decisions system. Problem formulation is the first key step in developing a machine learning solution and an especially pivotal one in ethical risk assessment. The problem formulation stage plays perhaps the largest role in determining what the end product will actually be. It is the stage at which the model objectives, requirements, target variable and training data are determined.</p>
<section id="deployment-bias" class="level4 unnumbered">
<h4 class="unnumbered">Deployment bias</h4>
<p>As part of problem formulation one should examine the machine learning cycle in the context of the biases in the data and consider the nature (direction and strength) of the feedback of resulting actions on future data. It’s important to consider other ways in which the model might be used (other than that intended) and understand the feedback cycle in those cases. How the model might be misused/misinterpreted? Are there ways in which it should not be used? Documenting these types of considerations is an essential step in preventing deployment bias; that is, systematic errors resulting through inapproriate model use or misinterpretation of model results. As creators of technologies which affect society at large, documenting our work might be interpreted as a civic duty. We consider documentation to be an essential part of a dataset and model without which it is incomplete and potentially harmful. As such we classify lack of documentation as a model issue.</p>
<p>Repurposing data of models is a risky thing to do and is often the source of bias in models. A good example of this was uncovered by researchers from Berkeley in 2019. They discovered racial bias in an algorithm used to make important health-care determinations for millions of Americans <span class="citation" data-cites="Obermeyer"><a href="#ref-Obermeyer" role="doc-biblioref">[39]</a></span><span class="marginnote"><span id="ref-Obermeyer" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[39] </span><span class="csl-right-inline">Z. Obermeyer, B. Powers, C. Vogeli, and S. Mullainathan, <span>“Dissecting racial bias in an algorithm used to manage the health of populations,”</span> <em>Science</em>, vol. 366, pp. 447–453, Oct. 2019, doi: <a href="https://doi.org/10.1126/science.aax2342">10.1126/science.aax2342</a>.</span>
</span>
</span>. The algorithm was being used to identify patients that would benefit from high-risk care management programs, which improve patient outcomes and reduce healthcare costs for patients with complex healthcare needs. The researchers found that Black patients who had the same risk scores as White patients were far less healthier and thus less likely to be selected for the programs. The bias was the result of data documenting healthcare costs being used to predict healthcare needs.</p>
<p>A thorough examination of ethical issues demands consideration of a diversity of voices, which is well known to be lacking in technology. This is the stage at which it is important to consider who is affected by the technology, consult with them and ensure their views are incorporated in the understanding of the problem and design of a potential solution. Who are the human experts? People who would have valuable insight and opinions on the potential impact of the model you plan on building? Who does the model advantage and who does it disadvantage? Want to use machine learning to help manage diabetes? What are the interests of the health insurance company funding the development? Have you consulted with diabetics in addition to specialist physicians? What are their concerns? What is the problem from the different perspectives? Would a model be able to help or are there simpler solutions?</p>
</section>
<section id="independent-model-validation" class="level4 unnumbered">
<h4 class="unnumbered">Independent model validation</h4>
<p>In any system that is vulnerable to costly errors, unit testing and pre-deployment independent review is a well established method of preventing costly foreseeable failures. Whether it’s a completely new solution built from scratch or a modification to an existing solution that’s being deployed, an independent review process is an important element of responsible model development. Below we describe the responsibilities of two separate roles, the model developers and the model validators.</p>
<p>The model developers role is to translate the business problem into a tractable machine learning problem and create a solution. They will work with the business and receive input from other necessary domain experts relevant to the application to develop a possible solution. This will include tasks such as acquiring and interpreting data that is relevant for the problem, determining a target variable, model objectives, performance measures, fairness measures and more. In terms of preventing failures, model developers are considered the <em>first line of defence</em>. The responsibility of developing a model responsibly lies, in the first instance, with them. The model developers should aim to create a model they believe to be production ready and more specifically, fulfill the requirements specified in the model governance standards.</p>
<p>As part of the pre-deployment process, the model should be reviewed. Model validators will have a similar skill set to model developers but their goal is different to that of the model developers. Where the developers primary objective is to create a solution to the business problem that meets a standard which will be approved by model owners, the role of a model validator is to critique that solution and expose problems with it - the more the better. Their role is to adversarially challenge the solution. They might challenge performance claims (error, bias, fairness) by changing the data or metrics, or demonstrate problems with the model by comparing with an alternative solution. The goal is to expose model weaknesses and demonstrate the limits of its validity in testing and documentation. The model validator might devise mitigation strategies for identified risks. Such strategies might include setting model usage limits (that might trigger a re-review for example) or additional monitoring requirements. They might for example identify additional cases when human review might be required or reject the proposed solution entirely if the problems with the model are great enough. The role of the reviewer could be thought of as something akin to a hacker but with the advantage of having the keys in the form of model documentation (provided by the developers). The model reviewer in pre-deployment can act as a gatekeeper.</p>
<p>Note that in our terminology, the model is simply a mapping. It need not be learned by calibration to historic data. Any algorithm where the decision being made is important enough should be treated as such and proper precautions should be taken. For an algorithm which will be used in production, no matter how simple, this should mean being subject to code review and unit testing that demonstrates its validity in some well chosen cases. A good example of where this would have been valuable came up in Decemeber 2020 when a bug in an algorithm, meant that <a href="https://www.npr.org/sections/coronavirus-live-updates/2020/12/18/948176807/stanford-apologizes-after-vaccine-allocation-leaves-out-nearly-all-medical-resid">Stanford Hospital Residents were not correctly prioritised for the COVID-19 vaccine</a>, despite working with COVID-19 patients daily. The algorithm did not apparently account for the fact that Resident doctors had a blank ‘location’ field in the data. We might never know the details of how it was implemented and tested but it hard to imagine such a bungle passed any decent unit test.</p>
<p>The model review process acts as the <em>second line of defence</em>. To be effective, the model reviewer’s role must be independent of the model developer’s to some extent. What does independence mean? We mentioned the distinct goals of their roles and this is important. The validator should not drive the development of a solution approach or model but instead focus on critique. In reality, it’s easy to see that the iterative nature of model development might mean that amendments adressing criticisms of the solution may get rolled into it’s development at multiple stages, blurring the lines between critique and collaboration. From an efficiency perspective, it might make sense for the solution to be reviewed at several critical stages of the development process making the overall process indeed more collaborative. If there’s a problem with the data that was missed, ideally the developer would want to fix it before going on to build and train a model on it. One of the challenges then is how to preserve independence between the roles, and ensure that the value of having adversarial criticism in preventing failures, is not lost in collaboration. How best to preserve independence will depend on the specifics and is something that should be determined within the model governance standards. In a bank, the model developers and validators are required (by the regulator) to serve under different business functions (the trading desk versus risk management). They have different reporting lines up to executive level, and work in physically separate locations.</p>
</section>
<section id="monitoring" class="level4 unnumbered">
<h4 class="unnumbered">Monitoring</h4>
<p>Post-deployment monitoring is an important part of responsible model development and deployment. Analysis should not stop once the model is deployed. Decisions on what to monitor and necessary feedback mechanisms should be determined during development. It’s important to understand if the model is performing in line with expectations (based on pre-deployment testing and analysis). Is the data coming out of the model more or less biased than the data going in? Distributional shifts should be of particular concern where the actions taken based on predictions have a strong impact on the composition of future data.</p>
</section>
<section id="domain-expertise" class="level4 unnumbered">
<h4 class="unnumbered">Domain expertise</h4>
<p>In section <a href="#sec_SimpsParadox" data-reference-type="ref" data-reference="sec_SimpsParadox">1.4</a> we spoke of the importance of domain knowledge in interpreting causal relationships in data. Consulting domain experts at the problem formulation stage can yield considerable ethical risk reducing benefits. Encorporating more diverse perpectives on a problem will surely result in a better design that will benefit a broader cross-section of society. Given that models are simplified representations of real world systems and we know that they will make errors, responsible development should build in processes for anticipating and dealing with such cases and, where appropriate, deferring to the judgement of a human expert.</p>
</section>
</section>
</section>
<section id="common-causes-of-harm" class="level2" data-number="2.4">
<h2 data-number="2.4"><span class="header-section-number">2.4</span> Common causes of harm</h2>
<p>There are many ways in which machine learning solutions can result in harm. In this section we present a taxonomy of common causes and provide examples. At the end of the section, we’ll relate the causes in our taxonomy to the corresponding stages of the model development and deployment life cycle (discussed earlier), indicating where consideration and intervention could prevent them from arising. The goal is for this to serve as a good starting point as a practical reference for developing fairer models. For practicing data scientists it could be helpful as a standard to compare our current practices against, avoid common pitfalls and hopefully help ensure we perform an appropriate level of due diligence before releasing our work. In our taxonomy, we aim to layout both the points at which issues arise and the various points at which one could assess and intervene. For this reason, the table may appear to contain duplications of the same problem viewed from different perspectives. This is intentional. Often different parts of an application are developed independently.<span class="sidenote-wrapper"><label for="sn-4" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-4" class="margin-toggle"/><span class="sidenote">It’s not uncommon for example (thanks to unprecidented growth in data markets), for a model to be built by one organisation, based on data collected by another.<br />
<br />
</span></span> Taking this approach is beneficial since it provides multiple opportunities to see and remedy the same problems.</p>
<p>Before presenting this taxonomy, it’s worth being clear that, in reality, there is no agreed upon terminology that describes the different types of issues that can arise or agreed upon framework for developing machine learning solutions that factor in ethical safety concerns (since regulation surrounding algorithmic decision systems is still in the process of being shaped). Indeed, developing one is the subject of recent research, <span class="citation" data-cites="ConsClfn"><a href="#ref-ConsClfn" role="doc-biblioref">[40]</a></span><span class="marginnote"><span id="ref-ConsClfn" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[40] </span><span class="csl-right-inline">B. d’Alessandro, C. O’Neil, and T. LaGatta, <span>“Conscientious classification: A data scientist’s guide to discrimination-aware classification,”</span> <em>Big Data</em>, vol. 5, no. 2, pp. 120–134, 2017, doi: <a href="https://doi.org/10.1089/big.2016.0048">10.1089/big.2016.0048</a>.</span>
</span>
</span>, <span class="citation" data-cites="BiasFramework"><a href="#ref-BiasFramework" role="doc-biblioref">[41]</a></span><span class="marginnote"><span id="ref-BiasFramework" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[41] </span><span class="csl-right-inline">H. Suresh and J. Guttag, <span>“A framework for understanding sources of harm throughout the machine learning life cycle,”</span> 2021.</span>
</span>
</span>, <span class="citation" data-cites="DS4DS"><a href="#ref-DS4DS" role="doc-biblioref">[42]</a></span><span class="marginnote"><span id="ref-DS4DS" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[42] </span><span class="csl-right-inline">T. Gebru <em>et al.</em>, <span>“Datasheets for datasets.”</span> 2020.Available: <a href="https://arxiv.org/abs/1803.09010">https://arxiv.org/abs/1803.09010</a></span>
</span>
</span>, <span class="citation" data-cites="MC4MR"><a href="#ref-MC4MR" role="doc-biblioref">[43]</a></span><span class="marginnote"><span id="ref-MC4MR" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[43] </span><span class="csl-right-inline">M. Mitchell <em>et al.</em>, <span>“Model cards for model reporting,”</span> <em>Proceedings of the Conference on Fairness, Accountability, and Transparency</em>, 2019, doi: <a href="https://doi.org/10.1145/3287560.3287596">10.1145/3287560.3287596</a>.</span>
</span>
</span>. The word bias itself has many definitions and even in a given context can have multiple valid interpretations. Different practitioners would likely describe the same type of bias differently. Causes of bias in machine learning applications are often numerous and overlapping, thus difficult to attribute to a single source or prescribe a single solution for. The most appropriate remedy itself will be very much context dependent and different practitioners will choose different approaches.</p>
<p>In creating this taxonomy, we take inspiration from that described by d’Alessandro et. al.<span class="citation" data-cites="ConsClfn"><a href="#ref-ConsClfn" role="doc-biblioref">[40]</a></span>, in which the <em>model</em> or algorithm (function mapping <span class="math inline">\(f\)</span> from features <span class="math inline">\((\boldsymbol{X}, \boldsymbol{Z})\)</span> to predictions <span class="math inline">\(\hat{Y}\)</span>), is distinguished from the larger <em>system</em> (people, infrastructure, processes, policies and risk controls) through which it is developed, deployed and managed. Evidence based medicine provides a rich terminology for different mechanisms through which systematic errors can be introduced in data and has perhaps the most comprehensive set of definitions and classification of bias types. This in itself can provide an important reference in determining which kinds of biases model developers should be aware of and we include some of them here. Table <a href="#tbl:Taxonomy" data-reference-type="ref" data-reference="tbl:Taxonomy">2.2</a> summarises our taxonomy of common causes of harm in machine learning systems.</p>
<div id="tbl:Taxonomy">
<table>
<caption>Table 2.2: Taxonomy of common causes of harm in machine learning systems.</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Element</th>
<th style="text-align: left;">Failure</th>
<th style="text-align: left;">Issue Type</th>
<th style="text-align: left;">Issue Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;" rowspan="11">System</td>
<td style="text-align: left;" rowspan="4">Policy</td>
<td style="text-align: left;" rowspan="2">Prioritisation</td>
<td style="text-align: left;">Failure to allocate appropriate/sufficient resource</td>
</tr>
<tr class="even">
<td style="text-align: left;">Failure to distribute power to manage conflicts of interest</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Governance</td>
<td style="text-align: left;">Failure to set or comply with application specific standards</td>
</tr>
<tr class="even">
<td style="text-align: left;">Risk assessment</td>
<td style="text-align: left;">Failure to identify and manage model related risk</td>
</tr>
<tr class="odd">
<td style="text-align: left;" rowspan="7">Controls</td>
<td style="text-align: left;">Deployment bias</td>
<td style="text-align: left;">Inappropriate model use / misinterpretation of model results</td>
</tr>
<tr class="even">
<td style="text-align: left;" rowspan="3">Independent model validation</td>
<td style="text-align: left;">Data appropriateness and preparation</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Modelling approach and implementation</td>
</tr>
<tr class="even">
<td style="text-align: left;">Model evaluation metrics (pre and post deployment)</td>
</tr>
<tr class="odd">
<td style="text-align: left;" rowspan="2">Monitoring</td>
<td style="text-align: left;">Poor monitoring of model validity and impact</td>
</tr>
<tr class="even">
<td style="text-align: left;">Poor monitoring of risk exposure</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Domain expertise</td>
<td style="text-align: left;">Non deference to human domain expert</td>
</tr>
<tr class="even">
<td style="text-align: left;" rowspan="18">Model</td>
<td style="text-align: left;" rowspan="7">Data</td>
<td style="text-align: left;">Historical bias</td>
<td style="text-align: left;">Data records wrongful discrimination</td>
</tr>
<tr class="odd">
<td style="text-align: left;" rowspan="3">Measurement bias</td>
<td style="text-align: left;">Quality of data varies across protected classes</td>
</tr>
<tr class="even">
<td style="text-align: left;">Measurement process varies across protected classes</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Recording proxies for immeasurable / ill defined variables</td>
</tr>
<tr class="even">
<td style="text-align: left;">Representation bias</td>
<td style="text-align: left;">Data not representative of target population</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Low support</td>
<td style="text-align: left;">Insufficient data for minority classes</td>
</tr>
<tr class="even">
<td style="text-align: left;">Documentation</td>
<td style="text-align: left;">Failure to adequately document</td>
</tr>
<tr class="odd">
<td style="text-align: left;" rowspan="11">Misspecification</td>
<td style="text-align: left;">Aggregation bias</td>
<td style="text-align: left;">Failure to model differences of type</td>
</tr>
<tr class="even">
<td style="text-align: left;" rowspan="3">Target variable</td>
<td style="text-align: left;">Target variable subjectivity</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Proxy target variable learning</td>
</tr>
<tr class="even">
<td style="text-align: left;">Heterogeneous target variable</td>
</tr>
<tr class="odd">
<td style="text-align: left;" rowspan="2">Features</td>
<td style="text-align: left;">Inclusion of protected features without control variables</td>
</tr>
<tr class="even">
<td style="text-align: left;">Inclusion of protected feature proxies (redlining)</td>
</tr>
<tr class="odd">
<td style="text-align: left;" rowspan="2">Cost function</td>
<td style="text-align: left;">Failure to specify asymmetric error costs</td>
</tr>
<tr class="even">
<td style="text-align: left;">Omitted discrimination penalties</td>
</tr>
<tr class="odd">
<td style="text-align: left;" rowspan="2">Evaluation bias</td>
<td style="text-align: left;">Poor choice of evaluation metrics</td>
</tr>
<tr class="even">
<td style="text-align: left;">Test data not representative of the target</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Documentation</td>
<td style="text-align: left;">Failure to adequately document</td>
</tr>
</tbody>
</table>
</div>
<p>In section <a href="#sec_ProcessPolicy" data-reference-type="ref" data-reference="sec_ProcessPolicy">2.3</a> we discussed a framework for responsible development and deployment of models. We summarise important elements of that discussion under <strong>system issues</strong> in our taxonomy of harms. The idea is that if having a process in place could avoid certain types of harms, then not having them is a failure of the system surrounding the model. In this section we discuss common causes of discrimination that relate directly to the model. We categorise these as originating from failures related to one of two sources:</p>
<ol>
<li><p><strong>Data issues</strong> refer to harms that arises as a direct result of issues with the data</p></li>
<li><p><strong>Misspecification</strong> refers to harms that arise through misspecification of the underlying problem in the modelling of it.</p></li>
</ol>
<p>The latter is an extension of the notion of model misspecification in statistics where the functional form of a model does not adequately reflect observed behaviour.</p>
<p>Before discussing our taxonomy for modelling issues, we address a point of contention in the machine learning community - that models are not biased, bias comes from data. The notion that bias is simply an artifact of data rather than a model is not uncommon among machine learning scholars and practitioners. In this book we’ve already discussed numerous examples of biased machine learning models, so where does this idea come from? In more theoretical disciplines a model is interpreted as being the parametric form. Under this definition of a model, different values of the parameters then don’t change what we consider to be our model. For example, the term <em>linear model</em> describes a family of models. More practical disciplines view a model as a function mapping - provided with input, the model returns output. By this definition of a model, if the parameters change, so does the function and thus the model. From a practical perspective then it’s clear that a model can discriminate since if the data documents historic discrimination, we would expect the trained model to reproduce it.</p>
<p>The idea that bias is a data problem, rather than a modelling one is at best a gross oversimplification of the problem and at worst misleading. It implies that in general, after training, a model will perfectly reproduce the joint distribution of the variables in data. Anyone who’s ever trained a model on real world data knows, is patently false. It suggests that models and data are independent when, in practice, they ought not be. Model development is an iterative process. The modelling choices we make can depend on the data and our model results should in turn influence our training data. Treating data and modelling as independent entities diminishes the responsibility of model developers in addressing the problem of biased and unfair applications. It ignores the very practical nature of developing models that serve real people and the societal impact they can have. For sociotechnical systems, the objectives must surely extend beyond utility. We consider defining those wider objectives and incorporating them part of the modelling process and thus failing to consider them a modelling problem.</p>
<section id="data-issues" class="level3" data-number="2.4.1">
<h3 data-number="2.4.1"><span class="header-section-number">2.4.1</span> Data issues</h3>
<p>When it comes to bias, data driven medicine provides a rich vocabulary for the different types. We mention three here.</p>
<section id="historical-bias" class="level4 unnumbered">
<h4 class="unnumbered">Historical bias</h4>
<p>Historical bias arises as a result of differences between accepted societal values and cultural norms and those captured by data. These need not be a result of errors in the data. Even if data perfectly represents some world state, it can still capture a reality which society deems unfair. Training a model on such data will naturally lead to similary unfair predictions. Historical bias can manifest itself in data in numerous ways, through unfair outcomes recorded in the data, differing data quality across groups and under or over-representation of groups to name just a few. Take medical data where racial and gender disparities in diagnosis and treatment are well publicised as the <em>health gap</em>. There is a growing body of research across the US and Europe that exposes systematic under-treatment and misdiagnosis of pain in women (<span class="citation" data-cites="CalderoneSexRoles"><a href="#ref-CalderoneSexRoles" role="doc-biblioref">[44]</a></span><span class="marginnote"><span id="ref-CalderoneSexRoles" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[44] </span><span class="csl-right-inline">K. L. Calderone, <span>“The influence of gender on the frequency of pain and sedative medication administered to postoperative patients,”</span> <em>Sex Roles</em>, vol. 23, pp. 713–725, 1990, doi: <a href="https://doi.org/10.1007/BF00289259">https://doi.org/10.1007/BF00289259</a>.</span>
</span>
</span>, <span class="citation" data-cites="SexAnalgesic"><a href="#ref-SexAnalgesic" role="doc-biblioref">[45]</a></span><span class="marginnote"><span id="ref-SexAnalgesic" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[45] </span><span class="csl-right-inline">E. H. C. MD <em>et al.</em>, <span>“Gender disparity in analgesic treatment of emergency department patients with acute abdominal pain,”</span> <em>Academic Emergency Medicine</em>, vol. 15, pp. 414–418, May 2008, doi: <a href="https://doi.org/10.1111/j.1553-2712.2008.00100.x">https://doi.org/10.1111/j.1553-2712.2008.00100.x</a>.</span>
</span>
</span>, <span class="citation" data-cites="GirlPain"><a href="#ref-GirlPain" role="doc-biblioref">[46]</a></span><span class="marginnote"><span id="ref-GirlPain" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[46] </span><span class="csl-right-inline">D. E. Hoffmann and A. J. Tarzian, <span>“The girl who cried pain: A bias against women in the treatment of pain,”</span> <em>SSRN</em>, 2001, doi: <a href="http://dx.doi.org/10.2139/ssrn.383803">http://dx.doi.org/10.2139/ssrn.383803</a>.</span>
</span>
</span>) and Black patients (despite prescription drug abuse being more prevalent among White Americans), <span class="citation" data-cites="RacialBiasPain"><a href="#ref-RacialBiasPain" role="doc-biblioref">[47]</a></span><span class="marginnote"><span id="ref-RacialBiasPain" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[47] </span><span class="csl-right-inline">K. M. Hoffman, S. Trawalter, J. R. Axt, and M. N. Oliver, <span>“Racial bias in pain assessment and treatment recommendations, and false beliefs about biological differences between blacks and whites,”</span> <em>Proceedings of the National Academy of Sciences</em>, vol. 113, no. 16, pp. 4296–4301, 2016, doi: <a href="https://doi.org/10.1073/pnas.1516047113">10.1073/pnas.1516047113</a>.</span>
</span>
</span>.</p>
</section>
<section id="measurement-bias" class="level4 unnumbered">
<h4 class="unnumbered">Measurement bias</h4>
<p>Measurement bias refers to non-random noise in measurements across groups. This can occur if for example, there are geographic disparities in services provided by an institution or the quantity and quality of the measuring instruments that mean the accuracy and completeness of records vary by location (and other highly correlated variables like race). In some cases institutions can systematically fail to produce accurate and timely records for certain groups. For example, in medical data, where more frequent misdiagnosis of rare diseases for women leads to a longer lag before accurate diagnosis. In particular, 12 compared to 20 months for Crohn’s disease (despite the disease being more prevalent among women) and 16 compared to 4 years for Ehlers-Danlos syndrome<span class="citation" data-cites="EU12K"><a href="#ref-EU12K" role="doc-biblioref">[48]</a></span><span class="marginnote"><span id="ref-EU12K" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[48] </span><span class="csl-right-inline"><span>“The voice of 12,000 patients: Experiences and expectations of rare disease patients on diagnosis and care in europe.”</span> 2009.</span>
</span>
</span>. Systematic delays in diagnosis for protected groups mean that for any given snapshot in time, the medical records for more frequently misdiagnosed groups are less accurate.</p>
<p>Another way in which measurement bias can manifest is if the measurement process varies across groups, for example where the level of scrutiny varies across groups. Predictive policing discussed earlier provides an example of this where there are existing disparities in the level of policing across neighbourhoods. But in practice any process (algorithmic or otherwise) which seeks to identify a behaviour or property (good or bad), but where disproportionate attention is allocated to some subgroup will result in disproportionately more instances of that behaviour or property being observed among members of that group. The result is induced correlation in the data, even in cases where there may in reality be none. One must be careful of making the assumption that where no observation was made the behaviour or property did not exist. The result can be a cycle that continually amplifies the association. Since data often measures and records features which are in fact noisy proxies for the true variables of interest, measurement bias includes cases where use of proxies leads to systematic errors.</p>
</section>
<section id="representation-bias" class="level4 unnumbered">
<h4 class="unnumbered">Representation bias</h4>
<p>Representation bias occurs as a result of biased sampling from the target population. It can be observed as differences in the prevalence of groups when comparing the target population and the sample data. Under-represented classes are exposed to higher error rates; a problem which arises as a result of ‘low support’, that is a smaller pool of data points to train the model on. Looked at from the perspective of the majority class which dominates the aggregate error, the algorithm is naturally incentivised to focus learning characteristics of majority classes.</p>
<p>One of the drivers behind big data initiatives is the plummeting cost of collection and storage data. Companies and institutions are able to train models that better target individuals, reducing costs and boosting profits. However, data collection methods often fail to adequately capture historically disadvantaged classes of people that are less engaged in data generating ecosystems. A good example of this, given by Barocas &amp; Selbst<span class="citation" data-cites="BarocasSelbst"><a href="#ref-BarocasSelbst" role="doc-biblioref">[4]</a></span> is that of the phone app <a href="https://www.boston.gov/transportation/street-bump">Street Bump</a>, which was developed by the City of Boston to reduce the cost and time taken to find (and consequently repair) pot holes. The app uses data generated by the accelerometers and GPS of Boston residents’ smart phones as they drive. Once a pothole is located it is automatically added to the city’s system to schedule a repair. One can see easily see how this method of data collection might fail to adequately capture data from poorer neighbourhoods, where car and smart phone ownership are less prevalent; neighbourhoods which probably correlate with race and are already likely to suffer from lack of investment.</p>
<p>In the extreme case of under-representation, there is no support, that is to say, no data points to train on at all. This can be a problem when say studies of symptoms or clinical trials for drugs have no representation for certain groups among which symptoms or drug effectiveness may well vary. A good example of this is diabetes, the impact of the disease and effectiveness of drugs for which have historically most often been measured on samples with few to no hispanic individuals in datasets at all.</p>
</section>
<section id="low-support" class="level4 unnumbered">
<h4 class="unnumbered">Low support</h4>
<p>Low support may lead to undesirably high errors for some groups even in the absence of representation bias, since minority classes naturally have fewer data points to train on. This is a particular problem for individuals belonging to multiple disadvantaged classes, for example Black women, which are often overlooked when studies seek to meet fairness metric targets.</p>
</section>
<section id="documentation" class="level4 unnumbered">
<h4 class="unnumbered">Documentation</h4>
<p>Documentation of datasets is an essential step in avoiding data misuse or misinterpretation of variables or relationships in the data due to lack of domain knowledge. Documentation should evidence that model governance standards were met. Summaries that explain the provenance of the data (who collected the data, for what purpose, what population was sampled from and how, limitations of the data, clear explanation of the target variables (including consideration of use cases for which it would not appropriate for), breakdown of the demographics and the variables by sensitive features pointing out classes that are not well represented. Documentation that is standardised through use of a template could ensure some level of consistency.</p>
</section>
</section>
<section id="misspecification" class="level3" data-number="2.4.2">
<h3 data-number="2.4.2"><span class="header-section-number">2.4.2</span> Misspecification</h3>
<section id="aggregation-bias" class="level4 unnumbered">
<h4 class="unnumbered">Aggregation bias</h4>
<p>Aggregation bias occurs when heterogenous groups are modelled as homogeneous. In this case we are assuming the same model is approriate for all groups when in fact it is not, it is a failure to recognise differences in type. There are many examples of this is medical models for diagnosis or that measure the effectiveness of treatments. Historically much of medical research is based on data that over-represents White men. Diseases that manifest differently across gender or race are more often misdiagnosed or less effectively treated. Take autism spectrum disorder (AUD) for example, in 2016 research estimated that autism is four times more prevalent in boys than girls. However more recent research has suggested that a contributing factor maybe that autism more often goes undiagnosed in women because studies of the disorder have historically been focused on male subjects. The most notable difference between autistic males and females is how the social (rather than behavioural) symptoms manifest. It is thought that women, especially at the high-functioning end of the spectrum, are more likely to camouflage their symptoms.</p>
</section>
<section id="target-variable-selection" class="level4 unnumbered">
<h4 class="unnumbered">Target variable selection</h4>
<p>One of the challenges in developing a machine learning is the translation of the underlying problem by defining a target variable - something which can be observed, measured and recorded or obtained easily (from a third party vendor), and that accurately reflects the variable we wish to predict. While there are relatively uncontentious examples that machine learning solutions lend themselves well to (spam detection for emails or on-base or slugging percentage for major league baseball player valuation) for many problems the translation is non-trivial and subjective. Take a job applicant filter for example, that aims to find the most promising applicants. The attributes that one might consider to be held by an applicant that make them promising are likely to be described differently by different people even if they work in the same team. Even if two individuals agree on the attributes, it’s likely they’ll weigh the attributes differently based on their experiences and preferences. Different choices will result in the different kinds of biases infiltrating our algorithm.</p>
<p>Often when data on the variable we want to affect doesn’t really exist we use a proxy. In 2018, Amazon was forced to scrap a recruitment tool it spent four years developing. The algorithm rated resumes of potential employees and was trained on 10 years worth of resumes submitted by job applicants. The exact details of the algorithm were not publicised but based on the training data, it is likely that the proxy variable they used was some measure of how the candidates had performed in the hiring process previously. Thus predicting who they would have hired in the past (given their historical and existing biases) rather than who was the best applicant. The problem with such systems is that often they end up being how we define the thing that it’s actually a proxy for.</p>
<p>Issues can also arise when defining a heterogeneous target variable, where a range of different events are coarsely grouped into a single outcome. This is a form of aggregation bias where the issue specifically concerns the target. This might happen for example where the event of particular interest is rare and by including more events in the target the predictive accuracy of the model increases as it has more data to learn from. D’Alessandro et. al<span class="citation" data-cites="ConsClfn"><a href="#ref-ConsClfn" role="doc-biblioref">[40]</a></span> provide a useful example in predictive policing where the model developer is initially interested in predicting violent crime but ends up incorporating petty crimes (which happen much more frequently) in the target variable in pursuit of a more accurate model. The model then ends up trying to learn the features of a more nebulous concept of crime ignoring important differences between different types. Another example might be building a gender recognition system and only recognising people as one of two genders<span class="citation" data-cites="GenderShades"><a href="#ref-GenderShades" role="doc-biblioref">[25]</a></span>.</p>
</section>
<section id="feature-selection" class="level4 unnumbered">
<h4 class="unnumbered">Feature selection</h4>
<p>In an ideal world we would train a machine learning model on a sufficiently large dataset consisting of a rich set of features that actually influence the target variable rather than simply being correlated to it. More often than not, the reality is rather different. Comprehensive data can be expensive and difficult to collect. Factors that influence the target variable might not be easily measured or be measurable at all, while data containing more erroneous indicators might simply be cheaper to obtain or more readily available. This is a common way in which bias against protected classes can enter our model.</p>
<p>The inclusion of protected features without control variables might arise because a protected feature appears to be predictive of the target variable where explanatory variables are not known or available. Of course in cases where using protected characteristics as inputs to an algorithm would lead to disparate treatment liability, this is not a problem one is typically faced with, but it’s worth reiterating the importance of controlling for confounding variables, in drawing conclusions about relationships between features from observational data (see section <a href="#sec_SimpsParadox" data-reference-type="ref" data-reference="sec_SimpsParadox">1.4</a>).</p>
<p>Inclusion of protected feature proxies, as is the case with redlining, is perhaps a more common problem. One where protected features are not used as inputs to the model, but features which are predictive of them are. Historically employers have taken the reputation of the university that applicants graduated from as a strong indicator of the calibre of the candidate. But many of the most reputable universities have very low rates of non-White/Asian students in attendance. A hiring process which is strongly influenced by the university from which the applicant graduated, can erroneously disadvantage racial groups that are less likely to have attended them. While the university an applicant graduated from, might correlate to some degree with success in a particular role, it is not in itself the driver. An algorithm that directly takes into account the skills and competencies required for the role would be more predictive and simultaneously less biased. Given the cost of collecting comprehensive data, one might argue that higher error rates for some classes would be financially justified (rational prejudice).</p>
</section>
<section id="cost-function" class="level4 unnumbered">
<h4 class="unnumbered">Cost function</h4>
<p>A critical consideration in how we specify our model is the cost function. It is how we evaluate our model in training and essentially determines the model (parameters) we end up with. The cost function can be interpreted as an expression of our model objectives and so provides a natural route to addressing discrimination concerns. A common failure in the design of classification models is proper accounting of the costs of the different types of classification errors (false negative versus false positives). If the harm caused by the different types of misclassification are asymmetric, the cost matrix should reflect this asymmetry.</p>
<p>More broadly (for both regression and classification), it is important to consider the contribution from each sample in the training data to the cost function in training. Upsampling (or simply up-weighting, depending on the learning algorithm you are using) is a valuable tool to keep in mind and can alleviate a number of the issues discussed above, that are common sources of bias. Let’s take the issue of low support. By upsampling minority classes, one can increase the importance of reducing errors for those data points, relative to other more abundant classes, during learning. Though it’s worth noting that it cannot resolve issues relating to a lack of richness of representation for classes with low support. Another case in which upsampling can help is that discussed in relation to definition of a heterogeneous target variable. By upsampling data points that correspond to the primary event of interest (violent crime in the example we discussed above), one can again increase the importance of the model fitting to those data points.</p>
<p>For an algorithm that solves a problem in a regulated domain, it would make sense for the absence of discrimination to be a model objective along with utility. This can be achieved by use of a penalty term in the cost function which relates to discrimination in the resulting predictions (just as we have terms that relate to the error or overfitting). Essentially the idea is similar to that of regularisation to avoid overfitting. We introduce an additional hyper-parameter to tune, which represents the strength of the penalty for discrimination in our cost. We will discuss this and upsampling in more detail when we discuss bias mitigation techniques, in part three of the book.</p>
</section>
<section id="evaluation-bias" class="level4 unnumbered">
<h4 class="unnumbered">Evaluation bias</h4>
<p>Evaluation bias arises when evaluating a model’s performance. There are two main components here, the metrics chosen to describe the model’s performance and the benchmark dataset on which they are calculated. Choosing either inappropriately will result in our evaluation metric inaccurately reflecting the efficacy of our model. For sociotechnical problems in particular choosing good metrics requires domain knowledge - the wider political, legal, social and historical context is important when defining what success and failure look like. For example, if building a gender recognition system, one should not simply think of the performance on the specific task but also the wider infrastructural systems which might find the technology useful. Where should we set the bar for such a technology? That should surely depend on how the technology is used after the prediction is made? Are there controls around model use? Should there be? What kinds of risk level does the model present? What might be the impact of the prediction being incorrect? When would an error be fair? What kind of examples would you expect your system to get wrong and why? What do they gave in common? Are they represented in the benchmark dataset? By asking these kinds of questions, when deciding what success looks like, it’s hard to imagine thinking that minimising the mean squared error on a conveniently available dataset would be sufficient.</p>
<p>One approach might be to set accuracy thresholds across all (skin colour) phenotype and gender combinations <span class="citation" data-cites="GenderShades"><a href="#ref-GenderShades" role="doc-biblioref">[25]</a></span>. This would be one way of thinking about success in a way that incorporates <em>some</em> of our societal values of equality. The gender recognition software we talked about in the previous chapter suffered from evaluation bias on both counts. The benchmark datasets used were not representative of the target population and the metrics that were chosen, failed to expose the models poor performance on darker skinned women. The problem of evaluation bias arising from poor choice of testing/benchmark data is often the result of trying to objectively compare performance across models and can lead to overfitting to said benchmark data.</p>
</section>
<section id="documentation-1" class="level4 unnumbered">
<h4 class="unnumbered">Documentation</h4>
<p>Documentation for models (as for datasets) can have a significant impact when it comes to avoiding model misuse (a model use it is not appropriate/approved for) and ensuring model limitations are well understood. It can reduce the risk of misinterpretation of variables as suitable proxies for other variables. Clear explanation of the model, testing that was performed, on what subgroups of the data can make it easier to know which tests might be missing that would offer insight into the validity of the model. Documentation should evidence that the model governance standards have been met. Descriptions of the data and model, motivation behind subjective decisions that were made to arrive at the solution (how to process the data, what features were used/ignored and why, model type, cost function, sample weights, bias and success metrics), known data/model issues, how the model was tested, what it’s limitations are, what it should and should not be used for with justification. Documentation of the model should provide enough detail to be able to re-implement the model, reproduce results and justify the solution approach. Documentation that is standardised through use of a template could ensure some level of consistency and efficiency across domains and applications. Recent research discusses the matter specifically for publicly released datasets<span class="citation" data-cites="DS4DS"><a href="#ref-DS4DS" role="doc-biblioref">[42]</a></span> and machine learning models<span class="citation" data-cites="MC4MR"><a href="#ref-MC4MR" role="doc-biblioref">[43]</a></span>. They suggest standardised analysis which for example demonstrates the performance of the algorithm for different subgroups of the population and requirements for proving efficacy for conjunctions of sensitive characteristics also.</p>
</section>
</section>
</section>
<section id="linking-common-causes-of-harm-to-the-workflow" class="level2" data-number="2.5">
<h2 data-number="2.5"><span class="header-section-number">2.5</span> Linking common causes of harm to the workflow</h2>
<p>In Figure <a href="#fig:Taxonomy" data-reference-type="ref" data-reference="fig:Taxonomy">2.5</a> we provide a visual summary of the taxonomy in Table <a href="#tbl:Taxonomy" data-reference-type="ref" data-reference="tbl:Taxonomy">2.2</a>, the goal being that it might be useful as a reference for teams developing machine learning technologies. Since failures of policy do not relate to any particular part of the model development and deployment life cycle but rather all of it, we omit these.</p>
<figure class="fullwidth">
<img src="02_EthicalDevelopment/figures/Fig_Taxonomy.png" id="fig:Taxonomy" alt="Figure 2.5: Taxonomy of common causes of bias in machine learning models together with the stages of the model development and deployment life cycle they relate to." /><figcaption aria-hidden="true">Figure 2.5: Taxonomy of common causes of bias in machine learning models together with the stages of the model development and deployment life cycle they relate to.</figcaption>
</figure>
<p>At the top of Figure <a href="#fig:Taxonomy" data-reference-type="ref" data-reference="fig:Taxonomy">2.5</a> we have a simplified version of the model development and deployment life cycle. Below this, the causes of harm are displayed in boxes which span the parts of the lifecycle to which they relate. We use colour to sepatate different categories of failures and curly brackets to group issues by type.</p>
</section>
<section id="summary-1" class="level2 unnumbered">
<h2 class="unnumbered">Summary</h2>
<section id="machine-learning-cycle-1" class="level3 unnumbered">
<h3 class="unnumbered">Machine learning cycle</h3>
<ul>
<li><p>Machine learning solutions can have long-term and compounding effects on the world around us. Figure <a href="#fig:MLCycle" data-reference-type="ref" data-reference="fig:MLCycle">2.1</a> illustrates the interaction between a machine learning solution and the real world.</p></li>
<li><p>The translation of a given prblem and objectives into a tractable machine learning problem, requires a series of subjective choices. Choices around what data to train the model on, what events to predict, what features to use, how to clean and process the data, how to evaluate the model and what the decision policy should be will all determine the model we create, the actions we take and ultimately the cycle we end up with.</p></li>
<li><p>Data is a necessarily subjective representation of the world. The sample may be biased, contain an inadequate collection of features, subjective decisions around how to categorise features into groups, systematic errors or be tainted with prejudice decisions. We may not even be able to measure the true metric we wish to impact. Data collected for one purpose is often reused for another under the assumption that it represents the ground truth when it does not.</p></li>
<li><p>In cases where the ground truth (target variable) assignment systematically disadvantages certain classes, actions taken based on predictions from models trained on the data are capable of reinforcing and further amplifying the bias.</p></li>
<li><p>Decisions made on the basis of results derived from machine learning algorithms trained on data that under or over-represents certain classes can have feedback effects that further skew the representation of those classes in future data.</p></li>
<li><p>The actions we take based on our model predictions define how we use the model. The same model used in a different way can result in a very different feedback cycle.</p></li>
<li><p>The magnitude of the feedback effect will depend how much control the institution making decisions based on the predictions, has over the data the training data.</p></li>
<li><p>Just as we can create pernicious machine learning cycles that exaggerate disparities, we can also create virtuous ones that have the effect of reducing disparities. Therefore it’s important to consider the whole machine learning cycle when formulating a machine learning problem</p></li>
</ul>
</section>
<section id="model-development-and-deployment-life-cycle" class="level3 unnumbered">
<h3 class="unnumbered">Model development and deployment life cycle</h3>
<ul>
<li><p>Figure <a href="#fig:Workflow" data-reference-type="ref" data-reference="fig:Workflow">2.4</a> depicts the model development, deployment and monitoring life cycle at a high level. Overarching the entire workflow, are the <strong>model governance standards</strong>. These essentially outline the processes, roles and responsibilities that constitute the development, deployment and management of the machine learning system. It defines and documents a set of standards for the activities that constitute each stage of the workflow.</p></li>
<li><p><strong>Problem formulation:</strong> Translating a business problem into a machine learning one.</p>
<ul>
<li><p>The problem formulation stage plays a pivotal role in what the end product will actually be. It is the stage at which the model objectives, requirements, target variable and training data are determined and it is the stage at which perhaps the most important ethical question (whether the model should be built at all) must be answered.</p></li>
<li><p>Consider who is affected by the technology, consult with them and ensure their views are understood and incorporated in the understanding of the problem and design of a potential solution.</p></li>
<li><p>Assess the materiality of the risk. What’s the worst that can happen? How likely is such a failure? How many people are exposed to the model?</p></li>
<li><p>Examine the machine learning cycle in the context of the biases in the data and consider the nature (direction and strength) of the feedback of resulting actions on future data.</p></li>
<li><p>Consider other ways in which the model might be used (other than that intended) and the corresponding feedback cycle in those cases. How the model might be misused?</p></li>
</ul></li>
<li><p><strong>Independent model validation:</strong> An independent review process is an important element of responsible model development. This means that pre-deployment there are two separate data science roles, model development (designing a solution) and the model validation (critical assessment of the solution).</p></li>
<li><p><strong>Model development:</strong> The model developers role is to translate the business problem into a tractable machine learning problem and create a model solution.</p>
<ul>
<li><p>The model developer will work with the business and receive input from other necessary domain experts relevant to the application to develop a possible solution.</p></li>
<li><p>The model developer should document the solution. Documentation should include descriptions of the data and model, justification of the approach, known issues and limitations, model testing (biases as well as performance), what the model should not be used for and why. Templates are a good way of standardising documentation.</p></li>
<li><p>In terms of preventing failures, the model developer is the <em>first line of defence</em>. The responsibility of developing a model responsibly and ethically lies, in the first instance, with them.</p></li>
</ul></li>
<li><p><strong>Model validation:</strong> The role of a model validator is to criticise the proposed solution.</p>
<ul>
<li><p>The model validator will identify and expose issues with the problem formulation, data and data processing. They will verify the model performance metrics (error, bias, fairness), look for model weaknesses and demonstrate them through testing. They may also devise mitigation strategies for identified risks.</p></li>
<li><p>The role of the reviewer might be thought of as a hacker but with the advantage of having access to the model documentation (provided by the model developer). They also act as a gate keeper.</p></li>
<li><p>The model reviewer must also document their analysis, testing and critique and recommendations regarding the solution.</p></li>
<li><p>The model reviewer acts as the <em>second line of defence</em>.</p></li>
</ul></li>
<li><p><strong>Model approval:</strong> The model owners collectively determine if a solution is ready for deployment.</p>
<ul>
<li><p>Model owners act as the final stage gate keepers before deployment. They will each have been involved in different aspects of the development and deployment of the machine learning system.</p></li>
<li><p>In effect, the model owners represent the different stakeholders of the risk associated with the model and collectively they are accountable, though for potentially differing aspects of it.</p></li>
<li><p>They will also be responsible for monitoring the model and risk materiality post-deployment and ensuring that periodic re-review, failure processes and post-mortems occur and are effective.</p></li>
<li><p>The model governance standards might be interpreted as a contract between the model owners that describes their commitments, individually and collectively in managing the risk.</p></li>
</ul></li>
<li><p><strong>Monitoring of deployed models:</strong> The world is dynamic and the risk associated with models evolves with it. Deployed models should be monitored to understand if they are behaving in line with expectations. The metrics which should be reported to model owners should be identified pre-deployment by the model developer and validator.</p></li>
<li><p><strong>Risk materiality tracking:</strong> As model usage increases so does the associated risk. As part of monitoring, metrics that give an indication of the risk associated with the model is should be reported to the model owners.</p></li>
<li><p><strong>Periodic re-review:</strong> The pre-deployment independent review of the model is just the first. Thereafter, periodic re-reviews of the model are a means to catch risks that may have been missed the first time around. The frequency of re-reviews will depend on the risk level of the model/application in question.</p></li>
<li><p><strong>Failure event process:</strong> Processes and procedures in the event of failures should be specified as part of the model governance standards, in particular what steps should be taken by which model owner. Having a robust process around dealing with failures when they occur should mean that action is taken in a timely manner and that meaningful changes are made as a result of them.</p></li>
<li><p><strong>Failure post-mortems:</strong> A post-mortem should focus on understanding the weaknesses of the model governance process (not the failure of individuals) that contributed to it and appropriately prioritise any changes required to remedy them.</p></li>
<li><p><strong>Measuring bias:</strong> Bias and fairness metrics are essentially calculated on data; the data going into our model (training data) and the data coming out of it (the predictions produced by our model); the data evaluation and model evaluation stages.</p></li>
<li><p><strong>Bias mitigation techniques:</strong> There are three stages at which one can intervene to mitigate bias when developing a machine learning model labelled <em>data pre-process</em>, <em>model training</em> and <em>model post-process</em> in Figure <a href="#fig:Workflow" data-reference-type="ref" data-reference="fig:Workflow">2.4</a>. We categorise them accordingly:</p>
<ul>
<li><p><strong>Pre-processing techniques</strong> modify to the historical data on which the model is trained, the idea being that fair/unbiased data will result in a fair/unbiased model once trained.</p></li>
<li><p><strong>In-processing techniques</strong> alter the training process or objective in order to create model with fairer/less biased predictions.</p></li>
<li><p><strong>Post-processing techniques</strong> take a trained model and modify the output such that the resulting predictions are fairer/less biased.</p></li>
</ul></li>
</ul>
</section>
<section id="responsible-model-development-and-deployment" class="level3 unnumbered">
<h3 class="unnumbered">Responsible model development and deployment</h3>
<section id="model-governance-standards-2" class="level4 unnumbered">
<h4 class="unnumbered">Model governance standards</h4>
<ul>
<li><p>Machine learning systems can be complicated and have many points of failure: problem formulation, the data collection, data processing, modelling, implementation, deployment. The only way to reduce the risk of failures is to be organised, deliberate and plan for them. Creating a set of standards does exactly that. They make sure the right questions get asked at the right time and that there is clarity around who is responsible for what.</p></li>
<li><p>The purpose of creating a set of model governance standards is to clearly define and communicate what responsible model development and deployment looks like for your specific application, domain, business, principles and values. It essentially documents and communicates the why, who, what and how of your model risk management approach.</p>
<ul>
<li><p><strong>Why is the work important?</strong> What kinds of events are you trying to avoid? What are the consequences of failures? What are the values of the company that you want to protect?</p></li>
<li><p><strong>Who is responsible?</strong> Who are the stakeholders? Who is accountable for managing the various identified risks?</p></li>
<li><p><strong>What are they responsible for?</strong> What are their roles/expertise? What authority do they have in relation to determining if the model is fit for deployment?</p></li>
<li><p><strong>How do you manage the risk?</strong> What are the policies, processes and requirements that ensure the companies values are maintained, people are treated fairly, the legal requirements are fulfilled and the model risks are appropriately managed? How do the stakeholders work together?</p></li>
</ul></li>
<li><p>In large companies that carry lots of model risk it can be difficult to ensure there is consistency in standards of due diligence in model development and deployment across the board. The role of internal audit is to provide independent and objective feedback on the risks, systems, processes and compliance at an executive level. From a model governance perspective they determine if that there are good processes in place and that the processes are being followed. From a risk management perspective internal audit’s role constitutes the <em>third line of defence</em>.</p></li>
</ul>
</section>
</section>
<section id="common-causes-of-harm-1" class="level3 unnumbered">
<h3 class="unnumbered">Common causes of harm</h3>
<ul>
<li><p>Table <a href="#tbl:Taxonomy" data-reference-type="ref" data-reference="tbl:Taxonomy">2.2</a> summarises the taxonomy of common causes of bias in a machine learning system.</p></li>
<li><p>Figure <a href="#fig:Taxonomy" data-reference-type="ref" data-reference="fig:Taxonomy">2.5</a> summarises common causes of bias in the context of the model development and deployment workflow, indicating both the stages of the workflow to which they relate and their categorisation within the taxonomy.</p></li>
</ul>
</section>
</section>
</section>
<section id="part-ii-measuring-bias" class="level1 unnumbered">
<h1 class="unnumbered">Part II Measuring Bias</h1>
<p>“To measure is to know. If you cannot measure, you cannot improve it." Lord Kelvin.</p>
<p>“When a measure becomes a target, it ceases to be a measure." Goodhart’s Law</p>
</section>
<section id="ch_GroupFairness" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Group Fairness</h1>
<div class="chapsumm">
<p><strong>This chapter at a glance</strong></p>
<ul>
<li><p>Group fairness concept and metrics</p></li>
<li><p>Comparing different group fairness metrics</p></li>
<li><p>Incompatibility of group fairness criteria</p></li>
</ul>
</div>
<p>The term <em>group fairness</em> is used to describe a class of metrics that are used to measure discrimination or bias in a given decision process (algorithmic or otherwise). In this chapter we will introduce the different types of group fairness metrics in a structured way. We will become familiar with the terminology for well known metrics and we will compare and analyse them, in terms of their meaning and potential implications. We’ll derive results that show how the various fairness metrics discussed, can in fact be incompatible in certain cases; that is to say, they cannot be satisfied simultaneously except in some degenerate cases. The goal of this chapter, is to develop a deeper understanding of group fairness and it’s associated metrics, that will enable us to make more educated decisions about which metrics might offer valuable insights for a given problem.</p>
<p>Group fairness metrics all stem from the same high level notion of fairness; the idea that in a fair system, some <em>property</em> should be similar for different <em>subgroups</em> of a population. The <em>subgroups</em> are typically determined by the values of <em>protected characteristics</em> such as gender or ethnicity. We might also describe these as <em>sensitive features</em>. Partitions of the population could be defined by a single feature or logical conjunctions of multiple sensitive features if we are interested in measuring <em>intersectional</em> disparities. For example, if we were considering both race and gender simultaneously, one group of the partition might be Black women, another White men, and so on. The <em>property</em> we’ll be interested in comparing will be some statistical measure; the particular kind, will depend on our beliefs about what fairness should mean in the context of the problem.</p>
<p>We broadly classify group fairness criteria into two types; those comparing <em>outcomes</em> across groups and those comparing <em>errors</em>. We discussed examples of both in chapter <a href="#ch_Background" data-reference-type="ref" data-reference="ch_Background">1</a>. Recall that in section <a href="#sec_SimpsParadox" data-reference-type="ref" data-reference="sec_SimpsParadox">1.4</a>, we compared outcomes (acceptance rates) for male and female applicants to Berkeley as an example of Simpson’s rule. In section <a href="#sec_harms" data-reference-type="ref" data-reference="sec_harms">1.5</a>, we discussed Gender Shades, a project that compared the errors (or equivalently accuracy) of a set of gender recognition systems, across subgroups defined by skin tone and gender. We’ll see how in general group fairness criterion can be understood as independence constraints on the joint distributions of the non-sensitive features <span class="math inline">\(X\)</span>, sensitive features, <span class="math inline">\(Z\)</span>, the target variable <span class="math inline">\(Y\)</span> and predicted target <span class="math inline">\(\hat{Y}\)</span> (or rather <span class="math inline">\(P\)</span> for a classification problem where we want our fairness criteria to hold for all thresholds). For brevity, we will express all constraints in terms of <span class="math inline">\(\hat{Y}\)</span>, but keep in mind that for classification problems we might want to instead impose it on the score <span class="math inline">\(P\)</span>. We will introduce the necessary mathematical notation as required throughout this book. A helpful summary is provided in the preamble on page .</p>
<section id="sec_BalOut" class="level2" data-number="3.1">
<h2 data-number="3.1"><span class="header-section-number">3.1</span> Comparing outcomes</h2>
<p>First we look at fairness constraints on the relationship between the sensitive features <span class="math inline">\(Z\)</span>, and the predicted target <span class="math inline">\(\hat{Y}\)</span> (or rather <span class="math inline">\(Y\)</span> if we are interested in measuring the fairness of our data rather than our model). We’ll discuss two fairness criteria. In the first we require the outcome <span class="math inline">\(\hat{Y}\)</span>, to be marginally (unconditionally) independent of the sensitive features <span class="math inline">\(Z\)</span>. In the second we are essentially trying to establish cause; we require the outcome <span class="math inline">\(\hat{Y}\)</span> to be independent of the sensitive features <span class="math inline">\(Z\)</span> when conditioned on all other (non-sensitive) features <span class="math inline">\(X\)</span>. We’ll describe the latter as the twin test, that is <span class="math inline">\(\hat{Y}\)</span> and <span class="math inline">\(Z\)</span> being independent ceteris paribus (all else, or rather all other variables <span class="math inline">\(X\)</span>, being equal).</p>
<section id="independence" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1"><span class="header-section-number">3.1.1</span> Independence</h3>
<p>Of all the fairness criteria, independence is the most well known and that which (as we’ll see later), imposes the strongest constraint. It requires the target variable to be marginally (unconditionally) independent of the sensitive feature, that is, <span class="math inline">\(\hat{Y} \bot Z\)</span>. This is true, if and only if (<span class="math inline">\(\iff\)</span>) the probability distribution of the target variable <span class="math inline">\(f_{\hat{Y}}(y)\)</span>, is the same for all values of the sensitive feature <span class="math inline">\(Z\)</span>, that is, <span class="math inline">\(f_{\hat{Y}|Z}(y)=f_{\hat{Y}}(y)\)</span>. For a discrete target variable we can say <span class="math display">\[\hat{Y} \bot Z \quad \iff \quad \mathbb{P}(\hat{Y}=\hat{y}|Z=z) = \mathbb{P}(\hat{Y}=\hat{y}) \quad \forall \quad y \in \mathcal{Y}, \quad z \in \mathcal{Z},\]</span> or <span class="math inline">\(\mathbb{P}(\hat{y}|z)=\mathbb{P}(\hat{y})\)</span> in our abbreviated notation.</p>
<p>Independence might be viewed as addressing disparate impact, since we are only interested in the relationship between the outcome and sensitive feature. Recall that for the 1973 Berkeley admissions example in section <a href="#sec_SimpsParadox" data-reference-type="ref" data-reference="sec_SimpsParadox">1.4</a>, we looked at independence criterion, by comparing acceptance rates across the sensitive feature gender. Imposing independence is a strong expression of the view that fairness is equality. It might be interpreted as the notion that abilities (or features of importance <span class="math inline">\(X\)</span>, in determining <span class="math inline">\(Y\)</span>) in all groups are, or should be, similarly distributed; the belief that observed differences in the joint distributions in training data are a manifestation of unfair discrimination, rather than inherent differences in people belonging to groups that differ by their sensitive features <span class="math inline">\(Z\)</span>.</p>
<section id="measures-of-independence" class="level4 unnumbered">
<h4 class="unnumbered">Measures of independence</h4>
<p>Below we will define a range of fairness metrics, which are all derived from the notion of independence. Along the way, we will familiarise ourselves with some of the terminology used to describe the various independence metrics that are commonly analysed. In each case the metric provides some measure, of how far from independent, the target variable and sensitive feature are. Notice that independence imposes a constraint on only two random variables, the predicted target <span class="math inline">\(\hat{Y}\)</span> and sensitive feature <span class="math inline">\(Z\)</span>. In the equations that follow, we provide metrics that quantify the fairness of our model output <span class="math inline">\(\hat{Y}\)</span>, but we could equally well replace the predicted target variable <span class="math inline">\(\hat{Y}\)</span>, with the actual target variable <span class="math inline">\(Y\)</span> to assess the fairness of our data under the same criterion.</p>
<p><strong>Mutual information</strong>, denoted <span class="math inline">\(I\)</span>, is popular in information theory for measuring dependence between random variables.</p>
<div id="eq:MutualInfo">
<table style="width:99%;">
<colgroup>
<col style="width: 90%" />
<col style="width: 9%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
I(\hat{Y},Z) = \int_{\hat{y} \in \mathcal{Y}} \int_{z \in \mathcal{Z}}
f_{\hat{Y},Z}(\hat{y},z) \log \frac{f_{\hat{Y},Z}(\hat{y},z)}{f_{\hat{Y}}(\hat{y})f_Z(z)}\,\mathrm{d}z\,\mathrm{d}\hat{y}.\]</span></td>
<td style="text-align: right;">(3.1)</td>
</tr>
</tbody>
</table>
</div>
<p>It is equal to zero, if and only if the joint distribution of <span class="math inline">\(Z\)</span> and <span class="math inline">\(\hat{Y}\)</span> is equal to the product of their marginal distributions, that is if <span class="math inline">\(f_{\hat{Y},Z}(\hat{y},z)=f_{\hat{Y}}(\hat{y})f_Z(z)\)</span>. Therefore, two variables which have zero mutual information are independent. The <strong>normalised prejudice index</strong><span class="citation" data-cites="Fukuchi"><a href="#ref-Fukuchi" role="doc-biblioref">[49]</a></span><span class="marginnote"><span id="ref-Fukuchi" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[49] </span><span class="csl-right-inline">K. Fukuchi, J. Sakuma, and T. Kamishima, <span>“Prediction with model-based neutrality,”</span> <em>IEICE TRANS. INF. &amp; SYS.</em>, vol. E98–D, no. 8, 2015, doi: <a href="https://doi.org/Fukuchi">Fukuchi</a>.</span>
</span>
</span> divides mutual information by a normalising factor so that the resulting value falls between zero and one:</p>
<div id="eq:npi">
<table style="width:99%;">
<colgroup>
<col style="width: 90%" />
<col style="width: 9%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
r_{\text{npi}} = \frac{I(\hat{Y},Z)}{\sqrt{H(\hat{Y})H(Z)}},\]</span></td>
<td style="text-align: right;">(3.2)</td>
</tr>
</tbody>
</table>
</div>
<p>where</p>
<div id="eq:entropy">
<table style="width:99%;">
<colgroup>
<col style="width: 90%" />
<col style="width: 9%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
H(Y) = -\int_{y \in \mathcal{Y}} f_Y(y) \log f_Y(y)\,\mathrm{d}y,\]</span></td>
<td style="text-align: right;">(3.3)</td>
</tr>
</tbody>
</table>
</div>
<p>is the entropy. We have provided the formula for continuous random variables, for discrete variables we simply replace the integrals with sums.</p>
<div class="lookbox">
<p><strong>Exercise: Normalised prejudice index</strong></p>
<p>Write a function that takes two arrays <span class="math inline">\(y\)</span> and <span class="math inline">\(z\)</span> of categorical features and returns the normalised prejudice index. Hint:</p>
<ol>
<li><p>Compute the probability distributions <span class="math inline">\(\mathbb{P}(y)\)</span>, <span class="math inline">\(\mathbb{P}(z)\)</span> and <span class="math inline">\(\mathbb{P}(y,z)\)</span>. Note that these can be thought of as the frequency with which each event occurs.</p></li>
<li><p>Compute the entropies <span class="math inline">\(H(y)\)</span> and <span class="math inline">\(H(z)\)</span> shown in equation (3.3) and use these to compute the normalising factor, <span class="math inline">\(\sqrt{H(y)H(z)}\)</span>.</p></li>
<li><p>Compute the mutual information <span class="math inline">\(I(z,y)\)</span> shown in equation (3.1) and divide by the normalising factor.</p></li>
</ol>
<p>Test your implementation against scikit-learn’s: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.normalized_mutual_info_score.html">sklearn.metrics.normalized_mutual_info_score</a>.<br />
<a href="#GF_NPI">Solution</a> in appendix <a href="#sec_app_GFSolutions" data-reference-type="ref" data-reference="sec_app_GFSolutions">D.1</a>.</p>
</div>
<p>A simple relaxation of independence requires only the mean predicted target variable (rather than the full distribution) to be equal for all values of the sensitive feature, that is, <span class="math display">\[\mathbb{E}(\hat{Y} | Z=a) = \mathbb{E}(\hat{Y} | Z=b).\]</span> A popular metric derived from this for regression problems is called the <strong>mean difference</strong> (illustrated in Figure <a href="#fig:distributions" data-reference-type="ref" data-reference="fig:distributions">3.1</a>) which (as the name suggests) looks at the difference between the mean predictions for different partitions of the population based on the sensitive feature <span class="math inline">\(Z\)</span>, <span class="math display">\[d = \mathbb{E}(\hat{Y} | Z=a) - \mathbb{E}(\hat{Y} | Z \ne a).\]</span></p>
<figure>
<img src="03_GroupFairness/figures/Fig_distributions.png" id="fig:distributions" alt="Figure 3.1: Visualisation of the mean difference for a continuous target variable." /><figcaption aria-hidden="true">Figure 3.1: Visualisation of the mean difference for a continuous target variable.</figcaption>
</figure>
<p>Taking the simplest example of discrete binary classifier where we have a binary sensitive feature. We can write the requirement of independence as, <span class="math display">\[\mathbb{P}(\hat{Y}=1 | Z=1) = \mathbb{P}(\hat{Y}=1 | Z=0).\]</span> This criterion goes by many names in research literature - <strong>demographic parity</strong>, <strong>statistical parity</strong> and <strong>parity impact</strong>, among others.</p>
<p>With this criterion for fairness of a classifier, we can quantify the disparity by looking at the difference or the ratio of the probabilities for each sensitive feature. Both are straightforward to calculate given the 2 <span class="math inline">\(\times\)</span> 2 contingency table (Table <a href="#tbl:independence" data-reference-type="ref" data-reference="tbl:independence">3.1</a>) summarising the observed relationship between the sensitive feature and outcome. Each cell of the contingency table shows the number of examples satisfying the conditions given in the corresponding row and column headers. So for example, <span class="math inline">\(n_{01}\)</span> is the number of data points for which <span class="math inline">\(Z=0\)</span> and <span class="math inline">\(\hat{Y}=1\)</span>.</p>
<div id="tbl:independence">
<table>
<caption>Table 3.1: Contingency table for prediction against the sensitive feature.</caption>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;"><span class="math inline">\(\hat{Y}=1\)</span></th>
<th style="text-align: right;"><span class="math inline">\(\hat{Y}=0\)</span></th>
<th style="text-align: right;">Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(Z=1\)</span></td>
<td style="text-align: right;"><span class="math inline">\(n_{11}\)</span></td>
<td style="text-align: right;"><span class="math inline">\(n_{10}\)</span></td>
<td style="text-align: right;"><span class="math inline">\(n_{Z=1}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(Z=0\)</span></td>
<td style="text-align: right;"><span class="math inline">\(n_{01}\)</span></td>
<td style="text-align: right;"><span class="math inline">\(n_{00}\)</span></td>
<td style="text-align: right;"><span class="math inline">\(n_{Z=0}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Total</td>
<td style="text-align: right;"><span class="math inline">\(n_{\hat{Y}=1}\)</span></td>
<td style="text-align: right;"><span class="math inline">\(n_{\hat{Y}=0}\)</span></td>
<td style="text-align: right;"><span class="math inline">\(n\)</span></td>
</tr>
</tbody>
</table>
</div>
<p>In bio-medical sciences, the <strong>risk difference</strong>: <span class="math display">\[d = \mathbb{P}(\hat{Y}=1 | Z=1) - \mathbb{P}(\hat{Y}=1 | Z=0)
  = \frac{n_{11}}{n_{Z=1}} - \frac{n_{01}}{n_{Z=0}},\]</span> measures the impact of treatment (or risk factors), <span class="math inline">\(Z\)</span> on outcome, <span class="math inline">\(\hat{Y}\)</span>. In discrimination literature, it has been described as the <strong>discrimination score</strong> and <strong>statistical parity difference</strong> among others. Note that if <span class="math inline">\(\hat{Y}=1\)</span> is the advantageous outcome and <span class="math inline">\(Z=1\)</span> is the advantaged group, we would expect <span class="math inline">\(d\)</span> to be non-negative. The algorithm is fair when <span class="math inline">\(d=0\)</span>. The further from zero, the more unfair. A modified version of this metric is the <strong>normalised difference</strong><span class="citation" data-cites="Zliobaite"><a href="#ref-Zliobaite" role="doc-biblioref">[50]</a></span><span class="marginnote"><span id="ref-Zliobaite" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[50] </span><span class="csl-right-inline">I. Zliobaite, <span>“On the relation between accuracy and fairness in binary classification.”</span> 2015.Available: <a href="https://arxiv.org/abs/1505.05723">https://arxiv.org/abs/1505.05723</a></span>
</span>
</span> which divides the difference by,</p>
<div id="eq:dmax">
<table style="width:99%;">
<colgroup>
<col style="width: 90%" />
<col style="width: 9%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
d_{\max} = \min\left\{ \frac{\mathbb{P}(\hat{Y}=1)}{\mathbb{P}(Z=1)},
                       \frac{\mathbb{P}(\hat{Y}=0)}{\mathbb{P}(Z=0)} \right\}
= \min\left\{ \frac{n_{\hat{Y}=1}}{n_{Z=1}},
              \frac{n_{\hat{Y}=0}}{n_{Z=0}} \right\},\]</span></td>
<td style="text-align: right;">(3.4)</td>
</tr>
</tbody>
</table>
</div>
<p>thus ensuring the normalised difference is bounded between plus and minus one.</p>
<div class="lookbox">
<p><strong>Exercise: Statistical parity difference maximum</strong></p>
<p>Show that <span class="math display">\[d_{\max} = \min\left\{ \frac{\mathbb{P}(\hat{Y}=1)}{\mathbb{P}(Z=1)},
                       \frac{\mathbb{P}(\hat{Y}=0)}{\mathbb{P}(Z=0)} \right\}.\]</span> <a href="#GF_SPDmax">Solution</a> in appendix <a href="#sec_app_GFSolutions" data-reference-type="ref" data-reference="sec_app_GFSolutions">D.1</a>.</p>
</div>
<p>Alternatively, we could instead take the ratio as a measure of discrimination: <span class="math display">\[r = \frac{\mathbb{P}(\hat{Y}=1 | Z=1)}{\mathbb{P}(\hat{Y}=1 | Z=0)}
  = \frac{n_{11}/n_{Z=1}}{n_{01}/n_{Z=0}}.\]</span> In biomedical sciences this measure is called the <strong>risk ratio</strong>. It is used to measure the strength of association between treatment (or risk factors), <span class="math inline">\(Z\)</span>, and outcome, <span class="math inline">\(\hat{Y}\)</span>. It has been described in discrimination aware machine learning literature as the <strong>impact ratio</strong> or <strong>disparate impact ratio</strong>. The algorithm is fair if <span class="math inline">\(r=1\)</span>. The further from one <span class="math inline">\(r\)</span> is, the more unfair. The Equal Employment Opportunity Commission (EEOC) have used this measure in their guidelines for identifying discrimination in employment selection processes<span class="citation" data-cites="US-EEOC"><a href="#ref-US-EEOC" role="doc-biblioref">[51]</a></span><span class="marginnote"><span id="ref-US-EEOC" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[51] </span><span class="csl-right-inline">U. S. E. E. O. Commission, <span>“Questions and answers to clarify and provide a common interpretation of the uniform guidelines on employee selection procedures,”</span> <em>Federal Register</em>, vol. 44, no. 43, 1979.</span>
</span>
</span>. As a rule of thumb, the EEOC determine that a company’s selection system is having an adverse impact on a particular group if the selection rate for that group is less than four-fifths (or 80%) that of the most advantaged group, that is, the impact ratio is less than 0.8 where <span class="math inline">\(Z=0\)</span> is the most advantaged group (for which the acceptance rate is the highest).</p>
<p>The <strong>elift ratio</strong><span class="citation" data-cites="Pedreschi"><a href="#ref-Pedreschi" role="doc-biblioref">[52]</a></span><span class="marginnote"><span id="ref-Pedreschi" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[52] </span><span class="csl-right-inline">D. Pedreschi, S. Ruggieri, and F. Turini, <span>“Discrimination-aware data mining,”</span> in <em>Proceedings of the ACM SIGKDD international conference on knowledge discovery and data mining</em>, 2008, pp. 560–568. doi: <a href="https://doi.org/10.1145/1401890.1401959">10.1145/1401890.1401959</a>.</span>
</span>
</span> is similar to the impact ratio but instead of comparing acceptance rates for protected groups to each other, we compare them to the overall acceptance rate: <span class="math display">\[r_{\text{elift}} = \frac{\mathbb{P}(\hat{Y}=1 | Z=0)}{\mathbb{P}(\hat{Y}=1)}.\]</span></p>
<p>In theory, any measure of association suitable for the data types can be used as a metric to understand the magnitude of discrimination in our data or predictions. The <strong>odds ratio</strong> (popular in natural, social and biomedical sciences) is the ratio of the odds of a positive prediction for each group. We can write it as: <span class="math display">\[r_{\text{odds}}
= \frac{\mathbb{P}(\hat{Y}=1 | Z=1)\mathbb{P}(\hat{Y}=0 | Z=0)}
       {\mathbb{P}(\hat{Y}=0 | Z=1)\mathbb{P}(\hat{Y}=1 | Z=0)}
= \frac{n_{11}n_{00}}{n_{10}n_{01}}.\]</span> The odds ratio is equal to one when there is no discrimination. Recall that the odds ratio is not a collapsible measure (see section <a href="#sec_collapsibility" data-reference-type="ref" data-reference="sec_collapsibility">1.4.3</a>).</p>
<div class="lookbox">
<p><strong>Exercise: Odds ratio</strong></p>
<p>Show that the odds ratio is always greater than or equal to one in the case where <span class="math inline">\(\hat{Y}=1\)</span> in the advantaged outcome and <span class="math inline">\(Z=1\)</span> is the privileged group. <a href="#GF_OR">Solution</a> in appendix <a href="#sec_app_GFSolutions" data-reference-type="ref" data-reference="sec_app_GFSolutions">D.1</a>.</p>
</div>
<p>As mentioned earlier, independence metrics is they can be evaluated on both the data and the model. A common problem in machine learning is that existing biases in the data can be exaggerated if protected groups are minorities in the population. By comparing bias metrics for the data with those of our model output, we can understand if our model is inadvertently introducing biases that do not originate from the data.</p>
<p>It might seem intuitive already, that independence can only be satisfied by a model (optimising for utility), if the target variable <span class="math inline">\(Y\)</span> and sensitive feature <span class="math inline">\(Z\)</span> are in fact independent. <span class="sidenote-wrapper"><label for="sn-5" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-5" class="margin-toggle"/><span class="sidenote">We’ll prove this to be true in section <a href="#sec_Impossible" data-reference-type="ref" data-reference="sec_Impossible">3.3</a>, for the case where our variables are binary.<br />
<br />
</span></span> If this is not the case, then satisfying independence for your model, will not permit the theoretically ‘perfect’ solution <span class="math inline">\(\hat{Y}=Y\)</span> (should your model be able to achieve it). We would also then naturally, expect that the stronger the relationship between the sensitive feature and target, the greater the trade-off between fairness and utility in satisfying independence criterion.</p>
<p>A major shortcoming of independence (discussed in section <a href="#sec_SimpsParadox" data-reference-type="ref" data-reference="sec_SimpsParadox">1.4</a>) is that it doesn’t consider that there may be confounding variables. It assumes that all relevant features, are distributed among all protected groups, equally. This is a strong assumption, and erronously enforcing it does not guarantee fairness in a broader sense. Consider a simple hypothetical example where, there are discrepancies between credit card approval rates for men and women at the population level, which disappear once you control for (the confounding variable) income. It could be argued then that the real issue with respect to fairness here, appears to be the fact that women, generally earn less than men. If the lender was to enforce independence between gender and its loan approval rate by, for example, setting lower income requirements for women than men, this might feasibly lead to higher default rates among women. Clearly a less than desirable solution which, arguably, doesn’t address the actual underlying problem. Furthermore, it might be argued that enforcing independence could lead to less fair outcomes, on an individual level; in the sense that a man and woman who were the same in all other respects (features) would receive different outcomes, violating the twin test, which we’ll talk about in the next section. We’ll talk about individual fairness in the next chapter.</p>
<p>Suppose we want to measure the relationship between the sensitive feature and outcome using one of the above metrics. A natural solution to the problem of confounding variables, is to control for them, that is if, you have them recorded in your dataset. Next, we consider the case where we condition on all the non-sensitive variables <span class="math inline">\(X\)</span>.</p>
</section>
</section>
<section id="sec_CondIndep" class="level3" data-number="3.1.2">
<h3 data-number="3.1.2"><span class="header-section-number">3.1.2</span> The twin test</h3>
<p>The twin test tries to establish cause, by conditioning on all non-sensitive features. In this case, our fairness criterion requires the predicted target variable to be independent of the sensitive features when conditioned on all other features. This is true, if and only if, the probability distribution of <span class="math inline">\(Y\)</span> conditioned on <span class="math inline">\(X\)</span> is the same, for all values of the sensitive feature; <span class="math display">\[\hat{Y} \bot Z | X \quad \iff \quad
f_{\hat{Y}|X}(\hat{y}, z; x) = f_{\hat{Y}|X}(\hat{y}; x).\]</span> Suppose we wish to establish a causal connection between the decision or outcome and an individual’s membership in some protected group. Typically, in a human decision process which is subjective, there are a number of unobserved variables. Take a job interview for example, the factors that determine who gets hired are typically subjective (which means that two people might rate an interview candiate differently even on the same feature, or, a candidate might rate differently on the same feature if they were given an alternative test. This makes proving a causal connection difficult. But in the case where a decision is made purely on the basis of an algorithm, making this causal connection becomes trivial. We simply perform a so called ‘twin test’. Imagine a ‘counterfactual’ world in which for every individual in this world (say John Doe) there exists an ‘identical twin’ in the counterfactual world which differers only by the sensitive feature gender (Jane Doe). If a model produces predictions that are different for for John and Jane, we have established the individual’s gender as being the reason for it.</p>
<p>Taking this approach to establishing cause with a model is pretty straight forward. We simply conduct a randomized experiment. The individuals for which we check the model output, need not exist, we can simply fabricate them, and compare model predictions with those of the counterfactual twin. Performing the twin test on a dataset (i.e. where you do not have access to the algorithm, only the outcomes recorded in the data) is less trivial, since the conterfactual twin for any given example, need not exist; and we have no way of producing them without the algorithm. Furthermore, for any given point in the non-sensitive feature space, the number of data points will likely be too small, to justify the use of statistical methods in establishing cause. Barring these limitations, using the counterfactual approach to establishing the fairness of our model, we can consider all the metrics we have above with independence as our fairness criterion but conditioned on <span class="math inline">\(X\)</span> as well as <span class="math inline">\(Z\)</span>. So for example we define the <strong>causal mean difference</strong> as <span class="math display">\[d = \mathbb{E}(\hat{Y} | Z=1, X=x) - \mathbb{E}(\hat{Y} | Z=0, X=x).\]</span> and the <strong>observed mean difference</strong> as <span class="math display">\[d = \mathbb{E}(Y | Z=1, X=x) - \mathbb{E}(Y | Z=0, X=x).\]</span> Calculating the mean difference is a popular way to establish disparate treatment in an algorithmic decision process because it exposes differing treatment of individuals based on protected class membership.</p>
</section>
</section>
<section id="sec_BalErr" class="level2" data-number="3.2">
<h2 data-number="3.2"><span class="header-section-number">3.2</span> Comparing errors</h2>
<p>In this section we learn about fairness criteria which seek to compare model errors across groups, rather than outcomes. A fundamental assumption here is that the training data is fair; it represents the the ground truth; the target variable is the that which we wish to affect, the data is accurate and representative of the population, and include the features related related to the target. Assuming we have said data, under these criteria, for our model to be fair, we require the errors, not only to be sufficiently small, but also distributed similarly for different subgroups of the population (defined by the values of the sensitive features). Expressed differently, we want the errors to be independent of protected characteristics, that is, <span class="math inline">\((\hat{Y}-Y) \bot Z\)</span>. We discussed earlier in the chapter how independence and twin test constraints have been interpreted as avoiding disparate impact and disparate treatment respectively. Analogously, criteria on model errors have been described as avoiding <strong>disparate mistreatment</strong><span class="citation" data-cites="DispMistreat"><a href="#ref-DispMistreat" role="doc-biblioref">[53]</a></span><span class="marginnote"><span id="ref-DispMistreat" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[53] </span><span class="csl-right-inline">M. B. Zafar, I. Valera, M. Gomez Rodriguez, and K. P. Gummadi, <span>“Fairness beyond disparate treatment &amp; disparate impact,”</span> <em>Proceedings of the 26th International Conference on World Wide Web</em>, 2017, doi: <a href="https://doi.org/10.1145/3038912.3052660">10.1145/3038912.3052660</a>.</span>
</span>
</span>.</p>
<section id="regression" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1"><span class="header-section-number">3.2.1</span> Regression</h3>
<p>Once again, relaxation of this criterion compares the mean error for the groups (rather than comparing the full distributions). <strong>Balanced residuals</strong><span class="citation" data-cites="BalRes"><a href="#ref-BalRes" role="doc-biblioref">[54]</a></span><span class="marginnote"><span id="ref-BalRes" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[54] </span><span class="csl-right-inline">T. Calders, A. Karim, F. Kamiran, W. Ali, and X. Zhang, <span>“Controlling attribute effect in linear regression,”</span> 2013. doi: <a href="https://doi.org/10.1109/ICDM.2013.114">10.1109/ICDM.2013.114</a>.</span>
</span>
</span> takes the difference of the mean errors as a measure of fairness: <span class="math display">\[d_{\text{err}} = \mathbb{E}(\hat{Y} - Y | Z=1) - \mathbb{E}(\hat{Y} - Y | Z=0).\]</span> This can be calculated for <span class="math inline">\(n=n_0+n_1\)</span> data points as, <span class="math display">\[d_{\text{err}} = \frac{1}{n_0}\sum_{i|z_i=0}(y_i-\hat{y}_i)
               - \frac{1}{n_1}\sum_{i|z_i=1}(y_i-\hat{y}_i).\]</span> Here <span class="math inline">\(d_{\text{err}}=0\)</span> would be considered fair.</p>
</section>
<section id="classification" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2"><span class="header-section-number">3.2.2</span> Classification</h3>
<p>For a classification problem the most obvious relaxation would be to ensure equal error rates (or equivalently accuracy) for all groups. As an example, recall the project Gender Shades we discussed in section <a href="#sec_harms" data-reference-type="ref" data-reference="sec_harms">1.5</a>, that audited several commercial gender classification packages measured their accuracy for different protected groups. To derive a measure of fairness from this criterion we could (as before) take the difference, or the ratio. The <strong>error rate difference</strong> is given by, <span class="math display">\[d_{\text{err}} = \mathbb{P}(\hat{Y}\neq Y | Z=1) - \mathbb{P}(\hat{Y}\neq Y | Z=0).\]</span> Again here <span class="math inline">\(d_{\text{err}}=0\)</span> would be considered fair. <strong>error rate ratio</strong> is given by <span class="math display">\[r_{\text{err}} = \frac{\mathbb{P}(\hat{Y}\neq Y | Z=1)}
                      {\mathbb{P}(\hat{Y}\neq Y | Z=0)}\]</span> in which case <span class="math inline">\(r_{\text{err}}=1\)</span> would be considered fair.</p>
<div id="tbl:CMErrMetrics">
<table>
<caption>Table 3.2: Summary of error rate types for a binary classifier</caption>
<thead>
<tr class="header">
<th style="text-align: center;" colspan="2"></th>
<th style="text-align: center;" colspan="2">Ground Truth</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;" colspan="2"></td>
<td style="text-align: center;"><span class="math inline">\(y=1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(y=0\)</span></td>
<td style="text-align: center;">Error Rate Type</td>
</tr>
<tr class="even">
<td style="text-align: center;" rowspan="4">Prediction</td>
<td style="text-align: center;" rowspan="2"><span class="math inline">\(\hat{y}=1\)</span></td>
<td style="text-align: center;" rowspan="2">True Positive</td>
<td style="text-align: center;" rowspan="2">False Positive<br>Type I Error</td>
<td style="text-align: center;" rowspan="2">False Discovery Rate<br><span class="math inline">\(\mathbb{P}(\hat{y}\neq y|\hat{y}=1)\)</span></td>
</tr>
<tr class="odd">
</tr>
<tr class="even">
<td style="text-align: center;" rowspan="2"><span class="math inline">\(\hat{y}=0\)</span></td>
<td style="text-align: center;" rowspan="2">False Negative<br>Type II Error</td>
<td style="text-align: center;" rowspan="2">True Negative</td>
<td style="text-align: center;" rowspan="2">False Omission Rate<br><span class="math inline">\(\mathbb{P}(\hat{y}\neq y|\hat{y}=0)\)</span></td>
</tr>
<tr class="odd">
</tr>
<tr class="even">
<td style="text-align: center;" colspan="2" rowspan="2">Error Rate Type</td>
<td style="text-align: center;" rowspan="2">False Negative Rate<br><span class="math inline">\(\mathbb{P}(\hat{y}\neq y|y=1)\)</span></td>
<td style="text-align: center;" rowspan="2">False Positive Rate<br><span class="math inline">\(\mathbb{P}(\hat{y}\neq y|y=0)\)</span></td>
<td style="text-align: center;" rowspan="2">Error Rate<br><span class="math inline">\(\mathbb{P}(\hat{y}\neq y)\)</span></td>
</tr>
<tr class="odd">
</tr>
</tbody>
</table>
</div>
<p>A binary classification model can make two different types of errors (false positives and false negatives), the costs of which will typically not be equal. Table <a href="#tbl:CMErrMetrics" data-reference-type="ref" data-reference="tbl:CMErrMetrics">3.2</a> summarises terminology for the different types of error rates for a binary classification model.</p>
<div id="tbl:CMPerfMetrics">
<table>
<caption>Table 3.3: Summary of performance metrics for a binary classifier</caption>
<thead>
<tr class="header">
<th style="text-align: center;" colspan="2"></th>
<th style="text-align: center;" colspan="2">Ground Truth</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;" colspan="2"></td>
<td style="text-align: center;"><span class="math inline">\(y=1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(y=0\)</span></td>
<td style="text-align: center;">Metric</td>
</tr>
<tr class="even">
<td style="text-align: center;" rowspan="4">Prediction</td>
<td style="text-align: center;" rowspan="2"><span class="math inline">\(\hat{y}=1\)</span></td>
<td style="text-align: center;" rowspan="2">True Positive</td>
<td style="text-align: center;" rowspan="2">False Positive<br>Type I Error</td>
<td style="text-align: center;" rowspan="2">Positive Predictive Value<sup>a</sup><br><span class="math inline">\(\mathbb{P}(\hat{y}=y|\hat{y}=1)\)</span></td>
</tr>
<tr class="odd">
</tr>
<tr class="even">
<td style="text-align: center;" rowspan="2"><span class="math inline">\(\hat{y}=0\)</span></td>
<td style="text-align: center;" rowspan="2">False Negative<br>Type II Error</td>
<td style="text-align: center;" rowspan="2">True Negative</td>
<td style="text-align: center;" rowspan="2">Negative Predictive Value<br><span class="math inline">\(\mathbb{P}(\hat{y}=y|\hat{y}=0)\)</span></td>
</tr>
<tr class="odd">
</tr>
<tr class="even">
<td style="text-align: center;" colspan="2" rowspan="2">Metric</td>
<td style="text-align: center;" rowspan="2">True Positive Rate<sup>b</sup><br><span class="math inline">\(\mathbb{P}(\hat{y}=y|y=1)\)</span></td>
<td style="text-align: center;" rowspan="2">True Negative Rate<br><span class="math inline">\(\mathbb{P}(\hat{y}=y|y=0)\)</span></td>
<td style="text-align: center;" rowspan="2">Accuracy<br><span class="math inline">\(\mathbb{P}(\hat{y}=y)\)</span></td>
</tr>
<tr class="odd">
</tr>
</tbody>
</table>
</div>
<div class="tablenotes">
<p><sup>a</sup> Positive Predictive Value = Precision</p>
<p><sup>b</sup> True Positive Rate = Recall</p>
</div>
<p>Fairness criteria that compare errors (or equivalently performance metrics - see Table <a href="#tbl:CMPerfMetrics" data-reference-type="ref" data-reference="tbl:CMPerfMetrics">3.3</a> for a summary) across groups can be broken down into two conditional independence constraints on the joint distributions of the sensitive features, <span class="math inline">\(Z\)</span>, the target feature <span class="math inline">\(Y\)</span> and predicted target <span class="math inline">\(\hat{Y}\)</span>. These can be described as <strong>separation</strong> (<span class="math inline">\(\hat{Y}\bot Z|Y\)</span>) and <strong>sufficiency</strong><span class="citation" data-cites="FairMLBook"><a href="#ref-FairMLBook" role="doc-biblioref">[55]</a></span><span class="marginnote"><span id="ref-FairMLBook" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[55] </span><span class="csl-right-inline">S. Barocas, M. Hardt, and A. Narayanan, <em>Fairness and machine learning</em>. fairmlbook.org, 2019.</span>
</span>
</span> (<span class="math inline">\(Y\bot Z|\hat{Y}\)</span>). Each of these criteria can be defined as a conditional independence constraint on the joint distributions of the sensitive features, <span class="math inline">\(Z\)</span>, the target feature <span class="math inline">\(Y\)</span> and predicted target <span class="math inline">\(\hat{Y}\)</span>. Separation requires equal error (false negative and false positive) rates along the columns (conditioning on <span class="math inline">\(Y\)</span>), while sufficiency requires equal error (false discovery and false omission) rates along the rows (conditioning on <span class="math inline">\(\hat{Y}\)</span>) of the confusion matrix (Table <a href="#tbl:CMErrMetrics" data-reference-type="ref" data-reference="tbl:CMErrMetrics">3.2</a>).</p>
<section id="separation" class="level4 unnumbered">
<h4 class="unnumbered">Separation</h4>
<p>Separation requires the predicted target variable to be independent of the sensitive feature, conditioned on the target variable, that is, <span class="math inline">\(\hat{Y} \bot (Z|Y)\)</span>. We can say that the predicted target <span class="math inline">\(\hat{Y}\)</span>, is ‘separated’ from the sensitive feature <span class="math inline">\(Z\)</span>, by the target variable <span class="math inline">\(Y\)</span>. The corresponding graphical model for separation criteria is shown in Figure <a href="#fig:separation" data-reference-type="ref" data-reference="fig:separation">3.2</a>.</p>
<figure>
<img src="03_GroupFairness/figures/Fig_Separation.png" id="fig:separation" style="width:50.0%" alt="Figure 3.2: Graphical model for separation." /><figcaption aria-hidden="true">Figure 3.2: Graphical model for separation.</figcaption>
</figure>
<p>Essentially we are saying that for a fixed value of the target variable, there should be no difference in the distribution of the predicted target variable, for different values of the sensitive feature. That is, <span class="math display">\[\mathbb{P}(\hat{y}|y, z) = \mathbb{P}(\hat{y}|y).\]</span> Unlike independence, separation, allows for dependence between the predicted target variable and the sensitive feature but only to the extent that it exists between the actual target variable and the sensitive feature.</p>
<p>Once again let’s take the simplest example of discrete binary classifier where we have a single sensitive binary feature. We can write this requirement (most well known as <strong>equalised odds</strong><span class="citation" data-cites="EqOfOp"><a href="#ref-EqOfOp" role="doc-biblioref">[56]</a></span><span class="marginnote"><span id="ref-EqOfOp" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[56] </span><span class="csl-right-inline">M. Hardt, E. Price, and N. Srebro, <span>“Equality of opportunity in supervised learning.”</span> 2016.Available: <a href="https://arxiv.org/abs/1610.02413">https://arxiv.org/abs/1610.02413</a></span>
</span>
</span>) as two conditions, <span class="math display">\[\begin{aligned}
\mathbb{P}(\hat{Y}=1 | Z=1, Y=1) &amp; = \mathbb{P}(\hat{Y}=1 | Z=0, Y=1), \\
\mathbb{P}(\hat{Y}=1 | Z=1, Y=0) &amp; = \mathbb{P}(\hat{Y}=1 | Z=0, Y=0).\end{aligned}\]</span> Recall that <span class="math inline">\(\mathbb{P}(\hat{Y}=1 | Y=1)\)</span> is the true positive rate (<span class="math inline">\(TPR\)</span>) of the classifier and <span class="math inline">\(\mathbb{P}(\hat{Y}=1 | Y=0)\)</span> is the false positive rate (<span class="math inline">\(FPR\)</span>). We see then that separation requires the true positive rate, and the false positive rate, to be the same for all values of the sensitive feature. Note that the true positive rate is equal if and only if the false negative rate is equal, so thinking in terms of error metrics only, separation requires the false negative and false positive rates to be equal for all values of the sensitive feature.</p>
<p>Two related metrics are the average odds difference and average odds error. The <strong>average odds difference</strong> measures the magnitude of unfairness as the average of the difference in true positive rate and false positive rate, that is, <span class="math display">\[d_{\text{av-odds}} = \frac{1}{2}
[ TPR_{Z=0} - TPR_{Z=1} + FPR_{Z=0} - FPR_{Z=1} ].\]</span> The <strong>average odds error</strong> measures the magnitude of unfairness as the average of the absolute difference in true positive rate and false positive rate, that is, <span class="math display">\[d_{\text{av-odds-err}} = \frac{1}{2}
[ |TPR_{Z=0} - TPR_{Z=1}| + |FPR_{Z=0} - FPR_{Z=1}| ].\]</span> A relaxed version of equalised odds, called <strong>equal opportunity</strong><span class="citation" data-cites="EqOfOp"><a href="#ref-EqOfOp" role="doc-biblioref">[56]</a></span>, requires only the true positive rates to be the same across all groups (assuming a positive prediction is the more advantageous ourcome). A metric which uses this as a criterion to measure unfairness is <strong>equal opportunity difference</strong> which takes the difference in true positive rates across groups, that is, <span class="math display">\[d_{\text{eq-op}} = TPR_{Z=0} - TPR_{Z=1}.\]</span></p>
<div class="lookbox">
<p><strong>Exercise: Fair equality of opportunity</strong></p>
<p>Can you see how the metric <em>equal opportunity</em> relates to the second principle of justice as fairness (Fair equality of opportunity) discussed in section <a href="#sec_FairnessJustice" data-reference-type="ref" data-reference="sec_FairnessJustice">1.2</a>?</p>
</div>
</section>
<section id="sufficiency" class="level4 unnumbered">
<h4 class="unnumbered">Sufficiency</h4>
<p>Sufficiency requires the sensitive feature <span class="math inline">\(Z\)</span> and target variable <span class="math inline">\(Y\)</span> to be independent, conditional on the predicted target variable <span class="math inline">\(\hat{Y}\)</span>, that is, <span class="math inline">\(Y \bot (Z|\hat{Y})\)</span>. We can say that the predicted target <span class="math inline">\(\hat{Y}\)</span> is ‘sufficient’ for the sensitive feature <span class="math inline">\(Z\)</span>. That is to say, given <span class="math inline">\(\hat{Y}\)</span>, <span class="math inline">\(Z\)</span> provides no additional information. The corresponding graphical model for sufficiency criteria is shown in Figure <a href="#fig:sufficiency" data-reference-type="ref" data-reference="fig:sufficiency">3.3</a>.</p>
<figure>
<img src="03_GroupFairness/figures/Fig_sufficiency.png" id="fig:sufficiency" style="width:50.0%" alt="Figure 3.3: Graphical model for sufficiency." /><figcaption aria-hidden="true">Figure 3.3: Graphical model for sufficiency.</figcaption>
</figure>
<p>Comparing sufficiency to separation we note that <span class="math inline">\(Y\)</span> and <span class="math inline">\(\hat{Y}\)</span> are reversed in the graphical model and conditional independence constraint. It should hopefully be straightforward to see then that sufficiency requires the false omission rate and false discovery rate (see Table <a href="#tbl:CMErrMetrics" data-reference-type="ref" data-reference="tbl:CMErrMetrics">3.2</a>) to be equal across protected groups.</p>
<div class="lookbox">
<p><strong>Exercise: Sufficiency</strong></p>
<p>Show that sufficiency is satisfied if and only if the false omission rate and false discovery rate are equal for all groups. <a href="#GF_Suff">Solution</a> in appendix <a href="#sec_app_GFSolutions" data-reference-type="ref" data-reference="sec_app_GFSolutions">D.1</a>.</p>
</div>
</section>
<section id="comparing-group-fairness-metrics" class="level4 unnumbered">
<h4 class="unnumbered">Comparing group fairness metrics</h4>
<p>There are some nice properties of separation and sufficiency criteria. Note that unlike criteria comparing outcomes they do not preclude the theoretically ‘perfect’ solution, <span class="math inline">\(\hat{Y}=Y\)</span>. The criteria also preclude large differences in error rates for different groups that are typical when disadvantaged classes are minorities suffering from low support. It’s worth reiterating that unlike independence, separation and sufficiency criteria assume that the relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(Z\)</span> prescribed by the training data is fair. In such cases, error comparison criteria allow flexibility in the choice which types of errors are important to equalize, based on the cost and our values. For example, in pretrial risk assessment we might choose to prioritise ensuring equal false positive rates if we believe that it is preferable to set free a guilty defendant than incarcerate an innocent one. As another example, let’s take the infamous <a href="https://en.wikipedia.org/wiki/Stop-and-frisk_in_New_York_City">NYPD stop-and-frisk program</a> where pedestrians were stopped, interrogated and searched on ‘reasonable’ suspicion of carrying contraband. In this case we might want to ensure false discovery rates are equal across groups to ensure we are not disproportionately targeting particular minority groups.</p>
<div class="lookbox">
<p><strong>Exercise: Stop-and-frisk</strong></p>
<ul>
<li><p>Why might we choose to compare false discovery rates for stop-and-frisk, rather than say false omission, false negative or false positive rates?</p></li>
<li><p>Is it fair if false discovery rates are similar?</p></li>
<li><p>How might we go about measuring the false omission rate if we wanted to compare them?</p></li>
</ul>
</div>
<p>Of our two fairness criteria, separation and sufficiency, the latter imposes a weaker constraint on our model. To understand why, we explore another interpretation of sufficiency which intuitively explains why it might be satisfied implicitly through the training process<span class="citation" data-cites="ImplicitFairness"><a href="#ref-ImplicitFairness" role="doc-biblioref">[57]</a></span><span class="marginnote"><span id="ref-ImplicitFairness" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[57] </span><span class="csl-right-inline">L. T. Liu, M. Simchowitz, and M. Hardt, <span>“The implicit fairness criterion of unconstrained learning.”</span> 2019.Available: <a href="https://arxiv.org/abs/1808.10013">https://arxiv.org/abs/1808.10013</a></span>
</span>
</span>. Let us look at sufficiency criteria in terms of the classification score <span class="math inline">\(P\)</span>, <span class="math display">\[\mathbb{P}(Y=1 | P=p, Z=1) = \mathbb{P}(Y=1 | P=p, Z=0) \quad \forall \, p\]</span> We say that a classifier score is calibrated if <span class="math display">\[\mathbb{P}(Y=1 | P=p) = p \quad \forall \, p.\]</span> Essentially, this is the requirement that the proportion of data points assigned the score <span class="math inline">\(p\)</span>, which did in fact have a positive outcome <span class="math inline">\(Y=1\)</span>, should be equal to the score <span class="math inline">\(p\)</span>. The score <span class="math inline">\(p\)</span> can then be interpreted, at the population level, as the probability that the a positive prediction <span class="math inline">\(\hat{Y}=1\)</span> would be correct<span class="sidenote-wrapper"><label for="sn-6" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-6" class="margin-toggle"/><span class="sidenote">For the score to be interpretable as this probability at the individual level, we would need to satisfy the stronger criteria <span class="math inline">\(P=\mathbb{E}[Y|X]\)</span>.<br />
<br />
</span></span>.</p>
<p>From the definitions above we can see that if our classifier scores are calibrated for all groups, sufficiency is automatically satisfied. If our model satisfies sufficiency but not calibration by group, we can calibrate our model score through a simple transformation. We simply pick a value for <span class="math inline">\(Z\)</span>, <span class="math inline">\(Z=1\)</span> say, and then calculate the mapping, <span class="math display">\[\mathbb{P}(Y=1|P=p, Z=1) = f(p).\]</span> We then transform all our scores to new scores (which satisfy calibration by group) by applying the inverse mapping <span class="math inline">\(f^{-1}(P)\)</span>. The resulting model is both sufficient and the model score is calibrated.</p>
</section>
</section>
</section>
<section id="sec_Impossible" class="level2" data-number="3.3">
<h2 data-number="3.3"><span class="header-section-number">3.3</span> Incompatibility between fairness criteria</h2>
<p>So far in this chapter we have learned a range of different group fairness criteria and seen how each of them can be viewed as imposing different constraints on the joint distributions of our variables <span class="math inline">\(X\)</span>, <span class="math inline">\(Z\)</span>, <span class="math inline">\(Y\)</span> and <span class="math inline">\(\hat{Y}\)</span>. In this section we will prove that these fairness criteria can be restrictive enough to mean that satisfying more than one of them is impossible, except in some degenerate cases. For a useful recap of the rules of probability (which we will use in our proofs), see in Appendix <a href="#app_ProbRules" data-reference-type="ref" data-reference="app_ProbRules">C</a>.</p>
<section id="independence-versus-sufficiency" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1"><span class="header-section-number">3.3.1</span> Independence versus Sufficiency</h3>
<div class="lookbox">
<p><strong>Independence versus Sufficiency</strong></p>
<p>Independence (<span class="math inline">\(Z \bot \hat{Y}\)</span>) and sufficiency (<span class="math inline">\(Z \bot Y | \hat{Y}\)</span>) can only be simultaneously satisfied if the sensitive feature, <span class="math inline">\(Z\)</span> and the target variable <span class="math inline">\(\hat{Y}\)</span> are independent (<span class="math inline">\(Z \bot Y\)</span>).</p>
</div>
<p>To prove this we consider the conditional distribution <span class="math inline">\(Z|Y,\hat{Y}\)</span>.</p>
<div id="eq:IndSuf1">
<table style="width:99%;">
<colgroup>
<col style="width: 90%" />
<col style="width: 9%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[\begin{aligned}
\textrm{Independence: } Z \bot \hat{Y} \quad
&amp; \Rightarrow \quad \mathbb{P}(z|y,\hat{y}) = \mathbb{P}(z|y) \nonumber\\
\textrm{Product rule} \quad
&amp; \Rightarrow \quad \mathbb{P}(z|y) = \frac{\mathbb{P}(z,y)}{\mathbb{P}(y)}\nonumber\\
&amp; \Rightarrow \quad \mathbb{P}(z|y,\hat{y}) = \frac{\mathbb{P}(z,y)}{\mathbb{P}(y)}.\end{aligned}\]</span></td>
<td style="text-align: right;">(3.5)</td>
</tr>
</tbody>
</table>
</div>
<p>Applying Sufficiency, followed by independence gives,</p>
<div id="eq:IndSuf2">
<table style="width:99%;">
<colgroup>
<col style="width: 90%" />
<col style="width: 9%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[\begin{aligned}
\textrm{Sufficiency: } Z \bot Y | \hat{Y} \quad
&amp; \Rightarrow \quad \mathbb{P}(z|y,\hat{y}) = \mathbb{P}(z|\hat{y})\nonumber\\
\textrm{Independence: } Z \bot \hat{Y} \quad 
&amp; \Rightarrow \quad \mathbb{P}(z|\hat{y}) = \mathbb{P}(z)\nonumber\\
&amp; \Rightarrow \quad \mathbb{P}(z|y,\hat{y}) = \mathbb{P}(z).\end{aligned}\]</span></td>
<td style="text-align: right;">(3.6)</td>
</tr>
</tbody>
</table>
</div>
<p>Equating (3.5) and (3.6) and rearranging gives, <span class="math display">\[\mathbb{P}(z,y) = \mathbb{P}(z)\mathbb{P}(y).\]</span> Thus, <span class="math inline">\(Z\)</span> and <span class="math inline">\(Y\)</span> must be independent.</p>
</section>
<section id="independence-versus-separation" class="level3" data-number="3.3.2">
<h3 data-number="3.3.2"><span class="header-section-number">3.3.2</span> Independence versus Separation</h3>
<div class="lookbox">
<p><strong>Independence versus Separation</strong></p>
<p>In the case that <span class="math inline">\(Y\)</span> is binary, independence (<span class="math inline">\(Z \bot \hat{Y}\)</span>) and separation (<span class="math inline">\(Z \bot \hat{Y} | Y\)</span>) criteria can only be simultaneously satisfied if either <span class="math inline">\(\hat{Y} \bot Y\)</span> or <span class="math inline">\(Y \bot Z\)</span>.</p>
</div>
<p>To prove this we consider the distribution of <span class="math inline">\(\hat{Y}\)</span>.</p>
<div id="eq:IndSep1">
<table style="width:99%;">
<colgroup>
<col style="width: 90%" />
<col style="width: 9%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[\begin{aligned}
\textrm{Sum rule:} \quad &amp; \Rightarrow \quad \mathbb{P}(\hat{y})
= \sum_{y\in\mathcal{Y}}  \mathbb{P}(\hat{y}, y).\nonumber\\
\textrm{Product rule} \quad &amp; \Rightarrow \quad \mathbb{P}(\hat{y})
= \sum_{y\in\mathcal{Y}} \mathbb{P}(\hat{y}|y) \mathbb{P}(y). \end{aligned}\]</span></td>
<td style="text-align: right;">(3.7)</td>
</tr>
</tbody>
</table>
</div>
<div id="eq:IndSep2">
<table style="width:99%;">
<colgroup>
<col style="width: 90%" />
<col style="width: 9%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[\begin{aligned}
\textrm{Conditioning on }Z \quad \Rightarrow \quad \mathbb{P}(\hat{y}|z)
= \sum_{y\in\mathcal{Y}} \mathbb{P}(\hat{y}|y, z) \mathbb{P}(y|z).\nonumber\\
\textrm{Independence: } \hat{Y} \bot Z \quad \Rightarrow \quad \mathbb{P}(\hat{y})
= \sum_{y\in\mathcal{Y}} \mathbb{P}(\hat{y}|y) \mathbb{P}(y|z). \end{aligned}\]</span></td>
<td style="text-align: right;">(3.8)</td>
</tr>
</tbody>
</table>
</div>
<p>Equating (3.7) and (3.8) and rearranging gives,</p>
<div id="eq:IndSep3">
<table style="width:99%;">
<colgroup>
<col style="width: 90%" />
<col style="width: 9%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
\sum_{y\in\mathcal{Y}} \mathbb{P}(\hat{y}|y) [\mathbb{P}(y)-\mathbb{P}(y|z)] = 0\]</span></td>
<td style="text-align: right;">(3.9)</td>
</tr>
</tbody>
</table>
</div>
<p>For binary <span class="math inline">\(Y\)</span>, <span class="math inline">\(\mathcal{Y}=\{0,1\}\)</span>. Denoting <span class="math inline">\(\mathbb{P}(y)=p_y\)</span> and <span class="math inline">\(\mathbb{P}(y|z) = q_y\)</span>, then <span class="math inline">\(p_1 = 1-p_0\)</span> and <span class="math inline">\(q_1 = 1-q_0\)</span>. Substituting into (3.9) gives, <span class="math display">\[\begin{aligned}
&amp; \phantom{[}\mathbb{P}(\hat{y}|Y=0)(p_0-q_0)+\mathbb{P}(\hat{y}|Y=1)[1-p_0-(1-q_0)] = 0 \\
\Leftrightarrow \quad &amp; [\mathbb{P}(\hat{y}|Y=0)-\mathbb{P}(\hat{y}|Y=1)](p_0-q_0) = 0\end{aligned}\]</span> which is true if and only if, <span class="math display">\[\begin{aligned}
&amp;\textrm{either }
  &amp; \mathbb{P}(\hat{y}|Y=0) = \mathbb{P}(\hat{y}|Y=1) \quad
    &amp; \Leftrightarrow \quad \hat{Y} \bot Y,\\
&amp; \textrm{or }
  &amp; p_0=q_0 \quad \Leftrightarrow \quad \mathbb{P}(Y=0) = \mathbb{P}(Y=0|z) \quad
    &amp; \Leftrightarrow \quad Y \bot Z.\end{aligned}\]</span></p>
</section>
<section id="separation-versus-sufficiency" class="level3" data-number="3.3.3">
<h3 data-number="3.3.3"><span class="header-section-number">3.3.3</span> Separation versus Sufficiency</h3>
<div class="lookbox">
<p><strong>Separation versus Sufficiency I</strong></p>
<p>In the case where all events in the joint distribution of <span class="math inline">\(Z\)</span>, <span class="math inline">\(Y\)</span> and <span class="math inline">\(\hat{Y}\)</span> have non zero probability, separation (<span class="math inline">\(Z \bot \hat{Y} | Y\)</span>) and sufficiency (<span class="math inline">\(Z \bot Y | \hat{Y}\)</span>) can only be simultaneously be satisfied if the sensitive feature, <span class="math inline">\(Z\)</span> is independent of both the target variable <span class="math inline">\(Y\)</span> and the predicted target <span class="math inline">\(\hat{Y}\)</span>, that is if <span class="math inline">\(Z \bot Y\)</span> and <span class="math inline">\(Z \bot \hat{Y}\)</span>.</p>
</div>
<p>To prove this we consider the conditional distribution <span class="math inline">\(\mathbb{P}(z|y,\hat{y})\)</span>.</p>
<div id="eq:SepSuf1">
<table style="width:99%;">
<colgroup>
<col style="width: 90%" />
<col style="width: 9%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[\begin{aligned}
\textrm{Separation: } Z \bot \hat{Y} | Y \quad
&amp; \Rightarrow \quad \mathbb{P}(z|y,\hat{y}) = \mathbb{P}(z|y) \nonumber\\
\textrm{Sufficiency: } Z \bot Y | \hat{Y} \quad 
&amp; \Rightarrow \quad \mathbb{P}(z|y,\hat{y}) = \mathbb{P}(z|\hat{y}) \nonumber\\
&amp; \Rightarrow \quad \mathbb{P}(z|y) = \mathbb{P}(z|\hat{y}).\end{aligned}\]</span></td>
<td style="text-align: right;">(3.10)</td>
</tr>
</tbody>
</table>
</div>
<div id="eq:SepSuf2">
<table style="width:99%;">
<colgroup>
<col style="width: 90%" />
<col style="width: 9%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[\begin{aligned}
\textrm{Product rule: } \quad\phantom{\Rightarrow} \mathbb{P}(z,y) &amp; = \mathbb{P}(z|y) \mathbb{P}(y)\nonumber\\
(3.10) \qquad\quad \Rightarrow \quad \mathbb{P}(z,y) &amp; = \mathbb{P}(z|\hat{y}) \mathbb{P}(y).\end{aligned}\]</span></td>
<td style="text-align: right;">(3.11)</td>
</tr>
</tbody>
</table>
</div>
<p><span class="math display">\[\begin{aligned}
\textrm{Sum rule: } \quad \phantom{\Rightarrow}\mathbb{P}(z) &amp; = \sum_{y\in\mathcal{Y}} \mathbb{P}(z,y)\\
(3.11) \quad\,\, \Rightarrow \quad
\mathbb{P}(z) &amp; = \sum_{y\in\mathcal{Y}} \mathbb{P}(z|\hat{y}) \mathbb{P}(y)\end{aligned}\]</span> If all events have non-zero probability, we can move <span class="math inline">\(\mathbb{P}(z|\hat{y})\)</span> outside of the summation,</p>
<div id="eq:SepSuf3">
<table style="width:99%;">
<colgroup>
<col style="width: 90%" />
<col style="width: 9%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
\mathbb{P}(z) = \mathbb{P}(z|\hat{y})\]</span></td>
<td style="text-align: right;">(3.12)</td>
</tr>
</tbody>
</table>
</div>
<p>Thus showing that <span class="math inline">\(Z\)</span> and <span class="math inline">\(\hat{Y}\)</span> must be independent. Equating (3.10) and (3.12) shows that <span class="math inline">\(Z\)</span> and <span class="math inline">\(Y\)</span> must also be independent.</p>
<div class="lookbox">
<p><strong>Separation versus Sufficiency II</strong></p>
<p>In the case where <span class="math inline">\(Y\)</span> is binary, separation and sufficiency can only be satisfied simultaneously if the sensitive feature is independent of the target variable, or the model has an accuracy of 100% (<span class="math inline">\(\hat{Y}=Y\)</span>) or 0% (<span class="math inline">\(\hat{Y}=1-Y\)</span>).</p>
</div>
<p>Consider the case where <span class="math inline">\(Y\)</span> is binary. Separation requires all groups to have the same true positive rate (recall or <span class="math inline">\(TPR\)</span>) and the same false positive rate (<span class="math inline">\(FPR\)</span>). On the other hand, sufficiency requires all groups to have the same positive predictive value (precision or <span class="math inline">\(PPV\)</span>) and the same negative predictive value (<span class="math inline">\(NPV\)</span>). A problem is evident at this point. For a fixed number of data points, the confusion matrix for a binary classifier only has three degrees of freedom but satisfying both separation and sufficiency introduces four constraints which requires four degrees of freedom in order be able to satisfy them. We can write the positive and negative predictive values in terms of the true positive and false positive rates as follows:</p>
<div id="eq:PPV">
<table style="width:99%;">
<colgroup>
<col style="width: 90%" />
<col style="width: 9%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
PPV = \frac{p TPR}{p TPR + (1-p)FPR}\]</span></td>
<td style="text-align: right;">(3.13)</td>
</tr>
</tbody>
</table>
</div>
<p>and</p>
<div id="eq:NPV">
<table style="width:99%;">
<colgroup>
<col style="width: 90%" />
<col style="width: 9%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
NPV = \frac{(1-p)(1-FPR)}{p(1-TPR) + (1-p)(1-FPR)}\]</span></td>
<td style="text-align: right;">(3.14)</td>
</tr>
</tbody>
</table>
</div>
<p>where <span class="math inline">\(p=\mathbb{P}(Y=1)\)</span>.</p>
<div class="lookbox">
<p><strong>Exercise: Predictive values</strong></p>
<p>Prove the results given in equations (3.13) and (3.14). Refer to Table <a href="#tbl:CMPerfMetrics" data-reference-type="ref" data-reference="tbl:CMPerfMetrics">3.3</a> for a summary of model performance metrics for a binary classifier. <a href="#GF_PredVal">Solution</a> in appendix <a href="#sec_app_GFSolutions" data-reference-type="ref" data-reference="sec_app_GFSolutions">D.1</a>.</p>
</div>
<p>Denote <span class="math inline">\(p_z=\mathbb{P}(Y=1|Z=z)\)</span> then we can show from equations (3.13) and (3.14) that for any distinct pair of groups <span class="math inline">\(Z=a\)</span> and <span class="math inline">\(Z=b\)</span> for both separation and sufficiency to hold we must have</p>
<div id="eq:PRT1">
<table style="width:99%;">
<colgroup>
<col style="width: 90%" />
<col style="width: 9%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
FPR (p_a-p_b) TPR = 0\]</span></td>
<td style="text-align: right;">(3.15)</td>
</tr>
</tbody>
</table>
</div>
<p>and</p>
<div id="eq:PRT2">
<table style="width:99%;">
<colgroup>
<col style="width: 90%" />
<col style="width: 9%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
(1-FPR) (p_a-p_b) (1-TPR) = 0\]</span></td>
<td style="text-align: right;">(3.16)</td>
</tr>
</tbody>
</table>
</div>
<p>respectively.</p>
<div class="lookbox">
<p><strong>Exercise: Separation versus sufficiency</strong></p>
<p>Show that for separation and sufficiency to hold equations (3.15) and (3.16) must hold for for any pair of groups <span class="math inline">\(Z=a\)</span> and <span class="math inline">\(Z=b\)</span>. <a href="#GF_SepVsSuff">Solution</a> in appendix <a href="#sec_app_GFSolutions" data-reference-type="ref" data-reference="sec_app_GFSolutions">D.1</a>.</p>
</div>
<p>Equations (3.15) and (3.16) can only be simultaneously satisfied in 3 cases:</p>
<ol>
<li><p><span class="math inline">\(p_a=p_b \, \forall \, a, b\)</span> in which case <span class="math inline">\(Y \bot Z\)</span>,</p></li>
<li><p><span class="math inline">\(FPR=0\)</span> and <span class="math inline">\(TPR=1\)</span> in which case <span class="math inline">\(Y=\hat{Y}\)</span>,</p></li>
<li><p><span class="math inline">\(FPR=1\)</span> and <span class="math inline">\(TPR=0\)</span> in which case <span class="math inline">\(Y=1-\hat{Y}\)</span>.</p></li>
</ol>
</section>
</section>
<section id="summary-2" class="level2 unnumbered">
<h2 class="unnumbered">Summary</h2>
<section id="group-fairness" class="level3 unnumbered">
<h3 class="unnumbered">Group fairness</h3>
<ul>
<li><p>The term group fairness is used to describe a class of metrics that all stem from the same high level idea; the notion that some property should be equally distributed across different subgroups of a population.</p></li>
<li><p>Group fairness metrics are often used to measure discrimination or bias in a given decision process.</p></li>
<li><p>In general group fairness criterion and measures can be derived from independence constraints on the joint distributions of the non-sensitive features <span class="math inline">\(X\)</span>, sensitive features, <span class="math inline">\(Z\)</span>, the target feature <span class="math inline">\(Y\)</span> and predicted target <span class="math inline">\(\hat{Y}\)</span>.</p></li>
<li><p>Group fairness criteria can be broadly classified into two types; those seeking to compare outcomes across groups and those comparing errors.</p></li>
</ul>
<div id="tbl:GFairSumm">
<table>
<caption>Table 3.4: Group fairness metrics summary.</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Comparing</th>
<th style="text-align: center;" colspan="2">Outcomes</th>
<th style="text-align: center;" colspan="2">Errors</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Criterion</td>
<td style="text-align: center;">Independence</td>
<td style="text-align: center;">Twin test</td>
<td style="text-align: center;">Separation</td>
<td style="text-align: center;">Sufficiency</td>
</tr>
<tr class="even">
<td style="text-align: left;">Constraint</td>
<td style="text-align: center;"><span class="math inline">\(Y\bot Z\)</span></td>
<td style="text-align: center;"><span class="math inline">\(Y\bot (Z|X)\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\hat{Y}\bot (Z|Y)\)</span></td>
<td style="text-align: center;"><span class="math inline">\(Y\bot (Z|\hat{Y})\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Preventing</td>
<td style="text-align: center;">Disparate impact</td>
<td style="text-align: center;">Disparate treatment</td>
<td style="text-align: center;" colspan="2">Disparate mistreatment</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="comparing-outcomes" class="level3 unnumbered">
<h3 class="unnumbered">Comparing Outcomes</h3>
<section id="independence-1" class="level4 unnumbered">
<h4 class="unnumbered">Independence</h4>
<ul>
<li><p>Independence is a strong expression of the view that fairness is equality. It might be interpreted as the notion that abilities (or features) in all groups are, or should be, equally distributed; the belief that observed differences in the distributions in training data are a manifestation of unfair discrimination, errors in data collection, or both, rather than inherent differences in the abilities of people belonging to one group or another.</p></li>
<li><p>Independence metrics is they can be evaluated on both the data and the model. A common problem in machine learning is that existing biases in the data can be exaggerated if protected groups are minorities in the population. By comparing independence metrics for the data and with those of our model output we can understand if our model is inadvertently introducing biases that do not originate from the data.</p></li>
<li><p>If the target variable <span class="math inline">\(Y\)</span> and sensitive feature <span class="math inline">\(Z\)</span> are not independent then satisfying independence for your model will not permit the theoretically ‘perfect’ solution <span class="math inline">\(Y = \hat{Y}\)</span>. We would naturally expect that the stronger the relationship between the sensitive feature and target, the greater the trade-off between fairness and utility in satisfying the independence criterion.</p></li>
<li><p>A major shortcoming of independence is that it doesn’t consider that there may be confounding variables. It assumes that all relevant features are held by all protected groups equally and where there are differences it assumes unfairness and passes the task of correcting for it to the decision maker.</p></li>
</ul>
</section>
<section id="the-twin-test" class="level4 unnumbered">
<h4 class="unnumbered">The twin test</h4>
<ul>
<li><p>The twin test has been interpreted as addressing disparate treatment, since it exposes differing treatment of individuals based on protected class membership. In reality, while it would be sufficient to demonstrate disparate treatment, it is not necessary. Using protected features in the algorithm would be enough to result in disparate treatment liability in the US, the impact of using the feature is irrelevant.</p></li>
<li><p>In the case where a decision is made purely on the basis of an algorithm and there are no unobserved variables, we can perform a ‘twin test’ to establish disparate treatment. We conduct a randomized experiment and compute the causal mean difference. If the value is non-zero, we have established the existence of disparate treatment.</p></li>
</ul>
</section>
</section>
<section id="comparing-errors" class="level3 unnumbered">
<h3 class="unnumbered">Comparing errors</h3>
<ul>
<li><p>Criteria comparing errors assume that the relationship between the target variable and sensitive feature prescribed by the training data is fair so only make sense if the target variable is reliable as the ground truth. Under this assumption that our data is fair, for our model to be fair, we require errors to be distributed similarly for different subgroups of the population (defined by the values of sensitive features).</p></li>
<li><p>Ensuring equal errors has been described as avoiding disparate mistreatment.</p></li>
<li><p>For a regression model balanced residuals takes the difference of the mean errors for each group as a measure of fairness.</p></li>
<li><p>For a classification problem we could could use the error rate difference or the error rate ratio as a measure of fairness.</p></li>
<li><p>Unlike criteria comparing outcomes, criteria comparing errors do not preclude the theoretically ‘perfect’ solution, <span class="math inline">\(\hat{Y}=Y\)</span>.</p></li>
</ul>
<section id="separation-1" class="level4 unnumbered">
<h4 class="unnumbered">Separation</h4>
<ul>
<li><p>Separation, allows for dependence between the predicted target variable and the sensitive feature but only to the extent that it exists between the actual target variable and the sensitive feature.</p></li>
<li><p>For a binary classification model, separation requires both the false negative and false positive rates to be equal across groups. This criterion is known as equalised odds</p></li>
<li><p>Equal opportunity criterion requires only the true positive rates to be the same across all groups (assuming a positive prediction is the more advantageous outcome).</p></li>
</ul>
</section>
<section id="sufficiency-1" class="level4 unnumbered">
<h4 class="unnumbered">Sufficiency</h4>
<ul>
<li><p>For a binary classification model, sufficiency requires both the false omission rate and false discovery rates to be equal across protected groups.</p></li>
<li><p>Sufficiency is is a weaker model constraint compared to separation as it is satisfied implicitly through the training process.</p></li>
</ul>
</section>
</section>
<section id="incompatibility-between-fairness-criteria" class="level3 unnumbered">
<h3 class="unnumbered">Incompatibility between fairness criteria</h3>
<ul>
<li><p>Independence (<span class="math inline">\(Z \bot \hat{Y}\)</span>) and sufficiency (<span class="math inline">\(Z \bot Y | \hat{Y}\)</span>) can only be simultaneously be satisfied if the sensitive feature <span class="math inline">\(Z\)</span>, and the target variable <span class="math inline">\(\hat{Y}\)</span>, are independent (<span class="math inline">\(Z \bot Y\)</span>).</p></li>
<li><p>In the case that <span class="math inline">\(Y\)</span> is binary, independence (<span class="math inline">\(Z \bot \hat{Y}\)</span>) and separation (<span class="math inline">\(Z \bot \hat{Y} | Y\)</span>) criteria can only be simultaneously satisfied if either <span class="math inline">\(\hat{Y} \bot Y\)</span> or <span class="math inline">\(Y \bot Z\)</span>.</p></li>
<li><p>Separation (<span class="math inline">\(Z \bot \hat{Y} | Y\)</span>) and sufficiency (<span class="math inline">\(Z \bot Y | \hat{Y}\)</span>) can only be simultaneously be satisfied if the sensitive feature, <span class="math inline">\(Z\)</span> is independent of both the target variable <span class="math inline">\(Y\)</span> and the predicted target <span class="math inline">\(\hat{Y}\)</span>, that is if <span class="math inline">\(Z \bot Y\)</span> and <span class="math inline">\(Z \bot \hat{Y}\)</span>.</p></li>
<li><p>In the case where <span class="math inline">\(Y\)</span> is binary, separation and sufficiency can only be satisfied simultaneously if the sensitive feature is independent of the target variable, or the model has an accuracy of 100% (<span class="math inline">\(\hat{Y}=Y\)</span>) or the model has an accuracy of 0% (<span class="math inline">\(\hat{Y}=1-Y\)</span>).</p></li>
</ul>
</section>
</section>
</section>
<section id="app_AIF360" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">A</span> AIF360</h1>
<section id="app_Install" class="level2" data-number="4.1">
<h2 data-number="4.1"><span class="header-section-number">A.1</span> Installing AIF360</h2>
<ol>
<li><p>In this book we will use Python in Jupyter notebooks from the <a href="https://www.anaconda.com//products/individual">Anaconda Python distribution platform</a>. If you don’t already have it download and install it.</p></li>
<li><p>Create an environment named <code>mbml</code>. Using the command line interface (CLI):</p>
<pre><code>\$ conda create --name mbml python=3.7</code></pre></li>
<li><p>Activate your new environment:</p>
<pre><code>$ conda activate mbml</code></pre></li>
<li><p>This book is a work in progress. As part of analysing the metrics and methods it uses code that is not yet available with the library<span class="sidenote-wrapper"><label for="sn-7" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-7" class="margin-toggle"/><span class="sidenote">If you’re interested, here is the <a href="https://github.com/Trusted-AI/AIF360/pull/214">open pull request</a>.<br />
<br />
</span></span>. Once it is merged, you will just be able to just pip install the aif360 library. Until then you must clone this <a href="https://github.com/leenamurgai/AIF360">fork of AIF360</a>:</p>
<pre><code>$ git clone https://github.com/leenamurgai/AIF360.git</code></pre></li>
<li><p>Download the notebook <code>mbml_german.ipynb</code> from <a href="https://git.manning.com/agileauthor/murgai/-/blob/master/code/mbml_german.ipynb">Manning’s GitLab repository</a> and save it in the "AIF360/examples" folder.</p></li>
<li><p>You should now be able to open and run the notebook from the CLI as you usually would:</p>
<pre><code>$ jupyter notebook mbml_german.ipynb</code></pre></li>
</ol>
</section>
<section id="app_AIF360_GF" class="level2" data-number="4.2">
<h2 data-number="4.2"><span class="header-section-number">A.2</span> Group fairness in AIF360</h2>
<section id="comparing-outcomes-1" class="level3" data-number="4.2.1">
<h3 data-number="4.2.1"><span class="header-section-number">A.2.1</span> Comparing outcomes</h3>
<p>Now that we have covered some measures of fairness, let’s dive into calculating them. In this book we are going to use IBM’s <a href="https://aif360.mybluemix.net/resources">AI Fairness 360 (AIF360)</a>. AIF360 is currently the most comprehensive open source library available for measuring and mitigating bias in machine learning models. The Python package includes an extensive set of metrics for datasets and models to test for biases, explanations for these metrics, and algorithms to mitigate bias in datasets and models many of which we will cover in this book. The system has been designed to be extensible, adopted software engineering best practices to maintain code quality, and is well <a href="https://aif360.readthedocs.io/en/latest/index.html">documented</a>. The package implements techniques from at-least eight published papers and includes over 71 bias detection metrics and nine bias mitigation algorithms. These techniques can all be called in a standard way, similar to scikit-learn’s fit/transform/predict paradigm.</p>
<p>In this section we’re going to use AIF360 to calculate some of the metrics we’ve talked about in the previous section as a means to get started working with it. For calculating the metrics we’ve talked about so far, using AIF360 might seem to add unnecessary overhead as they are reasonably straightforward to code up directly once you have your data in a Pandas DataFrame. But remember, the library contains implementations of more complicated metrics and bias mitigations algorithms that we’ll cover later on in this book. Before we can use the library, we need to install it. Instructions are provided in Appendix <a href="#app_Install" data-reference-type="ref" data-reference="app_Install">A.1</a>.</p>
<section id="statlog-german-credit-data-data-set" class="level4 unnumbered">
<h4 class="unnumbered">Statlog (German Credit Data) Data Set</h4>
<p>The Jupyter Notebook, <code>mbml_german.ipynb</code>, contains an example calculating some of the above fairness metrics on both a dataset and model output. It uses the <a href="https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)">Statlog (German Credit Data) Data Set</a>, in which one thousand loan applicants are classified as representing ‘good’ or ‘bad’ credit risks based on features such as loan term, loan amount, age, gender, marital status and more.</p>
<div class="lookbox">
<p><strong>Exercise: Statlog (German Credit Data) Data Set</strong></p>
<p>Sections 1-3 in the Jupyter Notebook, <code>mbml_german.ipynb</code>, load the data and perform some exploratory data analysis (EDA), looking at correlation heat maps (using a variety of different measures of association) and comparing distributions of the target for different values of the features. Open the notebook and run the code up to section four. You should be able to answer the following questions by working through the notebook.</p>
<ol>
<li><p>What proportion of the population is classified as male/female?</p></li>
<li><p>What proportion of the population have good credit vs bad?</p></li>
<li><p>How many continuous variables are there? What are they? Do any of them appear to be related? If so how?</p></li>
<li><p>How many categorical variables are there? What are they? Do any of them appear to be related? If so how?</p></li>
</ol>
</div>
</section>
<section id="calculating-independence-metrics" class="level4 unnumbered">
<h4 class="unnumbered">Calculating independence metrics</h4>
<p>In order to calculate our metrics on the data using AIF360, we must have it in the correct format; that is, in a Pandas DataFrame (<code>data_df</code>) containing only numeric data types. In code listing A.1, we calculate the rate at which male and female applicants are classified as being good credit risks (<code>base_rate</code>) along with the difference (<code>mean_difference</code>) and the ratio (<code>disparate_impact</code>) of these rates.</p>
<div id="lst:AIF360metric" class="listing">
<p>Listing A.1: Calculating independence metrics for the data using AIF360</p>
<pre><code># Create a DataFrame to store results in
outcomes_df = pd.DataFrame(columns=[`female&#39;, `male&#39;,
                                    `difference&#39;, `ratio&#39;],
                           index=[`data&#39;, `model&#39;,
                                  `train data&#39;, `train model&#39;,
                                  `test data&#39;, `test model&#39;])

# Define privileged and unprivileged groups
privileged_groups = [{`sex_male&#39;:1}]
unprivileged_groups = [{`sex_male&#39;:0}]

# Create an instance of BinaryLabelDataset
data_ds = BinaryLabelDataset(df = data_df,
    label_names = [`goodcredit&#39;],
    protected_attribute_names = [`sex&#39;])

# Create an instance of BinaryLabelDatasetMetric
data_metric = BinaryLabelDatasetMetric(data_ds,
    privileged_groups = privileged_groups,
    unprivileged_groups = unprivileged_groups)

# Compute the metrics with data_metric and store them in outcomes_df
outcomes_df.at[`data&#39;, `female&#39;] = data_metric.base_rate(privileged=0)
outcomes_df.at[`data&#39;, `male&#39;] = data_metric.base_rate(privileged=1)
outcomes_df.at[`data&#39;, `difference&#39;] = data_metric.mean_difference()
outcomes_df.at[`data&#39;, `ratio&#39;] = data_metric.disparate_impact()</code></pre>
</div>
<p>In the notebook we look at these metrics on both the data and the model output for three different sets of the data (the full dataset, the train set and the test set) with two different models (one trained on the full dataset and another trained only on a subset of the data - the training set). In code listing A.1, we create a DataFrame to display the results in (<code>outcomes_df</code>) and populate the first row of it. First we define our privileged and unprivileged groups.</p>
<div class="lookbox">
<p><strong>Defining privileged and unprivileged groups</strong></p>
<p>The format for these is a list of dictionaries. Each dictionary in the list defines a group, the key being a feature and the value being the value of the feature for members of the group. The key, value pairs in the dictionaries are joined with an intersection (AND operator) and the dictionaries in the list are joined with a union (OR operator). So for example,</p>
<pre><code>[{`sex&#39;: 1, `age&gt;=30&#39;: 1}, {`sex&#39;: 0}]</code></pre>
<p>corresponds to individuals such that,</p>
<pre><code>(data_df[`sex&#39;]==1 AND data_df[`age&gt;=30&#39;]==1)  OR (data_df[`sex&#39;]==0)</code></pre>
</div>
<p>Next we create a <a href="https://aif360.readthedocs.io/en/latest/modules/generated/aif360.datasets.BinaryLabelDataset.html#aif360.datasets.BinaryLabelDataset"><code>BinaryLabelDataset</code></a> object (<code>data_ds</code>) which in turn is used to create a <a href="https://aif360.readthedocs.io/en/latest/modules/generated/aif360.metrics.BinaryLabelDatasetMetric.html#aif360.metrics.BinaryLabelDatasetMetric"><code>BinaryLabelDatasetMetric</code></a> object (<code>data_metric</code>). We then calculate the fairness metrics from <code>data_metric</code> and store the results in <code>outcomes_df</code>.</p>
<div class="lookbox">
<p><strong>Exercise: Multiple sensitive features</strong></p>
<p>Calculate independence metrics (base rates, difference and ratio) for the full dataset in the case where the privileged group is males age 30 and over, and the unprivileged group is females under the age of 30. Do this two ways, using AIF360 and using Pandas. Compare your results to make sure they match.</p>
</div>
<p>Once we have trained a model and made predictions, similar code can be written to calculate independence metrics on the model predictions for the full dataset. Code listing A.2 shows how we do this using the predictions from the trained model <code>clf</code>.</p>
<div id="lst:AIF360metric1" class="listing">
<p>Listing A.2: Calculating independence metrics for the model using AIF360</p>
<pre><code># Create a DataFrame with the features and model predicted target
model_df = pd.concat([X, pd.Series(clf.predict(X), name=`goodcredit&#39;)],
	axis=1)

# Create an instance of BinaryLabelDataset
model_ds = BinaryLabelDataset(df = model_df,
    label_names = [`goodcredit&#39;],
    protected_attribute_names = [`sex_male&#39;])

# Create an instance of BinaryLabelDatasetMetric
model_metric = BinaryLabelDatasetMetric(model_ds,
    privileged_groups = privileged_groups,
    unprivileged_groups = unprivileged_groups)

# Compute the metrics with model_metric and store them in outcomes_df
outcomes_df.at[`model&#39;, `female&#39;] = model_metric.base_rate(privileged=0)
outcomes_df.at[`model&#39;, `male&#39;] = model_metric.base_rate(privileged=1)
outcomes_df.at[`model&#39;, `difference&#39;] = model_metric.mean_difference()
outcomes_df.at[`model&#39;, `ratio&#39;] = model_metric.disparate_impact()</code></pre>
</div>
<p>Table <a href="#tbl:app_IndependenceMetrics" data-reference-type="ref" data-reference="tbl:app_IndependenceMetrics">A.1</a> shows the results of the calculations stored in <code>outcomes_df</code> from the notebook. From Table <a href="#tbl:app_IndependenceMetrics" data-reference-type="ref" data-reference="tbl:app_IndependenceMetrics">A.1</a> we note some variation in the rates at which men and women are predicted to present good credit risks for the model versus the data. In particular, the model acceptance rates are higher for both male and female applicants than those observed in the data. There are particularly big differences when we compare results for the test data versus the model on the test data (test model), which is not surprising since the mean difference and impact ratio for the train data and test data are markedly different. In addition we are aware that our model is overfitting. Without intervention, our model appears to be reducing the bias present in the data for the test set (as measured by our independence metrics).</p>
<div id="tbl:app_IndependenceMetrics">
<table>
<caption>Table A.1: Acceptance rates for the Statlog (German Credit) Data Set.</caption>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">Female</th>
<th style="text-align: right;">Male</th>
<th style="text-align: right;">Difference</th>
<th style="text-align: right;">Ratio</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Data</td>
<td style="text-align: right;">0.648</td>
<td style="text-align: right;">0.723</td>
<td style="text-align: right;">-0.0748</td>
<td style="text-align: right;">0.897</td>
</tr>
<tr class="even">
<td style="text-align: left;">Model<sup>a</sup></td>
<td style="text-align: right;">0.674</td>
<td style="text-align: right;">0.749</td>
<td style="text-align: right;">-0.0751</td>
<td style="text-align: right;">0.900</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Train data</td>
<td style="text-align: right;">0.659</td>
<td style="text-align: right;">0.719</td>
<td style="text-align: right;">-0.0601</td>
<td style="text-align: right;">0.916</td>
</tr>
<tr class="even">
<td style="text-align: left;">Train model<sup>b</sup></td>
<td style="text-align: right;">0.667</td>
<td style="text-align: right;">0.731</td>
<td style="text-align: right;">-0.0647</td>
<td style="text-align: right;">0.911</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Test data</td>
<td style="text-align: right;">0.607</td>
<td style="text-align: right;">0.741</td>
<td style="text-align: right;">-0.1345</td>
<td style="text-align: right;">0.819</td>
</tr>
<tr class="even">
<td style="text-align: left;">Test model<sup>b</sup></td>
<td style="text-align: right;">0.705</td>
<td style="text-align: right;">0.820</td>
<td style="text-align: right;">-0.1152</td>
<td style="text-align: right;">0.860</td>
</tr>
</tbody>
</table>
</div>
<div class="tablenotes">
<p><sup>a</sup>Model trained on the full dataset.</p>
<p><sup>b</sup>Model trained on the train dataset only.</p>
</div>
<div class="lookbox">
<p><strong>Exercise: Twin test</strong></p>
<p>Implement the twin test (described in section <a href="#sec_CondIndep" data-reference-type="ref" data-reference="sec_CondIndep">3.1.2</a>) for the model trained on the full dataset. Calculate the causal mean difference between male and female applicants using 2000 data points (1000 male and 1000 female applicants) i.e. the full dataset together with the ‘twin’ of the opposite gender.</p>
</div>
</section>
</section>
<section id="comparing-errors-1" class="level3" data-number="4.2.2">
<h3 data-number="4.2.2"><span class="header-section-number">A.2.2</span> Comparing errors</h3>
<p>In order to calculate balanced error metrics with AIF360, we need to create an object of type <a href="https://aif360.readthedocs.io/en/latest/modules/generated/aif360.metrics.ClassificationMetric.html"><code>ClassificationMetric</code></a>. Returning to our example working with the German Credit Data, code listing A.3 calculates a series of balanced error metrics and populates the DataFrame <code>errors_df</code> with them. Note that <code>data_ds</code> and <code>model_ds</code> were created, and <code>privileged_groups</code> and <code>unprivileged_groups</code> were defined in earlier code listings.</p>
<div id="lst:AIF360metric2" class="listing">
<p>Listing A.3: Calculating balanced error metrics with AIF360</p>
<pre><code># Create a DataFrame to store results in
errors_df = pd.DataFrame(columns=[`female&#39;, `male&#39;,
                                  `difference&#39;, `ratio&#39;],
                         index=[`ERR&#39;, `FPR&#39;, `FNR&#39;, `FDR&#39;, `FOR&#39;])

# Create an instance of ClassificationMetric
clf_metric = ClassificationMetric(data_ds,
    model_ds,
    privileged_groups = privileged_groups,
    unprivileged_groups = unprivileged_groups)

# Compute the metrics with clf_metric and store them in errors_df
# Error rates for the unprivileged group
errors_df.at[`ERR&#39;, `female&#39;] = clf_metric.error_rate(privileged=False)
errors_df.at[`FPR&#39;, `female&#39;] =
    clf_metric.false_positive_rate(privileged=False)
errors_df.at[`FNR&#39;, `female&#39;] =
    clf_metric.false_negative_rate(privileged=False)
errors_df.at[`FDR&#39;, `female&#39;] =
    clf_metric.false_discovery_rate(privileged=False)
errors_df.at[`FOR&#39;, `female&#39;] =
    clf_metric.false_omission_rate(privileged=False)

# Error rates for the privileged group
errors_df.at[`ERR&#39;, `male&#39;] = clf_metric.error_rate(privileged=True)
errors_df.at[`FPR&#39;, `male&#39;] =
    clf_metric.false_positive_rate(privileged=True)
errors_df.at[`FNR&#39;, `male&#39;] =
    clf_metric.false_negative_rate(privileged=True)
errors_df.at[`FDR&#39;, `male&#39;] =
    clf_metric.false_discovery_rate(privileged=True)
errors_df.at[`FOR&#39;, `male&#39;] =
    clf_metric.false_omission_rate(privileged=True)

# Differences in error rates
errors_df.at[`ERR&#39;, `difference&#39;] = clf_metric.error_rate_difference()
errors_df.at[`FPR&#39;, `difference&#39;] =
    clf_metric.false_positive_rate_difference()
errors_df.at[`FNR&#39;, `difference&#39;] =
    clf_metric.false_negative_rate_difference()
errors_df.at[`FDR&#39;, `difference&#39;] =
    clf_metric.false_discovery_rate_difference()
errors_df.at[`FOR&#39;, `difference&#39;] =
    clf_metric.false_omission_rate_difference()

# Ratios of error rates
errors_df.at[`ERR&#39;, `ratio&#39;] = clf_metric.error_rate_ratio()
errors_df.at[`FPR&#39;, `ratio&#39;] = clf_metric.false_positive_rate_ratio()
errors_df.at[`FNR&#39;, `ratio&#39;] = clf_metric.false_negative_rate_ratio()
errors_df.at[`FDR&#39;, `ratio&#39;] = clf_metric.false_discovery_rate_ratio()
errors_df.at[`FOR&#39;, `ratio&#39;] = clf_metric.false_omission_rate_ratio()

display(errors_df)</code></pre>
</div>
<p>The DataFrame <code>error_df</code> is shown in Table <a href="#tbl:app_BalancedErrorMetrics" data-reference-type="ref" data-reference="tbl:app_BalancedErrorMetrics">A.2</a>.</p>
<div id="tbl:app_BalancedErrorMetrics">
<table>
<caption>Table A.2: Error metrics for the Statlog (German Credit Data) Data Set.</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Error metric<sup>a</sup></th>
<th style="text-align: right;">Female</th>
<th style="text-align: right;">Male</th>
<th style="text-align: right;">Difference</th>
<th style="text-align: right;">Ratio</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">ERR</td>
<td style="text-align: right;">0.246</td>
<td style="text-align: right;">0.180</td>
<td style="text-align: right;">0.066</td>
<td style="text-align: right;">1.37</td>
</tr>
<tr class="even">
<td style="text-align: left;">FPR</td>
<td style="text-align: right;">0.458</td>
<td style="text-align: right;">0.472</td>
<td style="text-align: right;">-0.014</td>
<td style="text-align: right;">0.97</td>
</tr>
<tr class="odd">
<td style="text-align: left;">FNR</td>
<td style="text-align: right;">0.108</td>
<td style="text-align: right;">0.078</td>
<td style="text-align: right;">0.030</td>
<td style="text-align: right;">1.39</td>
</tr>
<tr class="even">
<td style="text-align: left;">FDR</td>
<td style="text-align: right;">0.250</td>
<td style="text-align: right;">0.152</td>
<td style="text-align: right;">0.098</td>
<td style="text-align: right;">1.65</td>
</tr>
<tr class="odd">
<td style="text-align: left;">FOR</td>
<td style="text-align: right;">0.235</td>
<td style="text-align: right;">0.296</td>
<td style="text-align: right;">-0.061</td>
<td style="text-align: right;">0.79</td>
</tr>
</tbody>
</table>
</div>
<div class="tablenotes">
<p><sup>a</sup>We abbreviate error rate (ERR), false positive rate (FPR), false negative rate (FNR), false discovery rate (FDR) and false omission rate (FOR). See appendix <a href="#app_Metrics" data-reference-type="ref" data-reference="app_Metrics">B</a> for detailed descriptions of confusion matrix metrics.</p>
</div>
<p>This time we just look at the metrics for the model trained on the training set and calculated on the test set. We note that the overall error rate is 37% higher for female applicants. The false negative rate is 39% higher for female applicants, that is for female applicants we more often incorrectly predict that they represent bad credit risks when they are in fact good credit risks. We also note that the false discovery rate is 65% higher for female applicants which means that when we do predict women to be credit worthy they are more often not. The false omission rate is 21% lower for female applicants which means we are more often correct when we predict that they are not credit worthy. Our findings are not surprising given the difference in prevalence of credit worthy male and female applicants between our training and test sets shown in Table <a href="#tbl:app_IndependenceMetrics" data-reference-type="ref" data-reference="tbl:app_IndependenceMetrics">A.1</a>.</p>
<p>Recall that when we compared fairness metrics under the independence criterion, it appeared that our model was reducing the level of bias in the data. Note that comparing balanced error metrics (in addition to independence metrics) gives us a richer understanding of the behaviour of our model in relation to protected groups.</p>
</section>
</section>
</section>
<section id="app_Metrics" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">B</span> Performance Metrics</h1>
<section id="confusion-matrix-metrics" class="level3 unnumbered">
<h3 class="unnumbered">Confusion Matrix Metrics</h3>
<section id="performance-metrics" class="level4 unnumbered">
<h4 class="unnumbered">Performance Metrics</h4>
<div id="tbl:app_CMPerfMetrics">
<table>
<caption>Table B.1: Summary of performance metrics for a binary classifier</caption>
<thead>
<tr class="header">
<th style="text-align: center;" colspan="2"></th>
<th style="text-align: center;" colspan="2">Ground Truth</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;" colspan="2"></td>
<td style="text-align: center;"><span class="math inline">\(y=1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(y=0\)</span></td>
<td style="text-align: center;">Metric</td>
</tr>
<tr class="even">
<td style="text-align: center;" rowspan="4">Prediction</td>
<td style="text-align: center;" rowspan="2"><span class="math inline">\(\hat{y}=1\)</span></td>
<td style="text-align: center;" rowspan="2">True Positive</td>
<td style="text-align: center;" rowspan="2">False Positive<br>Type I Error</td>
<td style="text-align: center;" rowspan="2">Positive Predictive Value<sup>a</sup><br><span class="math inline">\(\mathbb{P}(\hat{y}=y|\hat{y}=1)\)</span></td>
</tr>
<tr class="odd">
</tr>
<tr class="even">
<td style="text-align: center;" rowspan="2"><span class="math inline">\(\hat{y}=0\)</span></td>
<td style="text-align: center;" rowspan="2">False Negative<br>Type II Error</td>
<td style="text-align: center;" rowspan="2">True Negative</td>
<td style="text-align: center;" rowspan="2">Negative Predictive Value<br><span class="math inline">\(\mathbb{P}(\hat{y}=y|\hat{y}=0)\)</span></td>
</tr>
<tr class="odd">
</tr>
<tr class="even">
<td style="text-align: center;" colspan="2" rowspan="2">Metric</td>
<td style="text-align: center;" rowspan="2">True Positive Rate<sup>b</sup><br><span class="math inline">\(\mathbb{P}(\hat{y}=y|y=1)\)</span></td>
<td style="text-align: center;" rowspan="2">True Negative Rate<br><span class="math inline">\(\mathbb{P}(\hat{y}=y|y=0)\)</span></td>
<td style="text-align: center;" rowspan="2">Accuracy<br><span class="math inline">\(\mathbb{P}(\hat{y}=y)\)</span></td>
</tr>
<tr class="odd">
</tr>
</tbody>
</table>
</div>
<div class="tablenotes">
<p><sup>a</sup> Positive Predictive Value = Precision</p>
<p><sup>b</sup> True Positive Rate = Recall</p>
</div>
</section>
<section id="error-metrics" class="level4 unnumbered">
<h4 class="unnumbered">Error Metrics</h4>
<div id="tbl:app_CMErrMetrics">
<table>
<caption>Table B.2: Summary of error rate types for a binary classifier</caption>
<thead>
<tr class="header">
<th style="text-align: center;" colspan="2"></th>
<th style="text-align: center;" colspan="2">Ground Truth</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;" colspan="2"></td>
<td style="text-align: center;"><span class="math inline">\(y=1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(y=0\)</span></td>
<td style="text-align: center;">Error Rate Type</td>
</tr>
<tr class="even">
<td style="text-align: center;" rowspan="4">Prediction</td>
<td style="text-align: center;" rowspan="2"><span class="math inline">\(\hat{y}=1\)</span></td>
<td style="text-align: center;" rowspan="2">True Positive</td>
<td style="text-align: center;" rowspan="2">False Positive<br>Type I Error</td>
<td style="text-align: center;" rowspan="2">False Discovery Rate<br><span class="math inline">\(\mathbb{P}(\hat{y}\neq y|\hat{y}=1)\)</span></td>
</tr>
<tr class="odd">
</tr>
<tr class="even">
<td style="text-align: center;" rowspan="2"><span class="math inline">\(\hat{y}=0\)</span></td>
<td style="text-align: center;" rowspan="2">False Negative<br>Type II Error</td>
<td style="text-align: center;" rowspan="2">True Negative</td>
<td style="text-align: center;" rowspan="2">False Omission Rate<br><span class="math inline">\(\mathbb{P}(\hat{y}\neq y|\hat{y}=0)\)</span></td>
</tr>
<tr class="odd">
</tr>
<tr class="even">
<td style="text-align: center;" colspan="2" rowspan="2">Error Rate Type</td>
<td style="text-align: center;" rowspan="2">False Negative Rate<br><span class="math inline">\(\mathbb{P}(\hat{y}\neq y|y=1)\)</span></td>
<td style="text-align: center;" rowspan="2">False Positive Rate<br><span class="math inline">\(\mathbb{P}(\hat{y}\neq y|y=0)\)</span></td>
<td style="text-align: center;" rowspan="2">Error Rate<br><span class="math inline">\(\mathbb{P}(\hat{y}\neq y)\)</span></td>
</tr>
<tr class="odd">
</tr>
</tbody>
</table>
</div>
</section>
<section id="combined-table" class="level4 unnumbered">
<h4 class="unnumbered">Combined table</h4>
<div id="tbl:app_CMPerfMetrics2">
<table>
<caption>Table B.3: Summary of performance metrics for a binary classifier</caption>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;" colspan="2">Ground Truth</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Prediction</td>
<td style="text-align: center;"><span class="math inline">\(y=1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(y=0\)</span></td>
<td style="text-align: center;">Performance</td>
<td style="text-align: center;">Error rate</td>
</tr>
<tr class="even">
<td style="text-align: center;" rowspan="2"><span class="math inline">\(\hat{y}=1\)</span></td>
<td style="text-align: center;" rowspan="2">True Positive</td>
<td style="text-align: center;" rowspan="2">False Positive<br>Type I Error</td>
<td style="text-align: center;" rowspan="2">Positive Predictive Value<sup>a</sup><br><span class="math inline">\(\mathbb{P}(\hat{y}=y|\hat{y}=1)\)</span></td>
<td style="text-align: center;" rowspan="2">False Discovery Rate<br><span class="math inline">\(\mathbb{P}(\hat{y}\neq y|\hat{y}=1)\)</span></td>
</tr>
<tr class="odd">
</tr>
<tr class="even">
<td style="text-align: center;" rowspan="2"><span class="math inline">\(\hat{y}=0\)</span></td>
<td style="text-align: center;" rowspan="2">False Negative<br>Type II Error</td>
<td style="text-align: center;" rowspan="2">True Negative</td>
<td style="text-align: center;" rowspan="2">Negative Predictive Value<br><span class="math inline">\(\mathbb{P}(\hat{y}=y|\hat{y}=0)\)</span></td>
<td style="text-align: center;" rowspan="2">False Omission Rate<br><span class="math inline">\(\mathbb{P}(\hat{y}\neq y|\hat{y}=0)\)</span></td>
</tr>
<tr class="odd">
</tr>
<tr class="even">
<td style="text-align: center;" rowspan="2">Performance</td>
<td style="text-align: center;" rowspan="2">True Positive Rate<sup>b</sup><br><span class="math inline">\(\mathbb{P}(\hat{y}=y|y=1)\)</span></td>
<td style="text-align: center;" rowspan="2">True Negative Rate<br><span class="math inline">\(\mathbb{P}(\hat{y}=y|y=0)\)</span></td>
<td style="text-align: center;" rowspan="2">Accuracy<br><span class="math inline">\(\mathbb{P}(\hat{y}=y)\)</span></td>
<td style="text-align: center;" rowspan="2"></td>
</tr>
<tr class="odd">
</tr>
<tr class="even">
<td style="text-align: center;" rowspan="2">Error Rate</td>
<td style="text-align: center;" rowspan="2">False Negative Rate<br><span class="math inline">\(\mathbb{P}(\hat{y}\neq y|y=1)\)</span></td>
<td style="text-align: center;" rowspan="2">False Positive Rate<br><span class="math inline">\(\mathbb{P}(\hat{y}\neq y|y=0)\)</span></td>
<td style="text-align: center;" rowspan="2"></td>
<td style="text-align: center;" rowspan="2">Error rate <span class="math inline">\(\mathbb{P}(\hat{y}\neq y)\)</span></td>
</tr>
<tr class="odd">
</tr>
</tbody>
</table>
</div>
<div class="tablenotes">
<p><sup>a</sup> Positive Predictive Value = Precision</p>
<p><sup>b</sup> True Positive Rate = Recall</p>
</div>
</section>
</section>
</section>
<section id="app_ProbRules" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">C</span> Rules of Probability</h1>
<div id="tbl:app_ProbRules">
<table>
<caption>Table C.1: Rules of probability</caption>
<tbody>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Rule</strong></td>
<td style="text-align: left;"><strong>Continuous Variables</strong></td>
<td style="text-align: left;"><strong>Discrete Variables</strong></td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Sum rule</strong></td>
<td style="text-align: left;"><span class="math inline">\(\displaystyle f_{X}(x) = \int_{y\in\mathcal{Y}} f_{X,Y}(x,y) \, \mathrm{d}y\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\displaystyle \mathbb{P}(x) = \sum_{y\in\mathcal{Y}} \mathbb{P}(x,y)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Product rule</strong></td>
<td style="text-align: left;"><span class="math inline">\(f_{X,Y}(x,y) = f_{Y|X}(x,y) f_X(x)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\mathbb{P}(x,y) = \mathbb{P}(y|x) \mathbb{P}(x)\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Bayes’ rule</strong></td>
<td style="text-align: left;"><span class="math inline">\(\displaystyle f_{Y|X}(x,y) = \frac{f_{X|Y}(x,y) f_Y(y)}{f_X(x)}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\displaystyle \mathbb{P}(y|x) = \frac{\mathbb{P}(x|y)\mathbb{P}(y)}{\mathbb{P}(x)}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Independence</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(X\bot Y\)</span></td>
<td style="text-align: left;"><span class="math inline">\(f_{Y|X}(x,y) = f_Y(y)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\mathbb{P}(y|x) = \mathbb{P}(y)\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">From the product rule</td>
<td style="text-align: left;"><span class="math inline">\(f_{X,Y}(x,y) = f_X(x)f_Y(y)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\mathbb{P}(x,y) = \mathbb{P}(x) \mathbb{P}(y)\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Conditional Independence</strong></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(X \bot Y | Z\)</span></td>
<td style="text-align: left;"><span class="math inline">\(f_{Y|X,Z}(x,y,z) = f_{Y|Z}(y,z)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\mathbb{P}(y|x,z) = \mathbb{P}(y|z)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Using the product rule</td>
<td style="text-align: left;"><span class="math inline">\(f_{X,Y|Z}(x,y,z) = f_{Y|X,Z}(x,y,z)f_{X|Z}(x,z)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\mathbb{P}(x,y|z) = \mathbb{P}(y|x,z)\mathbb{P}(x|z)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Substituting for <span class="math inline">\(Y|X,Z\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\phantom{f_{X,Y|Z}(x,y,z)} = f_{Y|Z}(y,z)f_{X|Z}(x,y)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\phantom{\mathbb{P}(x,y|z)} = \mathbb{P}(y|z)\mathbb{P}(x|z)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="app_Solutions" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">D</span> Solutions to Exercises</h1>
<section id="sec_app_GFSolutions" class="level2" data-number="7.1">
<h2 data-number="7.1"><span class="header-section-number">D.1</span> Chapter <a href="#ch_GroupFairness" data-reference-type="ref" data-reference="ch_GroupFairness">3</a>: Group Fairness</h2>
<section id="comparing-outcomes-2" class="level3" data-number="7.1.1">
<h3 data-number="7.1.1"><span class="header-section-number">D.1.1</span> Comparing outcomes</h3>
<div class="lookbox">
<div id="GF_NPI">
<p><strong>Exercise: Normalised prejudice index</strong></p>
</div>
<p>Write a function that takes two arrays <span class="math inline">\(y\)</span> and <span class="math inline">\(z\)</span> of categorical features and returns the normalised prejudice index. Hint:</p>
<ol>
<li><p>Compute the probability distributions <span class="math inline">\(\mathbb{P}(y)\)</span>, <span class="math inline">\(\mathbb{P}(z)\)</span> and <span class="math inline">\(\mathbb{P}(y,z)\)</span>. Note that these can be thought of as the frequency with which each event occurs.</p></li>
<li><p>Compute the entropies <span class="math inline">\(H(y)\)</span> and <span class="math inline">\(H(z)\)</span> shown in equation (3.3) and use these to compute the normalising factor, <span class="math inline">\(\sqrt{H(y)H(z)}\)</span>.</p></li>
<li><p>Compute the mutual information <span class="math inline">\(I(z,y)\)</span> shown in equation (3.1) and divide by the normalising factor.</p></li>
</ol>
<p>You can test your implementation against scikit-learn’s:<br />
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.normalized_mutual_info_score.html">sklearn.metrics.normalized_mutual_info_score</a>.</p>
</div>
<div id="lst:Ex-npi" class="listing">
<p>Listing D.1: Calculating the normalised prejudice index</p>
<pre><code># Import the necessary classes
import pandas as pd
import scipy.stats as ss

def normalised_mutual_information(x, y):
    &quot;&quot;&quot;normalised mutual information between x and y&quot;&quot;&quot;
    
    # Compute the probability distributions
    px   = x.value_counts(normalize=True)
    py   = y.value_counts(normalize=True)
    pxy  = pd.Series(zip(x,y)).value_counts(normalize=True)
    
    # Compute the normalising factor
    norm = math.sqrt( ss.entropy(px) * ss.entropy(py)  )
    
    # Compute mutual information, divide by the normalising factor
    # and return the result
    return sum([p * math.log(p / (px[xy[0]] * py[xy[1]]))
                for xy, p in p_xy.items()]) / norm</code></pre>
</div>
<div class="lookbox">
<div id="GF_SPDmax">
<p><strong>Exercise: Statistical parity difference maximum</strong></p>
</div>
<p>Show that <span class="math display">\[d_{\max} = \min\left\{ \frac{\mathbb{P}(\hat{Y}=1)}{\mathbb{P}(Z=1)},
                       \frac{\mathbb{P}(\hat{Y}=0)}{\mathbb{P}(Z=0)} \right\}.\]</span></p>
</div>
<p>We can write statistical parity difference as <span class="math display">\[d = \mathbb{P}(\hat{Y}=1 | Z=1) - \mathbb{P}(\hat{Y}=1 | Z=0).\]</span> Let’s rewrite this with advantaged and disadvantaged outcomes and groups to make it more concrete, <span class="math display">\[d = \mathbb{P}(y^+|z^+) - \mathbb{P}(y^+|z^-) \\
  = \frac{\mathbb{P}(y^+, z^+)}{\mathbb{P}(z^+)} - \frac{\mathbb{P}(y^+, z^-)}{\mathbb{P}(z^-)}
  \leq \frac{\mathbb{P}(y^+)}{\mathbb{P}(z^+)}.\]</span> This maximal value occurs when <span class="math display">\[\mathbb{P}(y^+, z^+) = \mathbb{P}(y^+) \quad \text{and} \quad \mathbb{P}(y^+, z^-)=0;\]</span> that is, when all members of the advantaged class, receive the advantaged outcome. We can also write, <span class="math display">\[\begin{aligned}
d = \mathbb{P}(y^+|z^+) - \mathbb{P}(y^+|z^-) 
  &amp; = \mathbb{P}(y^-|z^-) - \mathbb{P}(y^-|z^+) \\
  &amp; = \frac{\mathbb{P}(y^-, z^-)}{\mathbb{P}(z^-)}
    - \frac{\mathbb{P}(y^-, z^+)}{\mathbb{P}(z^+)} \leq \frac{\mathbb{P}(y^-)}{\mathbb{P}(z^-)}.\end{aligned}\]</span> Here the maximal value occurs when <span class="math display">\[\mathbb{P}(y^-, z^-) = \mathbb{P}(y^-) \quad \text{and} \quad \mathbb{P}(y^-, z^+)=0;\]</span> that is, when all members of the disadvantaged class, receive the disadvantaged outcome. Thus, <span class="math display">\[d_{max} = \min\left\{ \frac{\mathbb{P}(y^+)}{\mathbb{P}(z^+)},
                      \frac{\mathbb{P}(y^-)}{\mathbb{P}(z^-)} \right\}.\]</span> Note that, <span class="math display">\[\frac{\mathbb{P}(y^+)}{\mathbb{P}(z^+)} = \frac{\mathbb{P}(y^-)}{\mathbb{P}(z^-)} \quad \Leftrightarrow \quad \mathbb{P}(y_+) = \mathbb{P}(z_+);\]</span> that is, when all members of the advantaged class, receive the advantaged outcome and all members of the disadvantaged class, receive the disadvantaged outcome.</p>
<div class="lookbox">
<div id="GF_OR">
<p><strong>Exercise: Odds ratio</strong></p>
</div>
<p>Show that the odds ratio is always greater than or equal to one in the case where <span class="math inline">\(\hat{Y}=1\)</span> in the advantaged outcome and <span class="math inline">\(Z=1\)</span> is the privileged group.</p>
</div>
<p><span class="math display">\[r_{\text{odds}}
= \frac{\mathbb{P}(\hat{Y}=1 | Z=1)\mathbb{P}(\hat{Y}=0 | Z=0)}
       {\mathbb{P}(\hat{Y}=0 | Z=1)\mathbb{P}(\hat{Y}=1 | Z=0)}.\]</span> Let <span class="math inline">\(\hat{Y}=1\)</span> be the advantaged outcome and <span class="math inline">\(Z=1\)</span> be the privileged group, then we can write, <span class="math display">\[r_{\text{odds}} = \frac{\mathbb{P}(\hat{y}_+ | z_+)\mathbb{P}(\hat{y}_- | z_-)}
                       {\mathbb{P}(\hat{y}_- | z_+)\mathbb{P}(\hat{y}_+ | z_-)}\]</span> In this case, since <span class="math inline">\(\mathbb{P}(\hat{y}_+ | z_+)&gt;\mathbb{P}(\hat{y}_+ | z_-)\)</span> and <span class="math inline">\(\mathbb{P}(\hat{y}_- | z_-)&gt;\mathbb{P}(\hat{y}_- | z_+)\)</span>, the numerator is always greater than the denominator and the odds ratio will be greater than one.</p>
</section>
<section id="comparing-errors-2" class="level3" data-number="7.1.2">
<h3 data-number="7.1.2"><span class="header-section-number">D.1.2</span> Comparing errors</h3>
<div class="lookbox">
<div id="GF_Suff">
<p><strong>Exercise: Sufficiency</strong></p>
</div>
<p>Show that sufficiency is satisfied if and only if the false omission rate and false discovery rate are equal for all groups.</p>
</div>
<p>Sufficiency implies <span class="math display">\[\mathbb{P}(y|\hat{y}, z) = \mathbb{P}(y|\hat{y}).\]</span> For the simplest case of a binary classifier where we have a single sensitive binary feature. We can write this requirement as two conditions, <span class="math display">\[\begin{aligned}
\mathbb{P}(Y=1 | Z=1, \hat{Y}=1) &amp; = \mathbb{P}(Y=1 | Z=0, \hat{Y}=1), \\
\mathbb{P}(Y=1 | Z=1, \hat{Y}=0) &amp; = \mathbb{P}(Y=1 | Z=0, \hat{Y}=0).\end{aligned}\]</span> Recall that <span class="math inline">\(\mathbb{P}(Y=1 | \hat{Y}=1)\)</span> is the positive predictive value (<span class="math inline">\(PPV\)</span>) of the classifier and <span class="math inline">\(\mathbb{P}(Y=1 | \hat{Y}=0)\)</span> is the false omission rate (<span class="math inline">\(FOR\)</span>). We see then that sufficiency requires the positive predictive value to be the same for all values of the sensitive feature and the false omission rate to be the same for all values of the sensitive feature. Note that the positive predictive value is balanced if and only if the false discovery rate is balanced, so thinking in terms of error metrics only, separation requires the false discovery and false omission rates to be balanced.</p>
</section>
<section id="incompatibility-of-fairness-criteria" class="level3" data-number="7.1.3">
<h3 data-number="7.1.3"><span class="header-section-number">D.1.3</span> Incompatibility of fairness criteria</h3>
<section id="separation-versus-sufficiency-1" class="level4" data-number="7.1.3.1">
<h4 data-number="7.1.3.1"><span class="header-section-number">D.1.3.1</span> Separation versus Sufficiency</h4>
<div class="lookbox">
<div id="GF_PredVal">
<p><strong>Exercise: Predictive values</strong></p>
</div>
<p>Prove the results given in equations (3.13) and (3.14).</p>
</div>
<p>We want to write the positive and negative predictive values (<span class="math inline">\(PPV\)</span> and <span class="math inline">\(NPV\)</span> respectively) in terms of the true positive, false positive and acceptance rates (<span class="math inline">\(TPR\)</span>, <span class="math inline">\(FPR\)</span> and <span class="math inline">\(p\)</span> respectively). We start by looking at some relationships between the elements of a confusion matrix shown in Table <a href="#tbl:app_ConfMax" data-reference-type="ref" data-reference="tbl:app_ConfMax">D.1</a>.</p>
<div id="tbl:app_ConfMax">
<table>
<caption>Table D.1: Confusion matrix</caption>
<thead>
<tr class="header">
<th style="text-align: center;" colspan="2"></th>
<th style="text-align: center;" colspan="2">Ground Truth</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;" colspan="2"></td>
<td style="text-align: center;"><span class="math inline">\(y=1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(y=0\)</span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;" rowspan="4">Prediction</td>
<td style="text-align: center;" rowspan="2"><span class="math inline">\(\hat{y}=1\)</span></td>
<td style="text-align: center;" rowspan="2">True Positive<br>(<span class="math inline">\(TP\)</span>)</td>
<td style="text-align: center;" rowspan="2">False Positive<br>(<span class="math inline">\(FP\)</span>)</td>
<td style="text-align: center;" rowspan="2"><span class="math inline">\(\displaystyle PPV = \frac{TP}{TP+FP}\)</span></td>
</tr>
<tr class="odd">
</tr>
<tr class="even">
<td style="text-align: center;" rowspan="2"><span class="math inline">\(\hat{y}=0\)</span></td>
<td style="text-align: center;" rowspan="2">False Negative<br>(<span class="math inline">\(FN\)</span>)</td>
<td style="text-align: center;" rowspan="2">True Negative<br>(<span class="math inline">\(TN\)</span>)</td>
<td style="text-align: center;" rowspan="2"><span class="math inline">\(\displaystyle NPV = \frac{TN}{FN+TN}\)</span></td>
</tr>
<tr class="odd">
</tr>
<tr class="even">
<td style="text-align: center;" colspan="2"></td>
<td style="text-align: center;"><span class="math inline">\(\begin{aligned}
  TPR &amp; = \frac{TP}{TP+FN} \\
1-TPR &amp; = \frac{FN}{TP+FN} \\
    p &amp; = \frac{TP+FN}{n}  \end{aligned}\)</span></td>
<td style="text-align: center;"><span><span class="math inline">\(\begin{aligned}
  FPR &amp; = \frac{FP}{FP+TN} \\
1-FPR &amp; = \frac{TN}{FP+TN} \\
  1-p &amp; = \frac{FP+TN}{n}  \end{aligned}\)</span></span></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
</div>
<p>where <span class="math inline">\(n= TP+FP+FN+TN\)</span> denotes the total number of data points. Using the equations in the final row of the table we can write, <span class="math display">\[\begin{aligned}
    p TPR &amp; = \frac{TP}{n}, &amp;     (1-p) FPR &amp; = \frac{FP}{n}, \\
p (1-TPR) &amp; = \frac{FN}{n}, &amp; (1-p) (1-FPR) &amp; = \frac{TP}{n}.\end{aligned}\]</span> Finally, we can substitute these into our expressions for <span class="math inline">\(PPV\)</span> and <span class="math inline">\(NPV\)</span> in the right hand column of Table <a href="#tbl:app_ConfMax" data-reference-type="ref" data-reference="tbl:app_ConfMax">D.1</a> to find the relationships in equations (3.13) and (3.14). <span class="math display">\[\begin{aligned}
PPV &amp; = \frac{p TPR}{p TPR + (1-p)FPR} \\
NPV &amp; = \frac{(1-p)(1-FPR)}{p(1-TPR) + (1-p)(1-FPR)}.\end{aligned}\]</span></p>
<div class="lookbox">
<div id="GF_SepVsSuff">
<p><strong>Exercise: Separation versus sufficiency</strong></p>
</div>
<p>Show that for separation and sufficiency to hold equations (3.15) and (3.16) must hold for for any pair of groups <span class="math inline">\(Z=a\)</span> and <span class="math inline">\(Z=b\)</span>.</p>
</div>
<p>For separation to hold the true positive rate (<span class="math inline">\(TPR\)</span>) and false positive rate (<span class="math inline">\(FPR\)</span>) must be constant across all values of the sensitive features. For sufficiency to hold the positive predictive value (<span class="math inline">\(PPV\)</span>) and negative predictive value (<span class="math inline">\(NPV\)</span>) must be constant across all values of the sensitive features. For brevity we shal use a subscript to denote conditioning on <span class="math inline">\(Z\)</span>, for example <span class="math inline">\(p_z=\mathbb{P}(Y=1|Z=z)\)</span>. For both separation and sufficiency to hold, we must have <span class="math display">\[\begin{aligned}
&amp; PPV_a = PPV_b \\
&amp; \Leftrightarrow\quad \frac{p_a TPR}{p_a TPR + (1-p_a)FPR}
                     = \frac{p_b TPR}{p_b TPR + (1-p_b)FPR} \\
&amp; \Leftrightarrow\quad p_b TPR[p_a TPR + (1-p_a)FPR]
                     = p_a TPR[p_b TPR + (1-p_b)FPR] \\
&amp;\Leftrightarrow\quad p_b TPR(1-p_a)FPR
                     = p_a TPR(1-p_b)FPR \\
&amp;\Leftrightarrow\quad TPR(p_b-p_a)FPR = 0.\end{aligned}\]</span> Similarly, <span class="math display">\[\begin{aligned}
&amp; NPV_a = NPV_b \\
&amp; \Leftrightarrow \quad \frac{(1-p_a)(1-FPR)}{p_a(1-TPR) + (1-p_a)(1-FPR)}
= \frac{(1-p_b)(1-FPR)}{p_b(1-TPR) + (1-p_b)(1-FPR)} \\
&amp; \Leftrightarrow \quad (1-p_b)(1-FPR)[p_a(1-TPR) + (1-p_a)(1-FPR)] \\
&amp; \qquad\qquad = (1-p_a)(1-FPR)[p_b(1-TPR) + (1-p_b)(1-FPR)] \\
&amp; \Leftrightarrow \quad (1-p_b)(1-FPR)p_a(1-TPR) = (1-p_a)(1-FPR)p_b(1-TPR).\\
&amp; \Leftrightarrow \quad (1-FPR)(p_b-p_a)(1-TPR) = 0.
 \end{aligned}\]</span></p>
</section>
</section>
</section>
</section>
<section id="bibliography" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body" role="doc-bibliography">
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">D. Ingold and S. Soper, <span>“Amazon doesn’t consider the race of its customers. Should it?”</span> <em>Bloomberg</em>, 2016.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">J. Rawls, <em>Justice as fairness: A restatement</em>. Cambridge, Mass.: Harvard University Press, 2001.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">P. L. B. Johnson, <span>“Speech to a joint session of congress on march 15, 1965,”</span> <em>Public Papers of the Presidents of the United States</em>, vol. I, entry 107, pp. 281–287, 1965.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">S. Barocas and A. D. Selbst, <span>“Big data’s disparate impact,”</span> <em>Calif Law Rev.</em>, vol. 104, pp. 671–732, 2016.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline"><span>“<span class="nocase">Ricci v. DeStefano, 557 U.S. 557</span>.”</span> 2009.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline"><span>“<span class="nocase">Griggs v. Duke Power Co., 401 U.S. 424</span>.”</span> 1971.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline"><span>“<span class="nocase">Wards Cove Packing Co. v. Atonio, 490 U.S. 642</span>.”</span> 1989.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[8] </div><div class="csl-right-inline"><span>“<span class="nocase">Grutter v. Bollinger, 539 U.S. 306</span>.”</span> 2003.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[9] </div><div class="csl-right-inline"><span>“<span>General Data Protection Regulation (GDPR): (EU) 2016/679 Recital 71</span>.”</span> 2016.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[10] </div><div class="csl-right-inline"><span>“<span class="nocase">Europe fit for the Digital Age: Commission proposes new rules and actions for excellence and trust in Artificial Intelligence</span>.”</span> 2021.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[11] </div><div class="csl-right-inline">P. J. Bickel, E. A. Hammel, and J. W. O’Connell, <span>“Sex bias in graduate admissions: Data from berkeley,”</span> <em>Science</em>, vol. 187, Issue 4175, pp. 398–404, 1975.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[12] </div><div class="csl-right-inline">E. Simpson, <span>“The interpretation of interaction in contingency tables,”</span> <em>Journal of the Royal Statistical Society</em>, vol. Series B, 13, pp. 238–241, 1951.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[13] </div><div class="csl-right-inline">J. Angwin, J. Larson, S. Mattu, and L. Kirchner, <span>“Machine bias,”</span> <em>ProPublica</em>, 2016.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[14] </div><div class="csl-right-inline">X. Wu and X. Zhang, <span>“Automated inference on criminality using face images.”</span> 2017.Available: <a href="https://arxiv.org/abs/1611.04135">https://arxiv.org/abs/1611.04135</a></div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[15] </div><div class="csl-right-inline">Y. Wang and M. Kosinski, <span>“Deep neural networks are more accurate than humans at detecting sexual orientation from facial images,”</span> <em>Journal of Personality and Social Psychology</em>, 2018.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[16] </div><div class="csl-right-inline">C. Wang, Q. Zhang, W. Liu, Y. Liu, and L. Miao, <span>“Facial feature discovery for ethnicity recognition,”</span> <em>Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</em>, 2018.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[17] </div><div class="csl-right-inline">C. Cadwalladr, <em>Facebook’s role in <span>Brexit</span> - and the threat to democracy</em>. TED, 2019.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[18] </div><div class="csl-right-inline">O. Varol, E. Ferrara, C. A. Davis, F. Menczer, and A. Flammini, <span>“Online human-bot interactions: Detection, estimation, and characterization.”</span> 2017.Available: <a href="https://arxiv.org/abs/1703.03107">https://arxiv.org/abs/1703.03107</a></div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[19] </div><div class="csl-right-inline">B. Allyn, <span>“Researchers: Nearly half of accounts tweeting about coronavirus are likely bots,”</span> <em>NPR</em>, May 2020.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[20] </div><div class="csl-right-inline">J. Nicas, <span>“Does facebook really know how many fake accounts it has?”</span> <em>The New York Times</em>, 2019.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[21] </div><div class="csl-right-inline">E. O’Toole, <span>“A dictionary entry citing <span>‘rabid feminist’</span> doesn’t just reflect prejudice, it reinforces it,”</span> <em>The Guardian</em>, 2016.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[22] </div><div class="csl-right-inline">D. Shariatmadari, <span>“Eight words that reveal the sexism at the heart of the english language,”</span> <em>The Guardian</em>, 2016.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[23] </div><div class="csl-right-inline">T. Bolukbasi, K.-W. Chang, J. Zou, V. Saligrama, and A. Kalai, <span>“Man is to computer programmer as woman is to homemaker? Debiasing word embeddings.”</span> 2016.Available: <a href="https://arxiv.org/abs/1607.06520">https://arxiv.org/abs/1607.06520</a></div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[24] </div><div class="csl-right-inline">A. Caliskan, J. J. Bryson, and A. Narayanan, <span>“Semantics derived automatically from language corpora contain human-like biases,”</span> <em>Science</em>, vol. 356, pp. 183–186, 2017.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[25] </div><div class="csl-right-inline">J. Buolamwini and T. Gerbru, <em>Gender shades: Intersectional accuracy disparities in commercial gender classification</em>, vol. 81. Proceedings of Machine Learning Research, 2018, pp. 1–15.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[26] </div><div class="csl-right-inline">V. U. Prabhu and A. Birhane, <span>“Large image datasets: A pyrrhic win for computer vision?”</span> 2020.Available: <a href="https://arxiv.org/abs/2006.16923">https://arxiv.org/abs/2006.16923</a></div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[27] </div><div class="csl-right-inline">L. Sweeney, <span>“Discrimination in online ad delivery,”</span> <em>SSRN</em>, 2013.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[28] </div><div class="csl-right-inline">M. Kay, C. Matuszek, and S. A. Munson, <span>“Unequal representation and gender stereotypes in image search results for occupations,”</span> <em>ACM</em>, 2015.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[29] </div><div class="csl-right-inline"><span>“Rates of drug use and sales, by race; rates of drug related criminal justice measures, by race.”</span> The Hamilton Project, 2015.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[30] </div><div class="csl-right-inline">J. Larson, S. Mattu, L. Kirchner, and J. Angwin, <span>“How we analyzed the COMPAS recidivism algorithm,”</span> <em>ProPublica</em>, 2016.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[31] </div><div class="csl-right-inline">Northpointe, <em>Practitioners guide to COMPAS core</em>. 2015.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[32] </div><div class="csl-right-inline">J. Larson, <span>“ProPublica analysis of data from broward county, fla.”</span> ProPublica, 2016.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[33] </div><div class="csl-right-inline">C. Jarrett, <span>“How prison changes people,”</span> <em>BBC Future</em>, May 2018.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[34] </div><div class="csl-right-inline">D. S. Nagin, <span>“Deterrence in the twenty-first century: A review of the evidence,”</span> <em>Crime and Justice</em>, vol. 42, May 2018.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[35] </div><div class="csl-right-inline">M. Mauer, <span>“Long-term sentences: Time to reconsider the scale of punishment,”</span> <em>The Sentencing Project</em>, 2018.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[36] </div><div class="csl-right-inline">P. Wagner and W. Sawyer, <span>“States of incarceration: The global context,”</span> <em>Prison Policy Initiative</em>, 2018.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[37] </div><div class="csl-right-inline">B. Lufkin, <span>“The myth behind long prison sentences,”</span> <em>BBC Future</em>, May 2018.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[38] </div><div class="csl-right-inline">A. D. Selbst, D. Boyd, S. A. Friedler, S. Venkatasubramanian, and J. Vertesi, <span>“Fairness and abstraction in sociotechnical systems,”</span> in <em>Proceedings of the conference on fairness, accountability, and transparency</em>, 2019, pp. 59–68. doi: <a href="https://doi.org/10.1145/3287560.3287598">10.1145/3287560.3287598</a>.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[39] </div><div class="csl-right-inline">Z. Obermeyer, B. Powers, C. Vogeli, and S. Mullainathan, <span>“Dissecting racial bias in an algorithm used to manage the health of populations,”</span> <em>Science</em>, vol. 366, pp. 447–453, Oct. 2019, doi: <a href="https://doi.org/10.1126/science.aax2342">10.1126/science.aax2342</a>.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[40] </div><div class="csl-right-inline">B. d’Alessandro, C. O’Neil, and T. LaGatta, <span>“Conscientious classification: A data scientist’s guide to discrimination-aware classification,”</span> <em>Big Data</em>, vol. 5, no. 2, pp. 120–134, 2017, doi: <a href="https://doi.org/10.1089/big.2016.0048">10.1089/big.2016.0048</a>.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[41] </div><div class="csl-right-inline">H. Suresh and J. Guttag, <span>“A framework for understanding sources of harm throughout the machine learning life cycle,”</span> 2021.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[42] </div><div class="csl-right-inline">T. Gebru <em>et al.</em>, <span>“Datasheets for datasets.”</span> 2020.Available: <a href="https://arxiv.org/abs/1803.09010">https://arxiv.org/abs/1803.09010</a></div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[43] </div><div class="csl-right-inline">M. Mitchell <em>et al.</em>, <span>“Model cards for model reporting,”</span> <em>Proceedings of the Conference on Fairness, Accountability, and Transparency</em>, 2019, doi: <a href="https://doi.org/10.1145/3287560.3287596">10.1145/3287560.3287596</a>.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[44] </div><div class="csl-right-inline">K. L. Calderone, <span>“The influence of gender on the frequency of pain and sedative medication administered to postoperative patients,”</span> <em>Sex Roles</em>, vol. 23, pp. 713–725, 1990, doi: <a href="https://doi.org/10.1007/BF00289259">https://doi.org/10.1007/BF00289259</a>.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[45] </div><div class="csl-right-inline">E. H. C. MD <em>et al.</em>, <span>“Gender disparity in analgesic treatment of emergency department patients with acute abdominal pain,”</span> <em>Academic Emergency Medicine</em>, vol. 15, pp. 414–418, May 2008, doi: <a href="https://doi.org/10.1111/j.1553-2712.2008.00100.x">https://doi.org/10.1111/j.1553-2712.2008.00100.x</a>.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[46] </div><div class="csl-right-inline">D. E. Hoffmann and A. J. Tarzian, <span>“The girl who cried pain: A bias against women in the treatment of pain,”</span> <em>SSRN</em>, 2001, doi: <a href="http://dx.doi.org/10.2139/ssrn.383803">http://dx.doi.org/10.2139/ssrn.383803</a>.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[47] </div><div class="csl-right-inline">K. M. Hoffman, S. Trawalter, J. R. Axt, and M. N. Oliver, <span>“Racial bias in pain assessment and treatment recommendations, and false beliefs about biological differences between blacks and whites,”</span> <em>Proceedings of the National Academy of Sciences</em>, vol. 113, no. 16, pp. 4296–4301, 2016, doi: <a href="https://doi.org/10.1073/pnas.1516047113">10.1073/pnas.1516047113</a>.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[48] </div><div class="csl-right-inline"><span>“The voice of 12,000 patients: Experiences and expectations of rare disease patients on diagnosis and care in europe.”</span> 2009.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[49] </div><div class="csl-right-inline">K. Fukuchi, J. Sakuma, and T. Kamishima, <span>“Prediction with model-based neutrality,”</span> <em>IEICE TRANS. INF. &amp; SYS.</em>, vol. E98–D, no. 8, 2015, doi: <a href="https://doi.org/Fukuchi">Fukuchi</a>.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[50] </div><div class="csl-right-inline">I. Zliobaite, <span>“On the relation between accuracy and fairness in binary classification.”</span> 2015.Available: <a href="https://arxiv.org/abs/1505.05723">https://arxiv.org/abs/1505.05723</a></div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[51] </div><div class="csl-right-inline">U. S. E. E. O. Commission, <span>“Questions and answers to clarify and provide a common interpretation of the uniform guidelines on employee selection procedures,”</span> <em>Federal Register</em>, vol. 44, no. 43, 1979.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[52] </div><div class="csl-right-inline">D. Pedreschi, S. Ruggieri, and F. Turini, <span>“Discrimination-aware data mining,”</span> in <em>Proceedings of the ACM SIGKDD international conference on knowledge discovery and data mining</em>, 2008, pp. 560–568. doi: <a href="https://doi.org/10.1145/1401890.1401959">10.1145/1401890.1401959</a>.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[53] </div><div class="csl-right-inline">M. B. Zafar, I. Valera, M. Gomez Rodriguez, and K. P. Gummadi, <span>“Fairness beyond disparate treatment &amp; disparate impact,”</span> <em>Proceedings of the 26th International Conference on World Wide Web</em>, 2017, doi: <a href="https://doi.org/10.1145/3038912.3052660">10.1145/3038912.3052660</a>.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[54] </div><div class="csl-right-inline">T. Calders, A. Karim, F. Kamiran, W. Ali, and X. Zhang, <span>“Controlling attribute effect in linear regression,”</span> 2013. doi: <a href="https://doi.org/10.1109/ICDM.2013.114">10.1109/ICDM.2013.114</a>.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[55] </div><div class="csl-right-inline">S. Barocas, M. Hardt, and A. Narayanan, <em>Fairness and machine learning</em>. fairmlbook.org, 2019.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[56] </div><div class="csl-right-inline">M. Hardt, E. Price, and N. Srebro, <span>“Equality of opportunity in supervised learning.”</span> 2016.Available: <a href="https://arxiv.org/abs/1610.02413">https://arxiv.org/abs/1610.02413</a></div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[57] </div><div class="csl-right-inline">L. T. Liu, M. Simchowitz, and M. Hardt, <span>“The implicit fairness criterion of unconstrained learning.”</span> 2019.Available: <a href="https://arxiv.org/abs/1808.10013">https://arxiv.org/abs/1808.10013</a></div>
</div>
</div>
</section>
</article>
<script>
var coll = document.getElementsByClassName("collapsible");
var i;
for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var content = this.nextElementSibling;
    if (content.style.display === "block") {
      content.style.display = "none";
    } else {
      content.style.display = "block";
    }
  });
}
</script>
</body>
</html>
