<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Dr Leena Murgai">
  <meta name="dcterms.date" content="2023-02-16">
  <meta name="google-site-verification" content="P2QDxs8DF7oWhzhn6gwF-hjhnxJHhfG71FOX0v56hf0" />
  <meta property="og:url" content="https://mitigatingbias.ml">
  <meta property="og:type" content="book">
  <meta property="og:title" content="Mitigating Bias in Machine Learning">
  <meta property="og:description" content="Mitigating Bias in Machine Learning discusses how practicing model developers might build fairer predictive systems, and avoid causing harm. Part I offers context (philosophical, legal, technical) and practical solutions. Part II discusses how we quantify different notions of fairness, where possible making connections with ideologies from other disciplines (discussed in part I). Part III analyses methods for mitigating bias, looking at the impact on the various metrics (discussed in part II).">
  <meta property="og:book:author" content="Leena Murgai">
  <meta property="og:image" content="https://raw.githubusercontent.com/leenamurgai/leenamurgai.github.io/main/profile/figures/SocialPreviewLandscape.png">
  <meta property="og:image:type" content="image/png">
  <meta property="og:image:width" content="1280">
  <meta property="og:image:height" content="640">
  <title>Mitigating Bias in Machine Learning</title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="tex2html/tufte/tufte.css">
  <link rel="stylesheet" href="tex2html/css/pandoc.css">
  <link rel="stylesheet" href="tex2html/css/navbar.css">
  <link rel="stylesheet" href="tex2html/css/tweak.css">
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<article>
<header>
<h1 class="title">Mitigating Bias in Machine Learning</h1>
<p class="byline"><a href="https://leenamurgai.co.uk" target="_blank" rel="noopener noreferrer">Dr Leena Murgai</a></p>
<p class="byline"><a href="https://github.com/leenamurgai/mitigatingbias.ml" target="_blank" rel="noopener noreferrer">16 February 2023</a></p>
<p class="byline"><a href="https://raw.githubusercontent.com/leenamurgai/mitigatingbias.ml/main/profile/mbml_citation.bib" target="_blank" rel="noopener noreferrer">Cite this book</a></p>
</header>
<div class="TOC">
<nav id="TOC">
    <div class="shortthickbar"></div>
    <div class="shortthickbar"></div>
    <div class="shortthickbar"></div>
<ul>
<li><a href="#part-i-introduction" id="toc-part-i-introduction">Part I Introduction</a></li>
<li><a href="#ch_Background" id="toc-ch_Background"><span class="toc-section-number">1</span> Context</a>
<ul>
<li><a href="#bias-in-machine-learning" id="toc-bias-in-machine-learning"><span class="toc-section-number">1.1</span> Bias in Machine Learning</a></li>
<li><a href="#sec_FairnessJustice" id="toc-sec_FairnessJustice"><span class="toc-section-number">1.2</span> A Philosophical Perspective</a></li>
<li><a href="#a-legal-perspective" id="toc-a-legal-perspective"><span class="toc-section-number">1.3</span> A Legal Perspective</a></li>
<li><a href="#sec_SimpsParadox" id="toc-sec_SimpsParadox"><span class="toc-section-number">1.4</span> A Technical Perspective</a></li>
<li><a href="#sec_harms" id="toc-sec_harms"><span class="toc-section-number">1.5</span> What’s the Harm?</a></li>
<li><a href="#summary" id="toc-summary">Summary</a></li>
</ul></li>
<li><a href="#ch_EthicalDev" id="toc-ch_EthicalDev"><span class="toc-section-number">2</span> Ethical development</a>
<ul>
<li><a href="#machine-learning-cycle" id="toc-machine-learning-cycle"><span class="toc-section-number">2.1</span> Machine Learning Cycle</a></li>
<li><a href="#sec_ResponseDev" id="toc-sec_ResponseDev"><span class="toc-section-number">2.2</span> Model Development and Deployment Life Cycle</a></li>
<li><a href="#sec_ProcessPolicy" id="toc-sec_ProcessPolicy"><span class="toc-section-number">2.3</span> Responsible Model Development and Deployment</a></li>
<li><a href="#common-causes-of-harm" id="toc-common-causes-of-harm"><span class="toc-section-number">2.4</span> Common Causes of Harm</a></li>
<li><a href="#linking-common-causes-of-harm-to-the-workflow" id="toc-linking-common-causes-of-harm-to-the-workflow"><span class="toc-section-number">2.5</span> Linking Common Causes of Harm to the Workflow</a></li>
<li><a href="#summary-1" id="toc-summary-1">Summary</a></li>
</ul></li>
<li><a href="#part-ii-measuring-bias" id="toc-part-ii-measuring-bias">Part II Measuring Bias</a></li>
<li><a href="#ch_GroupFairness" id="toc-ch_GroupFairness"><span class="toc-section-number">3</span> Group Fairness</a>
<ul>
<li><a href="#sec_BalOut" id="toc-sec_BalOut"><span class="toc-section-number">3.1</span> Comparing Outcomes</a></li>
<li><a href="#sec_BalErr" id="toc-sec_BalErr"><span class="toc-section-number">3.2</span> Comparing Errors</a></li>
<li><a href="#sec_Impossible" id="toc-sec_Impossible"><span class="toc-section-number">3.3</span> Incompatibility Between Fairness Criteria</a></li>
<li><a href="#concluding-remarks" id="toc-concluding-remarks"><span class="toc-section-number">3.4</span> Concluding Remarks</a></li>
<li><a href="#summary-2" id="toc-summary-2">Summary</a></li>
</ul></li>
<li><a href="#ch_IndividualFairness" id="toc-ch_IndividualFairness"><span class="toc-section-number">4</span> Individual Fairness</a>
<ul>
<li><a href="#individual-fairness-as-continuity" id="toc-individual-fairness-as-continuity"><span class="toc-section-number">4.1</span> Individual Fairness as Continuity</a></li>
<li><a href="#individual-fairness-as-randomness" id="toc-individual-fairness-as-randomness"><span class="toc-section-number">4.2</span> Individual Fairness as Randomness</a></li>
<li><a href="#similarity-metrics" id="toc-similarity-metrics"><span class="toc-section-number">4.3</span> Similarity Metrics</a></li>
<li><a href="#measuring-individual-fairness-in-practice" id="toc-measuring-individual-fairness-in-practice"><span class="toc-section-number">4.4</span> Measuring Individual Fairness in Practice</a></li>
<li><a href="#summary-3" id="toc-summary-3">Summary</a></li>
</ul></li>
<li><a href="#ch_UtilityFairness" id="toc-ch_UtilityFairness"><span class="toc-section-number">5</span> Fairness as Utility</a>
<ul>
<li><a href="#generalised-entropy-indices" id="toc-generalised-entropy-indices"><span class="toc-section-number">5.1</span> Generalised Entropy Indices</a></li>
<li><a href="#defining-a-benefit-function" id="toc-defining-a-benefit-function"><span class="toc-section-number">5.2</span> Defining a Benefit Function</a></li>
<li><a href="#fairness-as-utility" id="toc-fairness-as-utility"><span class="toc-section-number">5.3</span> Fairness as Utility</a></li>
<li><a href="#summary-4" id="toc-summary-4">Summary</a></li>
</ul></li>
<li><a href="#app_Notation" id="toc-app_Notation"><span class="toc-section-number">A</span> Notation and Conventions</a></li>
<li><a href="#app_Metrics" id="toc-app_Metrics"><span class="toc-section-number">B</span> Performance Metrics</a></li>
<li><a href="#app_ProbRules" id="toc-app_ProbRules"><span class="toc-section-number">C</span> Rules of Probability</a></li>
<li><a href="#app_Solutions" id="toc-app_Solutions"><span class="toc-section-number">D</span> Proofs and Code</a>
<ul>
<li><a href="#sec_app_GFSolutions" id="toc-sec_app_GFSolutions"><span class="toc-section-number">D.1</span> Group Fairness</a></li>
<li><a href="#sec_app_IFSolutions" id="toc-sec_app_IFSolutions"><span class="toc-section-number">D.2</span> Individual Fairness</a></li>
<li><a href="#sec_app_IISolutions" id="toc-sec_app_IISolutions"><span class="toc-section-number">D.3</span> Utility as Fairness</a></li>
</ul></li>
<li><a href="#app_AIF360" id="toc-app_AIF360"><span class="toc-section-number">E</span> AIF360</a>
<ul>
<li><a href="#app_AIF360_GF" id="toc-app_AIF360_GF"><span class="toc-section-number">E.1</span> Group Fairness</a></li>
<li><a href="#app_AIF360_IF" id="toc-app_AIF360_IF"><span class="toc-section-number">E.2</span> Individual Fairness</a></li>
<li><a href="#app_AIF360_II" id="toc-app_AIF360_II"><span class="toc-section-number">E.3</span> Utility as Fairness</a></li>
</ul></li>
<li><a href="#bibliography" id="toc-bibliography">References</a></li>
</ul>
</nav>
</div>
<div id="collapsiblemenu">
  <button class="collapsible">
    <div class="shortthickbar"></div>
    <div class="shortthickbar"></div>
    <div class="shortthickbar"></div>
  </button>
  <div class="content">
    <ul>
    <li><a href="#part-i-introduction" id="toc-part-i-introduction">Part I Introduction</a></li>
    <li><a href="#ch_Background" id="toc-ch_Background"><span class="toc-section-number">1</span> Context</a>
    <ul>
    <li><a href="#bias-in-machine-learning" id="toc-bias-in-machine-learning"><span class="toc-section-number">1.1</span> Bias in Machine Learning</a></li>
    <li><a href="#sec_FairnessJustice" id="toc-sec_FairnessJustice"><span class="toc-section-number">1.2</span> A Philosophical Perspective</a></li>
    <li><a href="#a-legal-perspective" id="toc-a-legal-perspective"><span class="toc-section-number">1.3</span> A Legal Perspective</a></li>
    <li><a href="#sec_SimpsParadox" id="toc-sec_SimpsParadox"><span class="toc-section-number">1.4</span> A Technical Perspective</a></li>
    <li><a href="#sec_harms" id="toc-sec_harms"><span class="toc-section-number">1.5</span> What’s the Harm?</a></li>
    <li><a href="#summary" id="toc-summary">Summary</a></li>
    </ul></li>
    <li><a href="#ch_EthicalDev" id="toc-ch_EthicalDev"><span class="toc-section-number">2</span> Ethical development</a>
    <ul>
    <li><a href="#machine-learning-cycle" id="toc-machine-learning-cycle"><span class="toc-section-number">2.1</span> Machine Learning Cycle</a></li>
    <li><a href="#sec_ResponseDev" id="toc-sec_ResponseDev"><span class="toc-section-number">2.2</span> Model Development and Deployment Life Cycle</a></li>
    <li><a href="#sec_ProcessPolicy" id="toc-sec_ProcessPolicy"><span class="toc-section-number">2.3</span> Responsible Model Development and Deployment</a></li>
    <li><a href="#common-causes-of-harm" id="toc-common-causes-of-harm"><span class="toc-section-number">2.4</span> Common Causes of Harm</a></li>
    <li><a href="#linking-common-causes-of-harm-to-the-workflow" id="toc-linking-common-causes-of-harm-to-the-workflow"><span class="toc-section-number">2.5</span> Linking Common Causes of Harm to the Workflow</a></li>
    <li><a href="#summary-1" id="toc-summary-1">Summary</a></li>
    </ul></li>
    <li><a href="#part-ii-measuring-bias" id="toc-part-ii-measuring-bias">Part II Measuring Bias</a></li>
    <li><a href="#ch_GroupFairness" id="toc-ch_GroupFairness"><span class="toc-section-number">3</span> Group Fairness</a>
    <ul>
    <li><a href="#sec_BalOut" id="toc-sec_BalOut"><span class="toc-section-number">3.1</span> Comparing Outcomes</a></li>
    <li><a href="#sec_BalErr" id="toc-sec_BalErr"><span class="toc-section-number">3.2</span> Comparing Errors</a></li>
    <li><a href="#sec_Impossible" id="toc-sec_Impossible"><span class="toc-section-number">3.3</span> Incompatibility Between Fairness Criteria</a></li>
    <li><a href="#concluding-remarks" id="toc-concluding-remarks"><span class="toc-section-number">3.4</span> Concluding Remarks</a></li>
    <li><a href="#summary-2" id="toc-summary-2">Summary</a></li>
    </ul></li>
    <li><a href="#ch_IndividualFairness" id="toc-ch_IndividualFairness"><span class="toc-section-number">4</span> Individual Fairness</a>
    <ul>
    <li><a href="#individual-fairness-as-continuity" id="toc-individual-fairness-as-continuity"><span class="toc-section-number">4.1</span> Individual Fairness as Continuity</a></li>
    <li><a href="#individual-fairness-as-randomness" id="toc-individual-fairness-as-randomness"><span class="toc-section-number">4.2</span> Individual Fairness as Randomness</a></li>
    <li><a href="#similarity-metrics" id="toc-similarity-metrics"><span class="toc-section-number">4.3</span> Similarity Metrics</a></li>
    <li><a href="#measuring-individual-fairness-in-practice" id="toc-measuring-individual-fairness-in-practice"><span class="toc-section-number">4.4</span> Measuring Individual Fairness in Practice</a></li>
    <li><a href="#summary-3" id="toc-summary-3">Summary</a></li>
    </ul></li>
    <li><a href="#ch_UtilityFairness" id="toc-ch_UtilityFairness"><span class="toc-section-number">5</span> Fairness as Utility</a>
    <ul>
    <li><a href="#generalised-entropy-indices" id="toc-generalised-entropy-indices"><span class="toc-section-number">5.1</span> Generalised Entropy Indices</a></li>
    <li><a href="#defining-a-benefit-function" id="toc-defining-a-benefit-function"><span class="toc-section-number">5.2</span> Defining a Benefit Function</a></li>
    <li><a href="#fairness-as-utility" id="toc-fairness-as-utility"><span class="toc-section-number">5.3</span> Fairness as Utility</a></li>
    <li><a href="#summary-4" id="toc-summary-4">Summary</a></li>
    </ul></li>
    <li><a href="#app_Notation" id="toc-app_Notation"><span class="toc-section-number">A</span> Notation and Conventions</a></li>
    <li><a href="#app_Metrics" id="toc-app_Metrics"><span class="toc-section-number">B</span> Performance Metrics</a></li>
    <li><a href="#app_ProbRules" id="toc-app_ProbRules"><span class="toc-section-number">C</span> Rules of Probability</a></li>
    <li><a href="#app_Solutions" id="toc-app_Solutions"><span class="toc-section-number">D</span> Proofs and Code</a>
    <ul>
    <li><a href="#sec_app_GFSolutions" id="toc-sec_app_GFSolutions"><span class="toc-section-number">D.1</span> Group Fairness</a></li>
    <li><a href="#sec_app_IFSolutions" id="toc-sec_app_IFSolutions"><span class="toc-section-number">D.2</span> Individual Fairness</a></li>
    <li><a href="#sec_app_IISolutions" id="toc-sec_app_IISolutions"><span class="toc-section-number">D.3</span> Utility as Fairness</a></li>
    </ul></li>
    <li><a href="#app_AIF360" id="toc-app_AIF360"><span class="toc-section-number">E</span> AIF360</a>
    <ul>
    <li><a href="#app_AIF360_GF" id="toc-app_AIF360_GF"><span class="toc-section-number">E.1</span> Group Fairness</a></li>
    <li><a href="#app_AIF360_IF" id="toc-app_AIF360_IF"><span class="toc-section-number">E.2</span> Individual Fairness</a></li>
    <li><a href="#app_AIF360_II" id="toc-app_AIF360_II"><span class="toc-section-number">E.3</span> Utility as Fairness</a></li>
    </ul></li>
    <li><a href="#bibliography" id="toc-bibliography">References</a></li>
    </ul>
  </div>
</div>
<section id="part-i-introduction" class="level1 unnumbered">
<h1 class="unnumbered">Part I Introduction</h1>
<p>Welcome to Mitigating Bias in Machine Learning. If you’ve made it here chances are you’ve worked with models and have some awareness of the problem of biased machine learning algorithms. You might be a student with a foundational course in machine learning under your belt, or a Data Scientist or Machine Learning Engineer, concerned about the impact your models might have on the world.</p>
<p>In this book we are going to learn and analyse a whole host of techniques for measuring and mitigating bias in machine learning models. We’re going to compare them, in order to understand their strengths and weaknesses. Mathematics is an important part of modelling, and we won’t shy away from it. Where possible, we will aim to take a mathematically rigorous approach to answering questions.</p>
<p>Mathematics, just like code, can contain bugs. In this book, each has been used to verify the other. The analysis in this book, was completed using Python. The <a href="https://github.com/leenamurgai/mitigatingbias.ml/tree/main/code">Jupyter Notebooks</a> are available on GitHub, for those who would like to see/use them. That said, this book is intended to be self contained, and does not contain code. We will focus on the concepts, rather than the implementation.</p>
<p>Mitigating Bias in Machine Learning is ultimately about fairness. The goal of this book is to understand how we, as practising model developers, might build fairer predictive systems and avoid causing harm (sometimes that might mean not building something at all). There are many facets to solving a problem like this, not all of them involve equations and code. The first two chapters (part I) are dedicated to discussing these.</p>
<p>In a sense, over the course of the book, we will zoom in on the problem, or rather narrow our perspective. In chapter 1, we’ll discuss philosophical, political, legal, technical and social perspectives. In chapter two we take a more practical view on the problem of ethical development (how to build and organise the development of models, with a view to reducing ethical risk).</p>
<p>In part II we will talk about how we quantify different notions of fairness.</p>
<p>In part III, we will look at methods for mitigating bias through model interventions and analyse their impact.</p>
<p>Let’s get started.</p>
</section>
<section id="ch_Background" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Context</h1>
<div class="chapsumm">
<p><strong>This chapter at a glance</strong></p>
<ul>
<li><p>Problems with machine learning in sociopolitical domains</p></li>
<li><p>Contrasting socio-political theories of fairness in decision systems</p></li>
<li><p>The history, application and interpretation of anti-discrimination law</p></li>
<li><p>Association paradoxes and the difficulty in identifying bias</p></li>
<li><p>The different types of harm caused by biased systems</p></li>
</ul>
</div>
<p>The goal of this chapter is to shed light on the problem of bias in machine learning, from a variety of different perspectives. The word <em>bias</em> can mean many things but in this book, we use it interchangeably with the term <em>unfairness</em>. We’ll talk about why later.</p>
<p>Perhaps the biggest challenge in developing <em>sociotechnical systems</em> is that it inevitably involve questions which are social, philosophical, political, and legal in nature; questions to which there is often no definitive answer but rather competing viewpoints and trade-offs to be made. As we’ll see, this does not change when we attempt to quantify the problem. There are many multiple definitions of fairness that have been proven to be impossible to satisfy simultaneously. The problem of bias in sociotechnical systems is very much an interdisciplinary one and, in this chapter, we discuss them as such. We will make connections between concepts and language from the various subjects over the course of this book.</p>
<p>In this chapter we shall discuss some philosophical theories of fairness in sociopolitical systems and consider how they might relate to model training and fairness criteria. We’ll take a legal perspective, looking at anti-discrimination laws in the US as an example. We’ll discuss some of the history behind and practical application of them; and the tensions that exist in their interpretation. Data can be misleading; correlation does not imply causation which is why domain knowledge in building sociotechnical systems is imperative. We will discuss the technical difficulty in identifying bias in static data through illustrative examples of Simpson’s paradox. Finally, we’ll discuss why it’s important to consider the fairness of automated systems. We’ll finish the chapter by discussing some of the different types of harm caused by biased machine learning systems, not just allocative but representational harms which are currently less well defined and potentially valuable research areas.</p>
<p>Let’s start by describing the types of problems we are interested in.</p>
<section id="bias-in-machine-learning" class="level2" data-number="1.1">
<h2 data-number="1.1"><span class="header-section-number">1.1</span> Bias in Machine Learning</h2>
<p>Machine learning can be described as the study of computer algorithms that improve with (or learn) experience. It can be broadly subdivided into the fields of supervised, unsupervised and reinforcement learning.</p>
<section id="supervised-learning" class="level5 unnumbered">
<h5 class="unnumbered">Supervised learning</h5>
<p>For supervised learning problems, the experience come in the form of labelled training data. Given a set of features <span class="math inline">\(X\)</span> and labels (or targets) <span class="math inline">\(Y\)</span>, we want to learn a function or mapping <span class="math inline">\(f\)</span>, such that <span class="math inline">\(Y = f(X)\)</span>, where <span class="math inline">\(f\)</span> generalizes to previously unseen data.</p>
</section>
<section id="unsupervised-learning" class="level5 unnumbered">
<h5 class="unnumbered">Unsupervised learning</h5>
<p>For unsupervised learning problems there are no labels <span class="math inline">\(Y\)</span>, only features <span class="math inline">\(X\)</span>. Instead we are interested in looking for patterns and structure in the data. For example, we might want to subdivide the data into clusters of points with similar (previously unknown) characteristics or we might want to reduce the dimensionality of the data (to be able to visualize it or simply to make a supervised learning algorithm more efficient). In other words, we are looking for a new feature <span class="math inline">\(Y\)</span> and the mapping <span class="math inline">\(f\)</span> from <span class="math inline">\(X\)</span> to <span class="math inline">\(Y\)</span>.</p>
</section>
<section id="reinforcement-learning" class="level5 unnumbered">
<h5 class="unnumbered">Reinforcement learning</h5>
<p>Reinforcement learning is concerned with the problem of optimally navigating a state space to reach a goal state. The problem is framed as an agent that takes actions, which result in rewards (or penalties). The task is then to maximize the cumulative reward. As with unsupervised learning, the agent is not given a set of examples of optimal actions in various states, but rather must learn them through trial and error. A key aspect of reinforcement learning is the existence of a trade-off between exploration (searching unexplored territory in the hope of finding a better choice) and exploitation (exploiting what has been learned so far).</p>
<p>In this we will focus on the first two categories (essentially algorithms that capture and or exploit patterns in data), primarily because these are the fields in which problems related to bias in machine learning are most pertinent (automation and prediction). As one would expect then, these are also the areas in which many of the technical developments in measuring and mitigating bias have been concentrated.</p>
<p>The idea that the kinds of technologies described above are <em>learning</em> is an interesting one. The analogy is clear, learning by example is certainly a way to learn. In less modern disciplines one might simply think of <em>training</em> a model as; solving an equation, interpolating data, or optimising model parameters. So where does the terminology come from? The term <em>machine learning</em> was coined by Arthur Samuel in the 1950’s when, at IBM, he developed an algorithm capable of playing draughts (checkers). By the mid 70’s his algorithm was competitive at amateur level. Though it was not called reinforcement learning at the time, the algorithm was one of the earliest implementations of such ideas. Samuel used the term <em>rote learning</em> to describe a memorisation technique he implemented where the machine remembered all the states it had visited and the corresponding reward function, in order to extend the search tree.</p>
</section>
<section id="what-is-a-model" class="level3" data-number="1.1.1">
<h3 data-number="1.1.1"><span class="header-section-number">1.1.1</span> What is a Model?</h3>
<p>Underlying every machine learning algorithm is a model (often several of them) and these have been around for millennia. Based on the discovery of palaeolithic tally sticks (animal bones carved with notches) it’s believed that humans have kept numerical records for over 40,000 years. The earliest mathematical models (from around 4,000 BC) were geometric and used to advance the fields of astronomy and architecture. By 2,000 BC, mathematical models were being used in an algorithmic manner to solve specific problems by at least three civilizations (Babylon, Egypt and India).</p>
<p>A model is a simplified representation of some real world phenomena. It is an expression of the relationship between things; a function or mapping which, given a set of input variables (features), returns a decision or prediction (target). A model can be determined with the help of data, but it need not be. It can simply express an opinion as to how things should be related.</p>
<p>If we have a model which represents a theoretical understanding of the world (under a series of simplifying assumptions) we can test it by measuring and comparing the results to reality. Based on the results we can assess how accurate our understanding of the world was and update our model accordingly. In this way, making simplifying assumptions can be a means to iteratively improve our understanding of the world. Models play an incredibly important role in the pursuit of knowledge. They have provided a mechanism to understand the world around us, and explain why things behave as they do; to prove that the earth could not be flat, explain why the stars move and shift in brightness as they do or, (somewhat) more recently in the case of my PhD, explain why supersonic flows behave uncharacteristically, when a shock wave encounters a vortex.</p>
<p>As the use of models has been adopted by industry, increasingly their purpose has been geared towards prediction and automation, as a way to monetize that knowledge. But the pursuit of profit inevitably creates conflicts of interests. If your goal is to learn more, finding out where your theory is wrong and fixing it is the goal. In business, less so. I recall a joke I heard at school describing how one could tell which field of science an experiment belonged to. If it changes colour, it’s biology; if it explodes, it’s chemistry and if it doesn’t work, it’s physics. Models of real world phenomena fail. They are, by their very nature, a reductive representation of an infinitely more complex real world system. Obtaining adequately rich and relevant data is a major limitation of machine learning models and yet, they are increasingly being applied to problems, where that kind of data simply doesn’t exist.</p>
</section>
<section id="sociotechnical-systems" class="level3" data-number="1.1.2">
<h3 data-number="1.1.2"><span class="header-section-number">1.1.2</span> Sociotechnical systems</h3>
<p>We use the term <em>sociotechnical systems</em> to describe systems that involve algorithms that manage people. They make efficient decisions for and about us, determine what we see, direct us and more. But managing large numbers of people inevitably exerts a level of authority and control. An extreme example is the adoption of just-in-time scheduling algorithms by large retailers in the US to manage staffing needs. To predict footfall, the algorithms take into account everything from weather forecasts to sporting events. The cost of this efficiency is passed onto employees. The number of hours allocated are optimised to fall short of qualifying for costly health insurance. Employees are subjected to haphazard schedules that prevent them from being able to prioritise anything other than work; eliminating the possibility of any opportunity that might enable them to advance beyond the low-wage work pool.</p>
<p>Progress in the field of deep learning combined with increased availability and decreased cost of computational resources has led to an explosion in data and model use. Automation seemingly offers a path to making our lives easier, improving the efficiency and efficacy of the many industries we transact with day to day; but there are growing and legitimate concerns over how the benefit (and cost) of these efficiencies are distributed. Machine learning is already being used to automate decisions in just about every aspect of modern life; deciding which adverts to show to whom, deciding which transactions might be fraud when we shop, deciding who is able to access to financial services such as loans and credit cards, determining our treatment when sick, filtering candidates for education and employment opportunities, in determining which neighbourhoods to police and even in the criminal justice system to decide what level bail should be set at, or the length of a given sentence. At almost every major life event, going to university, getting a job, buying a house, getting sick, decisions are being made by machines.</p>
</section>
<section id="what-kind-of-bias" class="level3" data-number="1.1.3">
<h3 data-number="1.1.3"><span class="header-section-number">1.1.3</span> What Kind of Bias?</h3>
<p>The word <em>bias</em> is rather overloaded; it has numerous different interpretations even within the same discipline. Let’s talk about the kinds of biases that are relevant here. The word bias is used to describe systematic errors in variable estimation (predictions) from data. If the goal is to create systems that work similarly well for all types of people, we certainly want to avoid these. In a social context, bias is spoken of as prejudice or discrimination in a given context, based on characteristics that we as a society deem to be unacceptable or unfair (for example hiring practices that systematically disadvantage women). Mitigating bias though is not just about avoiding discriminating, it can also manifest when a system fails to adequately discriminate based on characteristics that are relevant to the problem (for example systematically higher rates of error in visual recognition systems for darker skinned individuals). Systemic bias and discrimination are observed in data in numerous ways; historical decisions of course are susceptible, but more importantly perhaps in the very definition of the categories, who is recognised and who is erased. Bias need not be conscious, in reality it starts at the very inception of technology, in deciding which problems are worth solving in the first place. Bias exists in how we measure the cost and benefit of new technologies. For sociotechnical systems, these are all deeply intertwined.</p>
<p>Ultimately, mitigating bias in our models is about fairness and in this book we shall use the terms interchangeably. Machine learning models are capable of not only of proliferating existing societal biases, but amplifying them, and are easily deployed at scale. But how do we even define fairness? And from whose perspective do we mean fair? The law can provide <em>some</em> context here. Laws, in many cases, define <em>protected</em> characteristics and domains (we’ll talk more about these later). We can potentially use these as a guide and we certainly have a responsibility to be law abiding citizens. A common approach historically has been to ignore protected characteristics. There’s a few reasons for this. One reason is the false belief that, an algorithm cannot discriminate based on features not included in the data. This assumption is is easy to disprove with a counter example. A reasonably fool-proof way to systematically discriminate by race or rather ethnicity (without explicitly using it), is to discriminate by location/residence; that is, another variable that’s strongly correlated and serves as a proxy. The legality of this practice depends on the domain. In truth, you don’t need a feature, or a proxy, to discriminate based on it, you just need enough data, to be able to predict it. If it is predictable, the information there and the algorithm is likely using it. Another reason for ignoring protected features is avoiding legal liability (we’ll talk more about this when we take a legal perspective later in the chapter).</p>
<section id="example-amazon-prime-same-day-delivery-service" class="level4 unnumbered">
<h4 class="unnumbered">Example: Amazon Prime same day delivery service</h4>
<p>In 2016, analysis published by Bloomberg uncovered racial disparities in eligibility for Amazon’s same day delivery services for Prime customers<span class="sidenote-wrapper"><label for="sn-0" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-0" class="margin-toggle"/><span class="sidenote">To be clear, the same day delivery was free for eligible Amazon Prime customers on sales exceeding $35. Amazon Prime members pay a fixed annual subscription fee, thus the disparity is in the level of service provided for Prime customers who are eligible verses those that are not.<br />
<br />
</span></span><span class="citation" data-cites="AmazonSameDayPrime"><a href="#ref-AmazonSameDayPrime" role="doc-biblioref">[1]</a></span><span class="marginnote"><span id="ref-AmazonSameDayPrime" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[1] </span><span class="csl-right-inline">D. Ingold and S. Soper, <span>“Amazon doesn’t consider the race of its customers. Should it?”</span> <em>Bloomberg</em>, 2016.</span>
</span>
</span>. The study used census data to identify Black and White residents and plot the data points on city maps which simultaneously showed the areas that qualified for the Prime customer same day delivery. The disparities are glaring at a glance. In six major cities, New York, Boston, Atlanta, Chicago, Dallas, and Washington, DC where the service did not have broad coverage, it was mainly Black neighbourhoods that were ineligible. In the latter four cities, Black residents were about half as likely to live in neighbourhoods eligible for Amazon same-day delivery as White residents.</p>
<p>At the time Amazon’s process in determining which ZIP codes to serve was reportedly a cost benefit calculation that did not explicitly take race into account but for those who have seen redlining maps from the 1930’s is hard to not see the resemblance. Redlining was the (now illegal) practice of declining (or raising prices for) financial products to people based on the neighbourhood where they lived. Because neighbourhoods were racially segregated (a legacy that lives on today), public and private institutions were able to systematically exclude minority populations from the housing market and deny loans for house improvements without explicitly taking race into account. Between 1934 and 1962, the Federal Housing Administration distributed $120 billion in loans. Thanks to redlining, 98% of these went to White families.</p>
<p>Amazon is a private enterprise, and it is legally entitled to make decisions about where to offer services based on how profitable it is. Some might argue they have a right to be able to make those decisions. Amazon is not responsible for the injustices that created such racial disparities, but the reality is that such disparities in access to goods and services perpetuate it. If same-day delivery sounds like a luxury, it’s worth considering the context. The cities affected have a long histories of racial segregation and economic inequality resulting from systemic racism, now deemed illegal. They are neighbourhoods which to this day are underserved by brick and mortar retailers, where residents are forced to travel further and pay more for household essentials. Now we are in the midst of a pandemic, where once delivery of household goods used to be a luxury, with so many forced to quarantine, suddenly it’s become far more of a necessity. What we consider to be a necessity changes over time, it depends on where one lives, one’s circumstances and more. Finally, consider the scale of Amazon’s operations, in 2016 one third of retail e-commerce spending in the US was with Amazon (that number has since risen to almost 50%).</p>
</section>
</section>
</section>
<section id="sec_FairnessJustice" class="level2" data-number="1.2">
<h2 data-number="1.2"><span class="header-section-number">1.2</span> A Philosophical Perspective</h2>
<p>Developing a model is not an objective scientific process, it involves making a series subjective choices. Cathy O’Neil describes them as “opinions embedded in code”. One of the most fundamental ways in which we impose our opinion on a machine learning model, is in deciding how we measure success. Let’s look at the process of training a model. We start with some parametric representation (a family of models), which you hope is sufficiently complex to be able to reflect the relationships between the variables in the data. The goal in training is to determine which model (in our chosen family) is <em>best</em>. The <em>best</em> model being the one that maximises it’s utility (from the model developers perspective).</p>
<p>For sociotechnical systems, our predictions don’t only impact the decision maker, they also result in a benefit (or harm) to those subjected to them. The very purpose of codifying a decision policy is often to cheaply deploy it at scale. The more people it processes, the more value there is in codifying the decision process. Another, way to look such models instead then, is as a system for distributing benefits (or harms) among a population. Given this, which model is the <em>right</em> one so to speak. In this section we briefly discuss some more philosophical theories relevant to these types of problems. We start with utilitarianism which is perhaps the easiest theory to draw parallels with in modelling.</p>
<section id="utilitarianism" class="level3" data-number="1.2.1">
<h3 data-number="1.2.1"><span class="header-section-number">1.2.1</span> Utilitarianism</h3>
<p>Utilitarianism provides a framework for moral reasoning in decision making. Under this framework, the correct course of action, when faced with a dilemma, is the one that maximises the benefit for the greatest number of people. The doctrine demands that the benefits to all people are are counted equally. Variations of the theory have evolved over the years. Some differ in their notion of how benefits are understood. Others distinguish between the quality of various kinds of benefit. In a business context, one might consider it as financial benefit (and cost). Although, this in itself depends on one’s perspective. Some doctrines advocate that the impact of the action in isolation should be considered, while others ask what the impact would be if everyone in the population took the same actions.</p>
<p>There are some practical problems with utilitarianism as the sole guiding principle for decision making. How do we measure benefit? How do we navigate the complexities of placing a value on immeasurable and vastly different consequences? What is a life, time, money or particular emotion worth and how do we compare and aggregate them? How can one even be certain of the consequences? Longer term consequences are hard if not impossible to predict. Perhaps the most significant flaw in utilitarianism for moral reasoning, is the omission of justice as a consideration.</p>
<p>Utilitarian reasoning judges actions based solely on consequences, and aggregates them over a population. So, if an action that unjustly harms a minority group happens to be the one that maximises the aggregate benefit over a population, it is nevertheless the correct action to take. Under utilitarianism, theft or infidelity might be morally justified, if those it would harm are none the wiser. Or punishing an innocent person for a crime they did not commit could be justified, if it served to quell unrest among a population. For this reason it is widely accepted that utilitarianism is insufficient as a framework for decision making.</p>
<p>Utilitarianism is a flavour of consequentialism, a branch of ethical theory that holds that consequences are the yard stick against which we must judge the morality of our actions. In contrast deontological ethics judges the morality of actions against a set of rules that define our duties or obligations towards others. Here it is not the consequences of our actions that matter but rather intent.</p>
<p>The conception of utilitarianism is attributed to British philosopher Jeremy Bentham who authored the first major book on the topic <em>An Introduction to the Principles of Morals and Legislation</em> in 1780. In it Bentham argues that, it is the pursuit of pleasure and avoidance of pain alone that motivate individuals to act. Given this he saw utilitarianism as a principle by which to govern. Broadly speaking, the role of government, in his view, was to assign rewards or punishments to actions, in proportion to the happiness or suffering they produced among the governed. At the time, the idea that the well-being of all people should be counted equally, and that that morality of actions should be judged accordingly was revolutionary. Bentham was a progressive in his time, he advocated for women’s rights (to vote, hold office and divorce), decriminalisation of homosexual acts, prison reform and the abolition of slavery and more. He argued many of his beliefs as a simple economic calculation of how much happiness they would produce. Importantly, he didn’t claim that all people were equal, but rather only that their happiness mattered equally.</p>
<p>Times have changed. Over the last century, as civil rights have advanced, the weaknesses of utilitarianism in practice have been exposed time and time again. Utilitarian reasoning has increasingly been seen as hindering social progress, rather than advancing it. For example, utilitarian arguments were used by Whites in apartheid South Africa, who claimed that all South Africans were better-off under White rule, and that a mixed government would lead to social decline as it had in other African nations. Utilitarian reasoning has been used widely by capitalist nations in the form of trickle-down economics. The theory being that the benefits of tax-breaks for the wealthy drive economic growth and ‘trickle-down’ to the rest of the population. But evidence suggests that trickle-down economic policies in more recent decades have done more damage than good, increasing national debt and fuelling income inequality. Utilitarian principles have also been tested in the debate over torture, capturing a rather callous conviction, one where the ‘means justify the ends’.</p>
<p>Historian and author, Yuval Noah Harari has eloquently abstracted this problem. He argues that historically, decentralization of power and efficiency have aligned; so much so, that many of us cannot think of democracy as being capable of failing, to more totalitarian regimes. But in this new age, data is power. We can train enormous models, that require vast amounts of data, to process people en masse, organise and sort them. And importantly, one does not have to have a perfect system in order to have an impact because of the scale on which they can be deployed. The question Yuval poses is, <em>might the benefits of centralised data, offer a great enough advantage, to tip the balance of efficiency, in favour of more centralised models of power?</em></p>
</section>
<section id="justice-as-fairness" class="level3" data-number="1.2.2">
<h3 data-number="1.2.2"><span class="header-section-number">1.2.2</span> Justice as Fairness</h3>
<p>In his theory Justice As Fairness<span class="citation" data-cites="JusticeFairness"><a href="#ref-JusticeFairness" role="doc-biblioref">[2]</a></span><span class="marginnote"><span id="ref-JusticeFairness" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[2] </span><span class="csl-right-inline">J. Rawls, <em>Justice as fairness: A restatement</em>. Cambridge, Mass.: Harvard University Press, 2001.</span>
</span>
</span>, John Rawls takes a different approach. He describes an idealised democratic framework, based on liberal principles and explains how unified laws can be applied (in a free society made up of people with disparate world views) to create a stable sociopolitical system. One where citizens would not only freely co-operate, but further advocate. He described a political conception of justice which would:</p>
<ol>
<li><p>grant all citizens a set of basic rights and liberties</p></li>
<li><p>give special priority to the aforementioned rights and liberties over demands to further the general good, e.g. increasing the national wealth</p></li>
<li><p>assure all citizens sufficient means to make use of their freedoms.</p></li>
</ol>
<p>The special priority given to the basic rights and liberties in the political conception of justice contrasts with a utilitarian doctrine. Here constraints are placed on how benefits can be distributed among the population and a strategy for determining some minimum.</p>
<section id="principles-of-justice-as-fairness" class="level4 unnumbered">
<h4 class="unnumbered">Principles of Justice as Fairness</h4>
<ol>
<li><p><strong>Liberty principle:</strong> Each person has the same indefeasible claim to a fully adequate scheme of equal basic liberties, which is compatible with the same scheme of liberties for all;</p></li>
<li><p><strong>Equality principle:</strong> Social and economic inequalities are to satisfy two conditions:</p>
<ol>
<li><p><strong>Fair equality of opportunity:</strong> The offices and positions to which they are attached are open to all, under conditions of fair equality of opportunity;</p></li>
<li><p><strong>Difference (maximin) principle</strong> They must be of the greatest benefit to the least-advantaged members of society.</p></li>
</ol></li>
</ol>
<p>The principles of Justice as Fairness are ordered by priority so that fulfilment of the liberty principle takes precedence over the equality principles and fair equality of opportunity takes precedence over the difference principle.</p>
<p>The first principle grants basic rights and liberties to all citizens which are prioritised above all else and cannot be traded for other societal benefits. It’s worth spending a moment thinking about what those rights and liberties look like. They are the the basic needs that are important for people to be free, to have choices and the means to pursue their aspirations. Today many of what Rawls considered to be basic rights and liberties are allocated algorithmically; education, employment, housing, healthcare, consistent treatment under the law to name a few.</p>
<p>The second principle requires positions to be allocated meritocratically, with all similarly talented (with respect to the skills and competencies required for the position) individuals having the same chance of attaining such positions i.e. that allocation of such positions should be independent of social class or background. We will return to the concept of <em>equality of opportunity</em> in chapter <a href="#ch_GroupFairness" data-reference-type="ref" data-reference="ch_GroupFairness">3</a> when discussing <em>Group Fairness</em>.</p>
<p>The third principle acts to prevent redistribution of social and economic currency from the rich to the poor by requiring that inequalities are of maximal benefit to the least advantaged in a society, also described as the maximin principle. In this principle, Rawls does not take the simplistic view that inequality and fairness are mutually exclusive but rather concisely articulates when the existence of inequality becomes unfair. In a sense Rawls opposes utilitarian thinking (that everyone matters equally) in prioritising the least advantaged. We shall return to maximin principle when we look at the use of <em>inequality indices</em> to measure algorithmic unfairness in a later chapter.</p>
</section>
</section>
</section>
<section id="a-legal-perspective" class="level2" data-number="1.3">
<h2 data-number="1.3"><span class="header-section-number">1.3</span> A Legal Perspective</h2>
<p>It’s important to remember that anti-discrimination laws are the result of long-standing and systemic discrimination against oppressed people. Their existence is a product of history; subjugation, genocide, civil war, mass displacement of entire communities, racial hierarchies and segregation, supremacist policies (exclusive access to publicly funded initiatives), voter suppression and more. The law provides an important historical record of what we as a society deem fair and unfair, but without history there is no context. The law does not define the benchmark for fairness. Laws vary by jurisdiction and change over time and in particular they often do not adequately recognise or address issues related to discrimination that are known and accepted by the sciences (social, mathematical, medical,...).</p>
<p>In this section we’ll look at the history, practical application and interpretation of the law in the US (acknowledging the narrow scope of our discussion) Finally, we’ll take a brief look at what might be on the legislative horizon for predictive algorithms, based on more recent global developments.</p>
<section id="a-brief-history-of-anti-discrimination-law-in-the-us" class="level3" data-number="1.3.1">
<h3 data-number="1.3.1"><span class="header-section-number">1.3.1</span> A Brief History of Anti-discrimination Law in the US</h3>
<p>Anti-discrimination laws in the US rest on the 14th amendment to the constitution which grants citizens <em>equal protections of the law</em>. Class action law suit Brown v Board (of Education of Topeka, Kansas) was a landmark case which in 1954, legally ended racial segregation in the US. Justices ruled unanimously that racial segregation of children in public schools was unconstitutional, establishing the precedent that “separate-but-equal” was, in fact, not equal at all. Though Brown v Board did not end segregation in practice, resistance to it in the south fuelled the civil rights movement. In the years that followed the NAACP (National Association for the Advancement of Coloured People) challenged segregation laws. In 1955, Rosa parks refusing to give up her seat on a bus in Montgomery (Alabama) led to sit ins and boycotts, many of them led by Martin Luther King Jr. The resulting Civil rights act of 1964 eventually brought an end to “Jim Crow” laws which barred Blacks from sharing buses, schools and other public facilities with Whites.</p>
<p>After the violent attack by Alabama state troopers on participants of a peaceful march from Selma to Montgomery was televised, The Voting Rights Act of 1965 was passed. It overcame many barriers (including literacy tests), at state and local level, used to prevent Black people from voting. Before this incidents of voting officials asking Black voters to “recite the entire Constitution or explain the most complex provisions of state laws”<span class="citation" data-cites="LBJ"><a href="#ref-LBJ" role="doc-biblioref">[3]</a></span><span class="marginnote"><span id="ref-LBJ" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[3] </span><span class="csl-right-inline">P. L. B. Johnson, <span>“Speech to a joint session of congress on march 15, 1965,”</span> <em>Public Papers of the Presidents of the United States</em>, vol. I, entry 107, pp. 281–287, 1965.</span>
</span>
</span> in the south were common place.</p>
<p>In the years following the second world war, there were many attempts to pass an Equal Pay Act. Initial efforts were led by unions who feared men’s salaries would be undercut by women who were paid less for doing their jobs during the war. By 1960, women made up 37% of the work force but earned on average 59 cents for each dollar earned by men. The Equal Pay Act was eventually passed in 1963 in a bill which endorsed “equal pay for equal work”. Laws for gender equality were strengthened the following year by the Civil Rights Act of 1964.</p>
<p>Throughout the 1800’s the American federal government displaced Native American communities to facilitate White settlement. In 1830 the Indian Removal Act was passed in order to relocate hundreds of thousands of Native Americans. Over the following two decades, thousands of those forced to march hundreds of miles west on the perilous “Trail of Tears” died. By the middle on the century, the term “manifest destiny” was popularised to describe the belief that White settlement in North America was ordained by God. In 1887, the Dawes Act laid the groundwork for the seizing and redistribution of reservation lands from Native to White Americans. Between 1945 and 1968 the federal government terminated recognition of more than 100 tribal nations placing them under state jurisdiction. Once again Native Americans were relocated, this time from reservations to urban centres.</p>
<p>In addition to displacing people of colour, the federal government also enacted policies that reduced barriers to home ownership almost exclusively for White citizens - subsidizing the development of prosperous "White Caucasian" tenant/owner only suburbs, guaranteeing mortgages and enabling access to job opportunities by building highway systems for White commuters, often through communities of colour, simultaneously devaluing the properties in them. Even government initiatives aimed at helping veterans of World War II to obtain home loans accommodated Jim Crow laws allowing exclusion of Black people. In the wake of the Vietnam war, just days after the assassination of Martin Luther King J, the Fair Housing Act of 1968 was passed, prohibiting discrimination concerning the sale, rental and financing of housing based on race, religion, national origin or sex.</p>
<p>The Civil Rights Act of 1964 acted as a catalyst for many other civil rights movements, including those protecting people with disabilities. The Rehabilitation Act (1973) removed architectural, structural and transportation barriers and set up affirmative action programs. The Individuals with Disabilities Education Act (IDEA 1975) required free, appropriate public education in the least restrictive environment possible for children with disabilities. The Air Carrier Access Act (1988) which prohibited discrimination on the basis of disability in air travel and ensured equal access to air transportation services. The Fair Housing Amendments Act (1988) prohibited discrimination in housing against people with disabilities.</p>
<p>Title IX of the education amendments of 1972 prohibits federally funded educational institutions from discriminating against students or employees based on sex. The law ensured that schools (elementary to university level) that were recipients of federal funding (nearly all schools) provided fair and equal treatment of the sexes in all areas, including athletics. Before this few opportunities existed for female athletes. The National Collegiate Athletic Association (NCAA) offered no athletic scholarships for women and held no championships for women’s teams. Since then the number of female college athletes has grown five fold. The amendment is credited with decreasing dropout rates and increasing the numbers of women gaining college degrees.</p>
<p>The Equal Credit Opportunity Act was passed in 1974 when discrimination against women applying for credit in the US was rife. It was common practice for mortgage lenders to discount incomes of women that were of ’child bearing’ age or simply deny credit to them. Two years later the law was amended to prohibit lending discrimination based on race, color, religion, national origin, age, the receipt of public assistance income, or exercising one’s rights under consumer protection laws.</p>
<p>In 1978, congress passed the Pregnancy Discrimination Act in response to two Supreme Court cases that ruled that excluding pregnancy related disabilities from disability benefit coverage was not gender based discrimination, and did not violate the equal protection clause.</p>
<p>Table <a href="#tbl:RegDom" data-reference-type="ref" data-reference="tbl:RegDom">1.1</a> shows a (far from exhaustive) summary of regulated domains with corresponding US legislation. Note that legislation in these domains extend to marketing and advertising not just the final decision.</p>
<div id="tbl:RegDom">
<table>
<caption>Table 1.1: Regulated domains in the private sector under US federal law.</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Domain</th>
<th style="text-align: left;">Legislation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Finance</td>
<td style="text-align: left;">Equal Credit Opportunity Act</td>
</tr>
<tr class="even">
<td rowspan="3" style="text-align: left;">Education</td>
<td style="text-align: left;">Civil Rights Act (1964)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Education Amendment (1972)</td>
</tr>
<tr class="even">
<td style="text-align: left;">IDEA (1975)</td>
</tr>
<tr class="odd">
<td rowspan="2" style="text-align: left;">Employment</td>
<td style="text-align: left;">Equal Pay Act(1963)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Civil Rights Act (1964)</td>
</tr>
<tr class="odd">
<td rowspan="2" style="text-align: left;">Housing</td>
<td style="text-align: left;">Fair Housing Act (1968)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Fair Housing Amendments Act (1988)</td>
</tr>
<tr class="odd">
<td rowspan="3" style="text-align: left;">Transport</td>
<td style="text-align: left;">Urban Mass Transit Act (1970)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Rehabilitation Act (1973)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Air Carrier Access Act (1988)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Public accommodation<sup>a</sup></td>
<td style="text-align: left;">Civil Rights Act (1964)</td>
</tr>
</tbody>
</table>
</div>
<div class="tablenotes">
<p><sup>a</sup>Prevents refusal of customers.</p>
</div>
<p>Table <a href="#tbl:ProtChar" data-reference-type="ref" data-reference="tbl:ProtChar">1.2</a> provides a list of protected characteristics under US federal law with corresponding legislation (again not exhaustive).</p>
<div id="tbl:ProtChar">
<table>
<caption>Table 1.2: Protected characteristics under US Federal Law.</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Protected Characteristic</th>
<th style="text-align: left;">Legislation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Race</td>
<td style="text-align: left;">Civil Rights Act (1964)</td>
</tr>
<tr class="even">
<td rowspan="3" style="text-align: left;">Sex</td>
<td style="text-align: left;">Equal Pay Act (1963)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Civil Rights Act (1964)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Pregnancy Discrimination Act (1978)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Religion</td>
<td style="text-align: left;">Civil Rights Act (1964)</td>
</tr>
<tr class="even">
<td style="text-align: left;">National Origin</td>
<td style="text-align: left;">Civil Rights Act (1964)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Citizenship</td>
<td style="text-align: left;">Immigration Reform &amp; Control Act</td>
</tr>
<tr class="even">
<td style="text-align: left;">Age</td>
<td style="text-align: left;">Age Discrimination in Employment Act (1967)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Familial status</td>
<td style="text-align: left;">Civil Rights Act (1968)</td>
</tr>
<tr class="even">
<td rowspan="2" style="text-align: left;">Disability status</td>
<td style="text-align: left;">Rehabilitation Act of 1973</td>
</tr>
<tr class="odd">
<td style="text-align: left;">American with Disabilities Act of 1990</td>
</tr>
<tr class="even">
<td rowspan="2" style="text-align: left;">Veteran status</td>
<td style="text-align: left;">Veterans’ Readjustment Assistance Act 1974</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Uniformed Services Employment &amp; Reemployment Rights Act</td>
</tr>
<tr class="even">
<td style="text-align: left;">Genetic Information</td>
<td style="text-align: left;">Civil Rights Act(1964)</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="sec_AppLaw" class="level3" data-number="1.3.2">
<h3 data-number="1.3.2"><span class="header-section-number">1.3.2</span> Application and Interpretation of the Law</h3>
<p>To get an idea of how anti-discrimination laws are be applied in practice and how they might translate to algorithmic decision making, we look at Title VII of the Civil rights act of 1964 in the context of employment discrimination<span class="citation" data-cites="BarocasSelbst"><a href="#ref-BarocasSelbst" role="doc-biblioref">[4]</a></span><span class="marginnote"><span id="ref-BarocasSelbst" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[4] </span><span class="csl-right-inline">S. Barocas and A. D. Selbst, <span>“Big data’s disparate impact,”</span> <em>Calif Law Rev.</em>, vol. 104, pp. 671–732, 2016.</span>
</span>
</span>. Legal liability for discrimination against protected classes can be established as disparate treatment and/or disparate impact. Disparate treatment (also described as direct discrimination in Europe) refers to both differing treatment of individuals based on protected characteristics, and intent to discriminate. Disparate impact (or indirect discrimination in Europe) does not consider intent but addresses policies and practices that disproportionately impact protected classes.</p>
<section id="disparate-treatment" class="level4 unnumbered">
<h4 class="unnumbered">Disparate Treatment</h4>
<p>Disparate treatment effectively prohibits rational prejudice (backed by data showing the protected feature to be correlated) as well as denial of opportunities based on protected characteristics. For an algorithm, it effectively prevents the use of protected characteristics as inputs. It’s noteworthy that in the case of disparate treatment, the actual impact of using the protected features on the outcome is irrelevant; so even if a company could show that the target variable produced by their model had zero correlation with the protected characteristic, the company would still be liable for disparate treatment. This fact is somewhat bizarre given that not using the protected feature in the algorithm provides no guarantee that the algorithm is not biased in relation to it. Indeed an organisation could very well use their data to predict the protected characteristic.</p>
<p>In an effort to avoid disparate treatment liability, many organisations do not even collect data relating to protected characteristics, leaving them unable to accurately measure, let alone address, bias in their algorithms, even if they might want to<span class="sidenote-wrapper"><label for="sn-1" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-1" class="margin-toggle"/><span class="sidenote">In fact, I met a data scientist at a conference, who was working for a financial institution, that said her team was trying to predict sensitive features such as race and gender in order to measure bias in their algorithms.<br />
<br />
</span></span>. In summary, disparate treatment as applied today does not resolve the problem of unconscious discrimination against disadvantaged classes through their use of machine learning algorithms. Further it acts as a deterrent to ethically minded companies that might want to measure the biases in their algorithms.</p>
<div class="lookbox">
<p><strong>Disparate treatment</strong></p>
<p>Suppose a company predicts the sensitive feature and uses this as an input to its model. Should this be considered disparate treatment?</p>
</div>
<p>What about the case where the employer implements an algorithm, finds out that it has a disparate impact, and uses it anyway? Doesn’t that become disparate treatment? No it doesn’t and in fact, somewhat surprisingly, deciding not to apply it upon noting the disparate impact could result in a disparate treatment claim in the opposite direction<span class="citation" data-cites="FireFighters"><a href="#ref-FireFighters" role="doc-biblioref">[5]</a></span><span class="marginnote"><span id="ref-FireFighters" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[5] </span><span class="csl-right-inline"><span>“<span class="nocase">Ricci v. DeStefano, 557 U.S. 557</span>.”</span> 2009.</span>
</span>
</span>. We’ll return to this later. Okay, so what about disparate impact?</p>
</section>
<section id="disparate-impact" class="level4 unnumbered">
<h4 class="unnumbered">Disparate Impact</h4>
<p>In order to establish a violation, it is not enough to simply show that there is a disparate impact, but it must also be shown either that there is no business justification for it, or if there is, that the employer refuses to use another, less discriminatory, means of achieving the desired result. So how much of an impact is enough to warrant a disparate impact claim? There are no rules here only guidelines. The Uniform Guidelines on Employment selection procedures from the Equal Employment Opportunity Commission (EEOC) provides a guideline that if the selection rate from one protected group is less than four fifths of that from another, it will generally be regarded as evidence of adverse impact, though it also states that the threshold would depend on the circumstances.</p>
<p>Assuming the disparate impact is demonstrated, the issue becomes proving business justification. The requirement for business justification has softened in favour of the employer over the years; treated as “business necessity”<span class="citation" data-cites="BusinessNecessity"><a href="#ref-BusinessNecessity" role="doc-biblioref">[6]</a></span><span class="marginnote"><span id="ref-BusinessNecessity" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[6] </span><span class="csl-right-inline"><span>“<span class="nocase">Griggs v. Duke Power Co., 401 U.S. 424</span>.”</span> 1971.</span>
</span>
</span> earlier on and later interpreted as “business justification”<span class="citation" data-cites="BusinessJustification"><a href="#ref-BusinessJustification" role="doc-biblioref">[7]</a></span><span class="marginnote"><span id="ref-BusinessJustification" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[7] </span><span class="csl-right-inline"><span>“<span class="nocase">Wards Cove Packing Co. v. Atonio, 490 U.S. 642</span>.”</span> 1989.</span>
</span>
</span>. Today, it’s generally accepted that business justification lies somewhere between the extremes of “job-relatedness” and “business necessity”. As a concrete example of disparate impact and taking the extreme of job-relatedness - the EEOC along with several federal courts have determined that discrimination on the sole basis of a criminal record to be a violation under disparate impact unless the particular conviction is related to the role, because Non-White applicants are more likely to have a criminal conviction.</p>
<p>For a machine learning algorithm, business justification boils down to the question of job-relatedness of the target variable. If the target variable is improperly chosen, a disparate impact violation can be established. In practice however the courts will accept most plausible explanations of job-relatedness since not accepting it would set a precedent that it is determined discriminatory. Assuming the target variable to be proven job-related then, there is no requirement to validate the model’s ability to predict said trait, only a guideline which sets a low bar (a statistical significance test showing that the target variable correlates with the trait) and which the court is free to ignore.</p>
<p>Assuming business justification is proven by the employer, the final burden then falls on the plaintiff to show that the employer refused to use a less discriminatory “alternative employment practice”. If the less discriminatory alternative would incur additional cost (as is likely) would this be considered refusing? Likely not.</p>
<p>While on the surface, disparate impact might seem like a solution, the current framework of a weak business justification (in terms of a plausible target variable) and the employer refusing an alternative employment practice with no requirement to validate the model offers little resolve. Clearly there is need for reform.</p>
</section>
<section id="anti-classification-versus-anti-subordination" class="level4 unnumbered">
<h4 class="unnumbered">Anti-classification versus Anti-subordination</h4>
<p>Just as the meaning of fairness is subjective so is the interpretation of anti-discrimination laws. At one extreme, anti-classification holds the weaker interpretation, that the law is intended to prevent classification of people based on protected characteristics. At the other extreme, anti-subordination defines the stronger stance, that anti-discrimination laws exist to prevent social hierarchies, class or caste systems based on protected features and, that it should actively work to eliminate them where they exist. An important ideological difference between the two schools of thought is in the application of positive discrimination policies. Under anti-subordination principles, one might advocate for affirmative action as a means to bridge gaps in access to employment, housing, education and other such pursuits, that are a direct result of historical systemic discrimination against particular groups. A strict interpretation of the anti-classification principle would prohibit such actions. Both anti-classification and anti-subordination ideologies have been argued and upheld in landmark cases.</p>
<p>In 2003, the Supreme Court held that a student admissions process that favours “under-represented minority groups” does not violate the Fourteenth Amendment<span class="citation" data-cites="UnderRepStudents"><a href="#ref-UnderRepStudents" role="doc-biblioref">[8]</a></span><span class="marginnote"><span id="ref-UnderRepStudents" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[8] </span><span class="csl-right-inline"><span>“<span class="nocase">Grutter v. Bollinger, 539 U.S. 306</span>.”</span> 2003.</span>
</span>
</span>, provided it evaluated applicants holistically at an individual level. The same year, the New Haven Fire Department administered a two part test in order to fill 15 openings. Examinations were governed in part by the City of New Haven. Under the city charter, civil service positions must be filled by one of the top three scoring individuals. 118 (White, Black and Hispanic) fire fighters took the exams. Of the resulting 19 candidates who scored highest on the tests and could the considered for the positions, none were Black. After heated public debate and under threat of legal action either way, the city threw out the test results. This action was later determined to be a disparate treatment violation. In 2009, the court ruled that disparate treatment could not be used to avoid disparate impact without sufficient evidence of liability of the latter<span class="citation" data-cites="FireFighters"><a href="#ref-FireFighters" role="doc-biblioref">[5]</a></span>. This landmark case was the first example of conflict between the two doctrines of disparate impact and disparate treatment or anti-classification and anti-subordination.</p>
<p>Disparate treatment seems to align well with anti-classification principles, seeking to prevent intentional discrimination based on protected characteristics. In the case of disparate impact, things are less clear. Is it a secondary ‘line of defence’ designed to weed out well masked intentional discrimination? Or is its intention to address inequity that exists as a direct result of historical injustice? One can draw parallels here with the ‘business necessity’ versus ‘business justification’ requirements discussed earlier.</p>
</section>
</section>
<section id="future-legislation" class="level3" data-number="1.3.3">
<h3 data-number="1.3.3"><span class="header-section-number">1.3.3</span> Future Legislation</h3>
<p>In May 2018, the European Union (EU) brought into action the General Data Protection (GDPR) a legal framework around the protection of personal data of EU citizens. The framework is divided into binding and non-binding recitals. The regulation sets provisions for processing of data in relation to decision making, described as ‘profiling’ under recital 71<span class="citation" data-cites="GDPR"><a href="#ref-GDPR" role="doc-biblioref">[9]</a></span><span class="marginnote"><span id="ref-GDPR" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[9] </span><span class="csl-right-inline"><span>“<span>General Data Protection Regulation (GDPR): (EU) 2016/679 Recital 71</span>.”</span> 2016.</span>
</span>
</span>. Though currently non-binding, it provides an indication of what’s to come. The recital talks specifically about having the right not to be subject to decisions based solely on automated processing. It specifically talks about credit applications, e-recruiting and any system which analyses or predicts aspects of a persons performance at work, economic situation, health, personal preferences or interests, reliability or behaviour, location or movements. The recital also talks about requirements around using “appropriate mathematical or statistical procedures” to prevent “discriminatory effects on natural persons on the basis of racial or ethnic origin, political opinion, religion or beliefs, trade union membership, genetic or health status or sexual orientation”. More recently in 2021, the EU has proposed taking a risk based approach to the question of which technologies should be regulated, dividing it into four categories. Unacceptable risk, high risk, limited risk, minimal risk<span class="citation" data-cites="ECPR"><a href="#ref-ECPR" role="doc-biblioref">[10]</a></span><span class="marginnote"><span id="ref-ECPR" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[10] </span><span class="csl-right-inline"><span>“<span class="nocase">Europe fit for the Digital Age: Commission proposes new rules and actions for excellence and trust in Artificial Intelligence</span>.”</span> 2021.</span>
</span>
</span>. While things may change as the proposed law is debated but once agreed, it’s not unlikely that it will serve as a prototype for legislation in the U.S. (and other countries around the world), as did GDPR.</p>
<p>In April 2019, the <a href="https://www.congress.gov/bill/116th-congress/house-bill/2231">Algorithmic Accountability Act</a> was proposed to the US Senate. The bill requires specified commercial entities to conduct impact assessments of automated decision systems and specifically states that assessments must include evaluations and risk assessment in relation to “accuracy, fairness, bias, discrimination, privacy, and security” not just for the model output but for the training data. The bill has cosponsors in 22 states and has been referred to the Committee on Commerce, Science, and Transportation for review. These examples are clear indications that the issues of fairness and bias in automated decision making systems are on the radar of regulators.</p>
</section>
</section>
<section id="sec_SimpsParadox" class="level2" data-number="1.4">
<h2 data-number="1.4"><span class="header-section-number">1.4</span> A Technical Perspective</h2>
<p>The problem of distinguishing correlation from causation is an important one in identifying bias. Using illustrative examples of Simpson’s paradox, we demonstrate the danger of assuming causal relationships based on observational data.</p>
<section id="simpsons-paradox" class="level3" data-number="1.4.1">
<h3 data-number="1.4.1"><span class="header-section-number">1.4.1</span> Simpson’s Paradox</h3>
<p>In 1973, University of California, Berkeley received approximately 15,000 applications for the fall quarter<span class="citation" data-cites="Berkeley"><a href="#ref-Berkeley" role="doc-biblioref">[11]</a></span><span class="marginnote"><span id="ref-Berkeley" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[11] </span><span class="csl-right-inline">P. J. Bickel, E. A. Hammel, and J. W. O’Connell, <span>“Sex bias in graduate admissions: Data from berkeley,”</span> <em>Science</em>, vol. 187, Issue 4175, pp. 398–404, 1975.</span>
</span>
</span>. At the time it was made up of 101 departments. 12,763 applications reached the decision stage. Of these 8442 were male and 4321 were female. The acceptance rates for the applicants were 44% and 35% respectively (see Table <a href="#tbl:BerkAdm1" data-reference-type="ref" data-reference="tbl:BerkAdm1">1.3</a>).</p>
<div id="tbl:BerkAdm1">
<table>
<caption>Table 1.3: Graduate admissions data from Berkeley (fall 1973).</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Gender</th>
<th style="text-align: right;">Admitted</th>
<th style="text-align: right;">Rejected</th>
<th style="text-align: right;">Total</th>
<th style="text-align: right;">Acceptance Rate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Male</td>
<td style="text-align: right;">3738</td>
<td style="text-align: right;">4704</td>
<td style="text-align: right;">8442</td>
<td style="text-align: right;">44.3%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Female</td>
<td style="text-align: right;">1494</td>
<td style="text-align: right;">2827</td>
<td style="text-align: right;">4321</td>
<td style="text-align: right;">34.6%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Aggregate</td>
<td style="text-align: right;">5232</td>
<td style="text-align: right;">7531</td>
<td style="text-align: right;">12763</td>
<td style="text-align: right;">41.0%</td>
</tr>
</tbody>
</table>
</div>
<p>With a whopping 10% difference in acceptance rates, it seems a likely case of discrimination against women. Indeed, a <span class="math inline">\(\chi^2\)</span> hypothesis test for independence between the variables (gender and application acceptance) reveals that the probability of observing such a result or worse, assuming they are independent, is <span class="math inline">\(6\times10^{-26}\)</span>. A strong indication that they are not independent and therefore evidence of bias in favour of male applicants. Since admissions are determined by the individual departments, it’s worth trying to understand which departments might be responsible. We focus on the data for the six largest departments, shown in Table <a href="#tbl:BerkAdm2" data-reference-type="ref" data-reference="tbl:BerkAdm2">1.4</a>. Here again we see a similar pattern. There appears to be bias in favour of male applicants, and a <span class="math inline">\(\chi^2\)</span> test shows that the probability of seeing this result under the assumption of independence is <span class="math inline">\(1\times10^{-21}\)</span>. It looks like we have quickly narrowed down our search.</p>
<div id="tbl:BerkAdm2">
<table>
<caption>Table 1.4: Graduate admissions data from Berkeley (fall 1973) for the six largest departments.</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Gender</th>
<th style="text-align: right;">Admitted</th>
<th style="text-align: right;">Rejected</th>
<th style="text-align: right;">Total</th>
<th style="text-align: right;">Acceptance Rate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Male</td>
<td style="text-align: right;">1198</td>
<td style="text-align: right;">1493</td>
<td style="text-align: right;">2691</td>
<td style="text-align: right;">44.5%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Female</td>
<td style="text-align: right;">557</td>
<td style="text-align: right;">1278</td>
<td style="text-align: right;">1835</td>
<td style="text-align: right;">30.4%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Aggregate</td>
<td style="text-align: right;">1755</td>
<td style="text-align: right;">2771</td>
<td style="text-align: right;">4526</td>
<td style="text-align: right;">38.8%</td>
</tr>
</tbody>
</table>
</div>
<p>Figure <a href="#fig:SimpsParAccByDept" data-reference-type="ref" data-reference="fig:SimpsParAccByDept">1.1</a> shows the acceptance rates for each department by gender, in decreasing order of acceptance rates. Performing <span class="math inline">\(\chi^2\)</span> tests for each department reveals the only department where there is strong evidence of bias is A, but the bias is in favour of female applicants. The probability of observing the data for department A, under the assumption of independence, is <span class="math inline">\(5\times10^{-5}\)</span>.</p>
<figure>
<img src="01_Context/figures/Fig_BerkeleyAccByDept.png" id="fig:SimpsParAccByDept" style="width:85.0%" alt="Figure 1.1: Acceptance rate distributions by department for male and female applicants." />
<figcaption aria-hidden="true">Figure 1.1: Acceptance rate distributions by department for male and female applicants.</figcaption>
</figure>
<p>So what’s going on? Figure <a href="#fig:SimpsParAppByDept" data-reference-type="ref" data-reference="fig:SimpsParAppByDept">1.2</a> shows the application distributions for male and female applicants for each of the six departments. From the plots we are able to see a pattern. Female applicants are more often applying for departments with a lower acceptance rate.</p>
<figure>
<img src="01_Context/figures/Fig_BerkeleyAppByDept.png" id="fig:SimpsParAppByDept" style="width:85.0%" alt="Figure 1.2: Application distributions by department for male and female applicants." />
<figcaption aria-hidden="true">Figure 1.2: Application distributions by department for male and female applicants.</figcaption>
</figure>
<p>In other words a larger proportion of the women are being filtered out overall, simply because they are applying to departments that are harder to get into.</p>
<p>This is a classic example of Simpson’s Paradox (also known as the reversal paradox and Yule-Simpson effect). We have an observable relationship between two categorical variables (in this case gender and acceptance) which disappears or reverses, after controlling for one or more other variables (in this case department). Simpson’s Paradox is a special case of so called association paradoxes (where the variables are categorical, and the relationship changes qualitatively), but the same rules also apply to continuous variables. The <em>marginal</em> (unconditional) measure of association (e.g. correlation) between two variables need not be bounded by the <em>partial</em> (conditional) measures of association (after controlling for one or more variables). Although Edward Hugh Simpson famously wrote about the paradox in 1951, it was not discovered by him. In fact, it was reported by George Udny Yule as early as 1903. The association paradox for continuous variables was demonstrated by Karl Pearson in 1899.</p>
<p>Let’s discuss another quick example. A 1996 follow-up study on the effects of smoking recorded the mortality rate for the participants over a 20 year period. They found higher mortality rates among the non-smokers, 31.4% compared to 23.9% which, in itself, might imply a considerable protective affect from smoking. Clearly there’s something fishy going on. Disaggregating the data by age group showed that the mortality rates were higher for smokers in all but one of them. Looking at the age distribution of the populations of smokers and non-smokers, it’s apparent that the age distribution of the non-smoking group is more positively skewed, and so they are older on average. This concords with the rationale that non-smokers live longer - hence the difference in age distributions of the participants.</p>
<figure>
<img src="01_Context/figures/Fig_SimpParaReg.png" id="fig:SimpsPara" style="width:98.0%" alt="Figure 1.3: Visualisation of Simpsons Paradox. Wikipedia." />
<figcaption aria-hidden="true">Figure 1.3: Visualisation of Simpsons Paradox. <a href="https://en.wikipedia.org/wiki/Simpson%27s_paradox">Wikipedia</a>.</figcaption>
</figure>
</section>
<section id="causality" class="level3" data-number="1.4.2">
<h3 data-number="1.4.2"><span class="header-section-number">1.4.2</span> Causality</h3>
<p>In both the above examples, it appears that the salient information is found in the disaggregated data (we’ll come back to this later). In both cases it is the disaggregated data that enables us to understand the <em>true nature</em> of the relationship between the variables of interest. As we shall see in this section, this need not be the case. To show this, we discuss two examples. In each case, the data is identical but the variables is not. The examples are those Simpson gave in his original 1951 paper<span class="citation" data-cites="Simpson"><a href="#ref-Simpson" role="doc-biblioref">[12]</a></span><span class="marginnote"><span id="ref-Simpson" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[12] </span><span class="csl-right-inline">E. Simpson, <span>“The interpretation of interaction in contingency tables,”</span> <em>Journal of the Royal Statistical Society</em>, vol. Series B, 13, pp. 238–241, 1951.</span>
</span>
</span>.</p>
<p>Suppose we have three binary variables, <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span> and <span class="math inline">\(C\)</span>, and we are interested in understanding the relationship between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> given a set of 52 data points. A summary of the data showing the association between variables <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are shown in Table <a href="#tbl:SimpPara" data-reference-type="ref" data-reference="tbl:SimpPara">1.5</a>, first for all the data points and then stratified (separated) by the value of <span class="math inline">\(C\)</span> (note the first table is the sum of the latter two). The first table indicates that <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are unconditionally independent (since changing the value of one variable does not change the distribution of the other). The next two tables suggest <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are conditionally dependent given <span class="math inline">\(C\)</span>.</p>
<div id="tbl:SimpPara">
<table>
<caption>Table 1.5: Data summary showing the association between variables <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, first for all the data points and then stratified by the value of <span class="math inline">\(C\)</span>.</caption>
<thead>
<tr class="header">
<th colspan="5" style="text-align: center;"></th>
<th colspan="4" style="text-align: center;"><span style="color: SteelBlue">Stained?</span> / <span style="color: FireBrick">Male?</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td colspan="5" style="text-align: center;"></td>
<td colspan="2" style="text-align: center;"><span class="math inline">\(C=1\)</span></td>
<td colspan="2" style="text-align: center;"><span class="math inline">\(C=0\)</span></td>
</tr>
<tr class="even">
<td rowspan="2" style="text-align: center;"><span style="color: SteelBlue">Black?</span>/ <span style="color: FireBrick">Died?</span></td>
<td colspan="2" style="text-align: center;"><span style="color: SteelBlue">Plain?</span>/ <span style="color: FireBrick">Treated?</span></td>
<td rowspan="5" style="text-align: center;"></td>
<td rowspan="2" style="text-align: center;"><span style="color: SteelBlue">Black?</span>/ <span style="color: FireBrick">Died?</span></td>
<td colspan="4" style="text-align: center;"><span style="color: SteelBlue">Plain?</span>/ <span style="color: FireBrick">Treated?</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(A=1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(A=0\)</span></td>
<td style="text-align: center;"><span class="math inline">\(A=1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(A=0\)</span></td>
<td style="text-align: center;"><span class="math inline">\(A=1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(A=0\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(B=1\)</span></td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;"><span class="math inline">\(B=1\)</span></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">3</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(B=0\)</span></td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;"><span class="math inline">\(B=0\)</span></td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(\mathbb{P}(B|A)\)</span></td>
<td style="text-align: center;">50%</td>
<td style="text-align: center;">50%</td>
<td style="text-align: center;"><span class="math inline">\(\mathbb{P}(B|A,C)\)</span></td>
<td style="text-align: center;">38%</td>
<td style="text-align: center;">43%</td>
<td style="text-align: center;">56%</td>
<td style="text-align: center;">60%</td>
</tr>
</tbody>
</table>
</div>
<div class="tablenotes">
<p><sup>a</sup>Each cell of the table shows the number of examples in the dataset satisfying the conditions given in the corresponding row and column headers.</p>
</div>
<section id="question" class="level5 unnumbered">
<h5 class="unnumbered">Question:</h5>
<p>Which distribution gives us the most relevant understanding of the association between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, the marginal (i.e. unconditional) <span class="math inline">\(\mathbb{P}(A,B)\)</span> or conditional distribution <span class="math inline">\(\mathbb{P}(A,B|C)\)</span>? To show that causal relationships matter, we consider two different examples.</p>
</section>
<section id="example-a-pack-of-cards-colliding-variable" class="level4 unnumbered">
<h4 class="unnumbered"><span style="color: SteelBlue">Example a) Pack of Cards (Colliding Variable)</span></h4>
<p>Suppose the population is a pack of cards. It so happens that baby Milen has been messing about with the cards and made some dirty in the process. Let’s summarise our variables,</p>
<ul>
<li><p><span class="math inline">\(A\)</span> tells us the character of the card, either plain (<span class="math inline">\(A=1\)</span>) or royal (King, Queen, Jack; <span class="math inline">\(A=0\)</span>).</p></li>
<li><p><span class="math inline">\(B\)</span> tells us the colour of the card, either black (<span class="math inline">\(B=1\)</span>) or red (<span class="math inline">\(B=0\)</span>).</p></li>
<li><p><span class="math inline">\(C\)</span> tells us if the card is dirty (<span class="math inline">\(C=1\)</span>) or clean (<span class="math inline">\(C=0\)</span>).</p></li>
</ul>
<p>In this case, the aggregated data showing <span class="math inline">\(\mathbb{P}(A,B)\)</span> is relevant since the cleanliness of the cards <span class="math inline">\(C\)</span> has no bearing on the association between the character <span class="math inline">\(A\)</span> and colour <span class="math inline">\(B\)</span> of the cards.</p>
</section>
<section id="example-b-treatment-effect-on-mortality-rate-confounding-variable" class="level4 unnumbered">
<h4 class="unnumbered"><span style="color: FireBrick">Example b) Treatment Effect on Mortality Rate (Confounding Variable)</span></h4>
<p>Next, suppose that the data relates to the results of medical trials for a drug on a potentially lethal illness. This time,</p>
<ul>
<li><p><span class="math inline">\(A\)</span> tells us if the subject was treated (<span class="math inline">\(A=1\)</span>) or not (<span class="math inline">\(A=0\)</span>).</p></li>
<li><p><span class="math inline">\(B\)</span> tells us if the subject died (<span class="math inline">\(B=1\)</span>) or recovered (<span class="math inline">\(B=0\)</span>).</p></li>
<li><p><span class="math inline">\(C\)</span> tells us the gender of the subject, either male (<span class="math inline">\(C=1\)</span>) or female (<span class="math inline">\(C=0\)</span>).</p></li>
</ul>
<p>In this case the disaggregated data shows the more relevant association, <span class="math inline">\(\mathbb{P}(A,B|C)\)</span>. From it, we can see that female patients are more likely to die than males overall; 56 and 60% versus 38 and 43%, depending on if they were treated or not. In both cases we see that treatment with the drug <span class="math inline">\(A\)</span> reduces the mortality rate for both male and female participants, and the effect is obscured by aggregating the data over gender <span class="math inline">\(C\)</span>.</p>
</section>
<section id="back-to-causality" class="level4 unnumbered">
<h4 class="unnumbered">Back to Causality</h4>
<p>The key difference between these examples is the causal relationship between the variables rather than the statistical structure of the data. In the first example with the playing cards, the variable <span class="math inline">\(C\)</span> is a <em>colliding</em> variable, in the second example looking at patient mortality, it is a <em>confounding</em> variable. Figure <a href="#fig:CollConfProg" data-reference-type="ref" data-reference="fig:CollConfProg">1.4</a> a) and b) show the causal relationships between the variables in the two cases.</p>
<figure class="fullwidth">
<img src="01_Context/figures/Fig_CollConfProg.png" id="fig:CollConfProg" alt="Figure 1.4: Causal diagrams for A, B and C when C is a colliding, confounding and prognostic variable." />
<figcaption aria-hidden="true">Figure 1.4: Causal diagrams for <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span> and <span class="math inline">\(C\)</span> when <span class="math inline">\(C\)</span> is a colliding, confounding and prognostic variable.</figcaption>
</figure>
<p>The causal diagram in Figure <a href="#fig:CollConfProg" data-reference-type="ref" data-reference="fig:CollConfProg">1.4</a> a) shows the variables <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span> and <span class="math inline">\(C\)</span> for the first example. The arrows exist both from card character and colour to cleanliness because apparently, baby Milen had a preference for royal cards over plain and red cards over black. Conditioning on a collider <span class="math inline">\(C\)</span> generates an association (e.g. correlation) between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, even if they are unconditionally independent. This common effect is often observed as <em>selection</em> or <em>representation bias</em>. Representation bias can induce correlation between variables, even where there is none. For decision systems, this can lead to feedback loops that increase the extremity of the representation bias in future data. We’ll come back to this in chapter <a href="#ch_EthicalDev" data-reference-type="ref" data-reference="ch_EthicalDev">2</a>, when we talk about common causes of bias.</p>
<p>The causal diagram in Figure <a href="#fig:CollConfProg" data-reference-type="ref" data-reference="fig:CollConfProg">1.4</a> b) shows the variables <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span> and <span class="math inline">\(C\)</span> for the second example. The arrows exist from <span class="math inline">\(gender\)</span> to treatment because men were less likely to be treated, and from gender to death because men were also less likely to die. The arrow from <span class="math inline">\(A\)</span> to <span class="math inline">\(B\)</span> represents the effect of treatment on mortality which is observable only by conditioning on gender. Note that there are two sources of association in opposite directions between variables <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> (treatment and death); a positive association, because men were less likely to be treated; and a negative association, because male patients are less likely to die. The two effects cancel each other out when the data is aggregated.</p>
<p>We see through the discussion of these two examples, that statistical reasoning is not sufficient to be able to determine which of the distributions (marginal or conditional) are relevant. Note that the above conclusions in relation to colliding and confounding variables does not generalize to complex time varying problems.</p>
<p>Before moving on from causality, we return to the example we discussed at the very start of this section. According to our analysis of the Berkeley admissions data, we concluded that the disaggregated data contained the <em>salient</em> information explaining the disparity in acceptance rates for male and female applicants. The problem is, we have only shown that application rates to be one of many possible <em>causes</em> of the differing acceptance rates (we cannot see outside of our data). In addition, we have not proven <em>gender discrimination</em>, not to be the cause. What we have evidenced, is the existence of disparities in both acceptance rates and application rates across sex. One problem is that <em>gender discrimination</em> is not a measurable thing in itself. It’s complicated. It is made up of many components, most of which are not contained in the data. Beliefs, personal preferences, behaviours, actions, and more. A valid question we cannot answer is, <em>why do the application rates differ by sex?</em> How do we know that this is itself, is not a result of gender discrimination. Perhaps some departments are less welcoming of women than others or, perhaps some are just much more welcoming of men than women? So how would we know if gender discrimination is at play here? We need to ask the right questions to collect the right data.</p>
</section>
</section>
<section id="sec_collapsibility" class="level3" data-number="1.4.3">
<h3 data-number="1.4.3"><span class="header-section-number">1.4.3</span> Collapsibility</h3>
<p>We have demonstrated that correlation does not imply causation in the manifestation of Simpson’s Paradox. But there is second factor that can have an impact; and that is the nature of the measure of association in question.</p>
<section id="example-c-treatment-effect-on-mortality-rate-prognostic-variable" class="level4 unnumbered">
<h4 class="unnumbered"><span style="color: SeaGreen">Example c) Treatment Effect on Mortality Rate (Prognostic Variable)</span></h4>
<p>Suppose that in the study of the efficacy of the treatment (in Example 2 above), we remedy the selection bias so that male and female patients are equally likely to be treated. We remove the causal relationship between variables <span class="math inline">\(A\)</span> and <span class="math inline">\(C\)</span> (treatment and gender). In this case, the variable <span class="math inline">\(C\)</span> becomes <em>prognostic</em> rather than confounding. See Figure <a href="#fig:CollConfProg" data-reference-type="ref" data-reference="fig:CollConfProg">1.4</a> c). In this case the decision as to which distributions (marginal or conditional) are most relevant would depend only on the target population in question. In the absence of the confounding variable in our study one might reasonably expect the marginal measure of association to be bounded by the partial measures of association. Such intuition is correct only if the measure of association is <em>collapsible</em> (that is, it can be expressed as the weighted average of the partial measures), not otherwise. Some examples of collapsible measures of association are the risk ratio and risk difference. The odds ratio however is not collapsible. If you don’t know what these are, don’t worry, we’ll return to them in chapter <a href="#ch_GroupFairness" data-reference-type="ref" data-reference="ch_GroupFairness">3</a>.</p>
</section>
</section>
</section>
<section id="sec_harms" class="level2" data-number="1.5">
<h2 data-number="1.5"><span class="header-section-number">1.5</span> What’s the Harm?</h2>
<p>In this section we discuss the recent and broader societal concerns related to machine learning technologies.</p>
<section id="the-illusion-of-objectivity" class="level3" data-number="1.5.1">
<h3 data-number="1.5.1"><span class="header-section-number">1.5.1</span> The Illusion of Objectivity</h3>
<p>One of the most concerning things about the machine learning revolution, is perception that these algorithms are somehow objective (unlike humans), and are therefore a better substitute for human judgement. This viewpoint is not just a belief of laymen but an idea that is also projected from within the machine learning community. There are often financial incentives to exaggerate the efficacy of such systems.</p>
<section id="automation-bias" class="level4 unnumbered">
<h4 class="unnumbered">Automation Bias</h4>
<p>The tendency for people to favour decisions made by automated systems despite contradictory information from non-automated sources, or <em>automation bias</em>, is a growing problem as we integrate more and more machines in our decision making processes especially in infrastructure - healthcare, transportation, communication, power plants and more.</p>
<p>It is important to be clear that in general, machine learning systems are not objective. Data is produced by a necessarily subjective set of decisions (how and who to sample, how to group events or characteristics, which features to collect). Modelling also involves making choices about how to process the data, what class of model to use and perhaps most importantly how success is determined. Finally, even if our model is calibrated to the data well, it says nothing about the distribution of errors across the population. The consistency of algorithms in decision making compared to humans (who individually make decisions on a case by case basis) is often described as a benefit<span class="sidenote-wrapper"><label for="sn-2" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-2" class="margin-toggle"/><span class="sidenote">One must not confuse consistency with objectivity. For algorithms, consistency also means consistently making the same errors.<br />
<br />
</span></span>, but it’s their very consistency that makes them dangerous - capable of discriminating systematically and at scale.</p>
<section id="example-compas" class="level5 unnumbered">
<h5 class="unnumbered">Example: COMPAS</h5>
<p>(Correctional Offender Management Profiling for Alternative Sanctions) is a “case management system for criminal justice practitioners”. The system, produces recidivism risk scores. It has been used in New York, California and Florida, but most extensively in Wisconsin since 2012, at a variety of stages in the criminal justice, from sentencing to parole. The <a href="https://assets.documentcloud.org/documents/2840784/Practitioner-s-Guide-to-COMPAS-Core.pdf">documentation</a> for the software describes it as an “objective statistical risk assessment tool”.</p>
<p>In 2013, Paul Zilly was convicted of stealing a push lawnmower and some tools in Barron County, Wisconsin. The prosecutor recommended a year in county jail and follow-up supervision that could help Zilly with “staying on the right path.” His lawyer agreed to a plea deal. But Judge James Babler upon seeing Zilly’s COMPAS risk scores overturned the plea deal that had been agreed on by the prosecution and defence, and imposed two years in state prison and three years of supervision. At an appeals hearing later that year, Babler said “Had I not had the COMPAS, I believe it would likely be that I would have given one year, six months”<span class="citation" data-cites="ProPub1"><a href="#ref-ProPub1" role="doc-biblioref">[13]</a></span><span class="marginnote"><span id="ref-ProPub1" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[13] </span><span class="csl-right-inline">J. Angwin, J. Larson, S. Mattu, and L. Kirchner, <span>“Machine bias,”</span> <em>ProPublica</em>, 2016.</span>
</span>
</span>. In other words the judge believed the risk scoring system to hold more insight that the prosecutor who had personally interacted with the defendant.</p>
</section>
</section>
<section id="the-ethics-of-classification" class="level4 unnumbered">
<h4 class="unnumbered">The Ethics of Classification</h4>
<p>The appeal of classification is clear. It creates a sense of order and understanding. It enables us to formulate problems neatly and solve them. An email is spam or it’s not; an x-ray shows tuberculosis or it doesn’t; a treatment was effective or it wasn’t. It can make finding things more efficient in a library or online. There are lots of useful applications of classification.</p>
<p>We tend to think of taxonomies as objective categorisations, but often they are not. They are snapshots in time, representative of the culture and biases of the creators. The very act of creating a taxonomy, can give life by existence to some individuals, while erasing others. Classifying people inevitably has the effect of reducing them to labels; labels that can result in people being treated as members of a group, rather than individuals; labels that can linger for much longer than they should (something it’s easy to forget when creating them). The Dewey Decimal System for example, was developed in the late 1800’s and widely adopted in the 1930’s to classify books. Until 2015, it categorised homosexuality as a mental derangement.</p>
</section>
<section id="classification-of-people" class="level4 unnumbered">
<h4 class="unnumbered">Classification of People</h4>
<p>From the 1930’s until the second world war, machine classification systems were used by Nazi Germany to process census data in order to identify and locate Jews, determine what property and businesses they owned, find anything of value that could be seized and finally to send them to their deaths in concentration camps. Classification systems have often been entangled with political and social struggle across the world. In Apartheid South Africa, they were been used extensively in many parts of the world to enforce social and racial hierarchies that determined everything from where people could live and work to whom they could marry. In 2019 it was estimated that some half a million Uyghurs (and other minority Muslims) are being held in internment camps in China without charge for the purposes of countering extremism and promoting social integration.</p>
<p>Recent papers on detecting criminality”<span class="citation" data-cites="CriminalFace"><a href="#ref-CriminalFace" role="doc-biblioref">[14]</a></span><span class="marginnote"><span id="ref-CriminalFace" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[14] </span><span class="csl-right-inline">X. Wu and X. Zhang, <span>“Automated inference on criminality using face images.”</span> 2017.Available: <a href="https://arxiv.org/abs/1611.04135">https://arxiv.org/abs/1611.04135</a></span>
</span>
</span> and sexuality<span class="citation" data-cites="SexualityFace"><a href="#ref-SexualityFace" role="doc-biblioref">[15]</a></span><span class="marginnote"><span id="ref-SexualityFace" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[15] </span><span class="csl-right-inline">Y. Wang and M. Kosinski, <span>“Deep neural networks are more accurate than humans at detecting sexual orientation from facial images,”</span> <em>Journal of Personality and Social Psychology</em>, 2018.</span>
</span>
</span> and ethnicity<span class="citation" data-cites="EthnicityFace"><a href="#ref-EthnicityFace" role="doc-biblioref">[16]</a></span><span class="marginnote"><span id="ref-EthnicityFace" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[16] </span><span class="csl-right-inline">C. Wang, Q. Zhang, W. Liu, Y. Liu, and L. Miao, <span>“Facial feature discovery for ethnicity recognition,”</span> <em>Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</em>, 2018.</span>
</span>
</span> from facial images have sparked controversy in the academic community. The latter in particular looks for facial features that identify among others, Chinese Uyghurs. Physiognomy (judging character from the physical features of a persons face) and phrenology (judging a persons level of intelligence from the shape and dimensions of their cranium) have historically been used as pseudo-scientific tools of oppressors, to prove the inferiority races and justify subordination and genocide. it is not without merit then to ask if some technologies should be built at all. Machine gaydar might be a fun application to mess about with friends for some, but in the 70 countries where homosexuality is still illegal (some of which enforce the death penalty) it is something rather different.</p>
</section>
</section>
<section id="personalisation-and-the-filter-bubble" class="level3" data-number="1.5.2">
<h3 data-number="1.5.2"><span class="header-section-number">1.5.2</span> Personalisation and the Filter Bubble</h3>
<p>Many believed the internet would breath new life into democracy. The decreased cost and increased accessibility of information would result in greater decentralization of power and flatter social structures. In this new era, people would be able to connect, share ideas and organise grass roots movements at a such a scale that would enable a step change in the rate of social progress. Some of these ideas have been realised to an extent but the increased ability to create and distribute content and corresponding volume of data has created new problems. The amount of information available to us through the internet is overwhelming. Email, blog posts, Twitter, Facebook, Instagram, Linked In, What’s App, You Tube, Netflix, TikTok and more. Today there are seemingly endless ways and places for us to communicate and share information. This barrage of information has resulted in what has been described as the attention crash. There is simply too much information for us to attend to all of it meaningfully. The mechanisms through which we can acquire new information that demands our attention too have expanded. We carry our smart phones everywhere we go and sleep beside them. There is hardly a waking moment, when we are unplugged and inaccessible. The demands on our attention and focus have never been greater. Media producers themselves have adapted their content in order to accommodate our new shortened attention spans.</p>
<p>With so much information available it’s easy to see the appeal of automatic filtering and curation. And of course, how good would said system really be if it didn’t take into account our personal tastes and preferences? So what’s the problem?! Over the last decade, personalisation has become entrenched in the systems we interact with day to day. Targeted advertising was just the beginning. Now it’s not just the trainers you browsed once that follow you around the web until you buy them, it’s everything. Since 2009, Google has returned personalised results every time someone queries their search engine, so two people who enter the same text don’t get the same result. In 2021 You Tube had more than two billion logged-in monthly users. Three quarters of adults in the US use it (more than facebook and Instagram) and 80% of U.S. parents of children under 11 watch it. It is the second most visited site in the world, after Google with visitors checking on average just under 9 pages, and spending 42 minutes per day there. In 2018, 70% of the videos people watched on You Tube were recommended. Some 40% of Americans under thirty get their news through social networking sites such as twitter and Facebook but this may be happening without you even knowing. Since 2010, it’s not the Washington Post that decides which news story you see in the prime real estate that is the top right hand corner of their home page, it’s Facebook - the same goes for the New York Times. So the kinds of algorithms that once determined what we spent our money on now determine our very perception of the world around us. The only question is, what are they optimising for?</p>
<p>Ignoring, for a moment, the fact that having the power to shape people’s perception of the world, in just a few powerful hands is in itself a problem. A question worth pondering on is what kind of citizens people who only ever see things they ‘like’, or feel the impulse to ’comment’ on (or indeed any other proxy for interest/engagement/attention) would make. As Eli Pariser put it in his book The Filter Bubble, “what one seems to like may not be what one actually wants, let alone what one needs to know to be an informed member of their community or country”. The internet has made the world smaller and with it we’ve seen great benefits. But the idea that, because anyone (regardless of their background) could be our neighbour, people would find common ground has not been realised to the extent people hoped. In some senses personalisation does the exact opposite. It risks us all living in a world full of mirrors, where we only ever hear the voices of people who see the world as we do, being deprived of differing perspectives. Of course we have always lived in our own filter bubble in some respects but the thing that has changed is that now we don’t make the choice and often don’t even know when we are in it. We don’t know when or how decisions are made about what we should see. We are more alone in our bubbles than we have ever been before.</p>
<p>Social capital is created by the interpersonal bonds we build in shared identity, values, trust and reciprocity. It encourages people to collaborate in order to solve common problems for the common good. There are two kinds of social capital, bonding and bridging. Bonding capital is acquired through development of connections in groups that have high levels of similarity in demographics and attitudes - the kind you might build by, say socialising with colleagues from work. Bridging capital is created when people from different backgrounds (race, religion, class) connect - something that might happen at a town hall meeting say. The problem with personalisation is that by construction it reduces opportunities to see the world through the eyes of people who don’t necessarily look like us. It reduces bridging capital and that exactly the kind of social capital we need to solve wider problems that extend beyond our own narrow or short term self interests.</p>
</section>
<section id="disinformation" class="level3" data-number="1.5.3">
<h3 data-number="1.5.3"><span class="header-section-number">1.5.3</span> Disinformation</h3>
<p>In June 2016, it was announced that Britain would be leaving the EU. 33.5 million people voted in the referendum of which 51.9% voted to leave. The decision that will impact the UK for, not just a term, but generations to come, rested on less than 2% of voters. Ebbw Vale is a small town in Wales where 62% of the electorate (the largest majority in the country) voted to leave. The town has a history in steel and coal dating back to the late 1700’s. By the 1930’s the Ebbw Vale Steelworks was the largest in Europe by volume. In the 1960’s it employed some 14,500 people. But, towards the end of the 1900’s, after the collapse of the UK steel industry, the town suffered one of the highest unemployment rates in Britain. What was strange about the overwhelming support to leave was that Ebbw Vale was perhaps one of the largest recipients of EU development funding in the UK. A £350m regeneration project funded by the EU replaced the industrial wasteland left behind when the steelworks closed in 2002 with The Works (a housing, retail and office space, wetlands, learning campus and more). A further £33.5 in funding from the European Social Fund paid for a new college and apprenticeships, to help young people learn a trade. An additional £30 million for a new railway line, £80 million for road improvements and shortly before the vote a further £12.2 million for other upgrades and improvements were all from the EU.</p>
<p>When journalist Carole Cadwalladr returned to the small town where she had grown up to report on why residents had voted so overwhelmingly in favour of leaving the EU, she was no less confused. It was clear how much the town had benefited from being part of the EU. The new road, train station, college, leisure centre and enterprise zones (flagged an EU tier 1 area, eligible for the highest level of grant aid in the UK), everywhere she went she saw signs with proudly displayed EU flags saying so. So she wandered around town asking people and was no less perplexed by their answers. Time and time again people complained about immigration and foreigners. They wanted to take back control. But the immigrants were nowhere to be found, because Ebbw Vale had one of the lowest rates of immigration in the country. So how did this happen? How did a town with hundreds of millions of pounds of EU funding vote to leave the EU because of immigrants that didn’t exist? In her emotive TED talk<span class="citation" data-cites="CarCadTED"><a href="#ref-CarCadTED" role="doc-biblioref">[17]</a></span><span class="marginnote"><span id="ref-CarCadTED" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[17] </span><span class="csl-right-inline">C. Cadwalladr, <em>Facebook’s role in <span>Brexit</span> - and the threat to democracy</em>. TED, 2019.</span>
</span>
</span>, Carole shows images of some the adverts on Facebook, people were targeted with as part of the leave campaign (see Figure <a href="#fig:Brexit" data-reference-type="ref" data-reference="fig:Brexit">1.5</a>). They were all centred around a lie - that Turkey was joining the EU.</p>
<figure>
<img src="01_Context/figures/Fig_Brexit.png" id="fig:Brexit" alt="Figure 1.5: Targeted disinformation adverts shown on Facebook[17]." />
<figcaption aria-hidden="true">Figure 1.5: Targeted disinformation adverts shown on Facebook<sup><span class="citation" data-cites="CarCadTED"><a href="#ref-CarCadTED" role="doc-biblioref">[17]</a></span></sup>.</figcaption>
</figure>
<p>Most people in the UK saw adverts on buses and billboards with false claims, for example that the National Health Service (NHS) would have an extra £350 million a week, if we left the EU. Although many believed them, those adverts circulated in the open for everyone to see, giving the mainstream media at the opportunity to debunk them. The same cannot be said for the adverts in Figure <a href="#fig:Brexit" data-reference-type="ref" data-reference="fig:Brexit">1.5</a>. They were targeted towards specific individuals, as part of an evolving stream of information displayed in their Facebook ‘news’ feed. The leave campaign paid Cambridge Analytica (a company that had illegally gained access to the data of 87 million Facebook users) to identify individuals that could be manipulated into voting leave. In the UK, spending on elections in the is limited by law as a means to ensure fair elections. After a nine month investigation, the UK’s Electoral Commission confirmed these spending limits had been breached by the leave campaign. There are ongoing criminal investigations into where the funds for the campaign originate (overseas funding of election campaigns is also illegal) but evidence suggests ties with Russia. Brexit was the precursor to the Trump administration winning the US election just a few months later that year. The same people and companies used the same strategies. It’s become clear that current legislation protecting democracy is inadequate. Facebook, was able to profit from politically motivated money without recognizing any responsibility in ensuring the transactions were legal. Five years later, the full extent of the disinformation campaign on Facebook has yet to be understood. Who was shown what and when, how people were targeted, what other lies were told, who paid for the adverts or where the money came from.</p>
<p>Since then deep learning technology has advanced to the point of being able to pose as human in important ways that risk enabling disinformation not just through targeted advertising but machines impersonating humans. GANs can fabricate facial images, videos (deepfakes) and audio. Advancements in language models (Open AIs GPT-2 and more recently GPT-3) are capable of creating lengthy human like prose given just a few prompts. Deep learning now provides all the tools to fabricate human identities and target dissemination of false information at scale. There are growing concerns that in the future, bots will drown out actual human voices. As for the current state of play, it’s difficult to know the exact numbers but in 2017, researchers estimated that between 9 and 15% of all twitter accounts were bots<span class="citation" data-cites="FakeTwitter"><a href="#ref-FakeTwitter" role="doc-biblioref">[18]</a></span><span class="marginnote"><span id="ref-FakeTwitter" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[18] </span><span class="csl-right-inline">O. Varol, E. Ferrara, C. A. Davis, F. Menczer, and A. Flammini, <span>“Online human-bot interactions: Detection, estimation, and characterization.”</span> 2017.Available: <a href="https://arxiv.org/abs/1703.03107">https://arxiv.org/abs/1703.03107</a></span>
</span>
</span>. In 2020 a study by researchers at Carnegie Mellon University reported that 45% of the 200 million tweets they analysed discussing coronavirus came from accounts that behaved like bots<span class="citation" data-cites="FakeCovid"><a href="#ref-FakeCovid" role="doc-biblioref">[19]</a></span><span class="marginnote"><span id="ref-FakeCovid" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[19] </span><span class="csl-right-inline">B. Allyn, <span>“Researchers: Nearly half of accounts tweeting about coronavirus are likely bots,”</span> <em>NPR</em>, May 2020.</span>
</span>
</span>. For Facebook, things are less clear as we must rely on their own reporting. In mid-2019, Facebook estimated that only 5% of its 2.4 billion monthly active users were fake though its reporting raised some questions<span class="citation" data-cites="FakeFB"><a href="#ref-FakeFB" role="doc-biblioref">[20]</a></span><span class="marginnote"><span id="ref-FakeFB" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[20] </span><span class="csl-right-inline">J. Nicas, <span>“Does facebook really know how many fake accounts it has?”</span> <em>The New York Times</em>, 2019.</span>
</span>
</span>.</p>
</section>
<section id="harms-of-representation" class="level3" data-number="1.5.4">
<h3 data-number="1.5.4"><span class="header-section-number">1.5.4</span> Harms of Representation</h3>
<p>The interventions we’ll talk about in most of this book are designed to measure and mitigate harms of allocation in machine learning systems.</p>
<section id="harms-of-allocation" class="level4 unnumbered">
<h4 class="unnumbered">Harms of Allocation</h4>
<p>An allocative harm happens when a system allocates or withholds an opportunity or resource. Systems that approve or deny credit allocate financial resources; systems that decide who should and should not see adverts for high paying jobs allocate employment opportunities and systems that determine who will make a good tenant allocate housing resources. Harms of allocation happen as a result of discrete decisions at a given point in time, the immediate impact of which can be quantified. This makes it possible to challenge the justice and fairness of specific determinations and outcomes.</p>
<p>Increasingly however, machine learning systems are affecting us, not just through allocation, but are shaping our view of the world and society at large by deciding what we do and don’t see. These harms are far more difficult to quantify.</p>
</section>
<section id="harms-of-representation-1" class="level4 unnumbered">
<h4 class="unnumbered">Harms of Representation</h4>
<p>Harms of representation occur when systems enforce the subordination of groups through characterizations that affect the perception of them. In contrast to harms of allocation, harms of representation have long-term effects on attitudes and beliefs. They create identities and labels for humans, societies and their cultures. Harms of representation don’t just affect our perception of each other, they affect how we see ourselves. They are difficult to formalise and in turn difficult to quantify but the effect is real.</p>
<div class="lookbox">
<p><strong>The Surgeon’s Dilemma</strong></p>
<p>A father and his son are involved in a horrific car crash and the man died at the scene. But when the child arrived at the hospital and was rushed into the operating theatre, the surgeon pulled away and said: “I can’t operate on this boy, he’s my son”. How can this be?</p>
</div>
<p>Did you figure it out? How long did it take? There is, of course, no reason why the surgeon couldn’t be the boy’s mother. If it took you a while to figure out, or came to a different conclusion, you’re not alone. More than half the people presented with this riddle do, and that includes women. The point of this riddle is to demonstrate the existence of unconscious bias. Representational harms are insidious. They silently fix ideas in peoples subconscious about what people of a particular gender, nationality, faith, race, occupation and more, are like. They draw boundaries between people and affect our perception of world. Below we describe five different harms of representation:</p>
</section>
<section id="stereotyping" class="level4 unnumbered">
<h4 class="unnumbered">Stereotyping</h4>
<p>Stereotyping occurs through excessively generalised portrayals of groups. In 2016, the Oxford English Dictionary was publicly criticised<span class="citation" data-cites="SexistOED"><a href="#ref-SexistOED" role="doc-biblioref">[21]</a></span><span class="marginnote"><span id="ref-SexistOED" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[21] </span><span class="csl-right-inline">E. O’Toole, <span>“A dictionary entry citing <span>‘rabid feminist’</span> doesn’t just reflect prejudice, it reinforces it,”</span> <em>The Guardian</em>, 2016.</span>
</span>
</span> for employing the phrase “rabid feminist” as a usage example for the word rabid. The dictionary included similarly sexist common usages for other words like shrill, nagging and bossy. But even before this, historical linguists observed that words referring to women undergo pejoration (when the meaning of a word deteriorates over time) far more often than those referring to men<span class="citation" data-cites="Pejoration"><a href="#ref-Pejoration" role="doc-biblioref">[22]</a></span><span class="marginnote"><span id="ref-Pejoration" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[22] </span><span class="csl-right-inline">D. Shariatmadari, <span>“Eight words that reveal the sexism at the heart of the english language,”</span> <em>The Guardian</em>, 2016.</span>
</span>
</span>. Consider words like mistress (once simply the female equivalent of master, now used to describe a woman in an illicit relationship with a married man); madam (once simply the female equivalent of sir, now also used to describe a woman who runs a brothel); hussy (once a neutral term for the head of a household, now used to describe an immoral or ill-behaved woman); and governess (female equivalent of governor, later used to describe a woman responsible for the care of children).</p>
<p>Unsurprisingly then, gender stereotyping is known to be a problem in natural language processing systems. In 2016 Bolukbasi et al. showed that word embeddings exhibited familiar gender biases in relation to occupations<span class="citation" data-cites="WomanHomemaker"><a href="#ref-WomanHomemaker" role="doc-biblioref">[23]</a></span><span class="marginnote"><span id="ref-WomanHomemaker" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[23] </span><span class="csl-right-inline">T. Bolukbasi, K.-W. Chang, J. Zou, V. Saligrama, and A. Kalai, <span>“Man is to computer programmer as woman is to homemaker? Debiasing word embeddings.”</span> 2016.Available: <a href="https://arxiv.org/abs/1607.06520">https://arxiv.org/abs/1607.06520</a></span>
</span>
</span>. By performing arithmetic on word vectors, they were able to uncover relationships such as <span class="math display">\[\overrightarrow{\textrm{man}} - \overrightarrow{\textrm{woman}} \approx \overrightarrow{\textrm{computer programmer}} - \overrightarrow{\textrm{homemaker}}.\]</span></p>
<p>In 2017 Caliskan et al. found that Google Translate contained similar gender biases.<span class="citation" data-cites="BiasSemantics"><a href="#ref-BiasSemantics" role="doc-biblioref">[24]</a></span><span class="marginnote"><span id="ref-BiasSemantics" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[24] </span><span class="csl-right-inline">A. Caliskan, J. J. Bryson, and A. Narayanan, <span>“Semantics derived automatically from language corpora contain human-like biases,”</span> <em>Science</em>, vol. 356, pp. 183–186, 2017.</span>
</span>
</span> In their research they found that “translations to English from many gender-neutral languages such as Finnish, Estonian, Hungarian, Persian, and Turkish led to gender-stereotyped sentences”. So for example when they translated Turkish sentences with genderless pronouns: “O bir doktor. O bir hemişre.”, the resulting English sentences were: “He is a doctor. She is a nurse.” They performed these types of tests for 50 occupations and found that the stereotypical gender association of the word almost perfectly predicted the resulting pronoun in the English translation.</p>
</section>
<section id="recognition" class="level4 unnumbered">
<h4 class="unnumbered">Recognition</h4>
<p>Harms of recognition happen when groups of people are in some senses erased by a system through failure to recognise. In her <a href="https://www.ted.com/talks/joy_buolamwini_how_i_m_fighting_bias_in_algorithms/transcript?language=en">TED Talk</a>, Joy Buolamwini, talks about how as an undergraduate studying computer science she worked on social robots. One of her projects involved creating a robot which could play peek-a-boo, but she found that her robot (which used third party software for facial recognition) could not see her. She was forced to borrow her roommate’s face to complete the project. After her work auditing several popular gender classification packages from IBM, Microsoft and Face++ in the project <a href="http://gendershades.org/overview.html">Gender Shades</a><span class="citation" data-cites="GenderShades"><a href="#ref-GenderShades" role="doc-biblioref">[25]</a></span><span class="marginnote"><span id="ref-GenderShades" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[25] </span><span class="csl-right-inline">J. Buolamwini and T. Gerbru, <em>Gender shades: Intersectional accuracy disparities in commercial gender classification</em>, vol. 81. Proceedings of Machine Learning Research, 2018, pp. 1–15.</span>
</span>
</span> in 2017 and seeing the failure of these technologies on the faces of some of the most recognizable Black women of her time, including Oprah Winfrey, Michelle Obama, and Serena Williams, she was prompted to echo the words of Sojourner Truth in asking “<a href="https://medium.com/@Joy.Buolamwini/when-ai-fails-on-oprah-serena-williams-and-michelle-obama-its-time-to-face-truth-bf7c2c8a4119">Ain’t I a Woman?</a>”. Harms of recognition are failures in seeing humanity in people.</p>
</section>
<section id="denigration" class="level4 unnumbered">
<h4 class="unnumbered">Denigration</h4>
<p>In 2015, much to the horror of many people, it was reported that <a href="https://www.bbc.com/news/technology-33347866">Google Photos had labelled a photo of a Black couple as Gorillas</a>. It’s hard to find the right words to describe just how offensive an error this is. It demonstrated how a machine, carrying out a seemingly benign task of labelling photos, could deliver an attack on a person’s human dignity.</p>
<p>In 2020, an ethical audit of several large computer vision datasets<span class="citation" data-cites="TinyImages"><a href="#ref-TinyImages" role="doc-biblioref">[26]</a></span><span class="marginnote"><span id="ref-TinyImages" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[26] </span><span class="csl-right-inline">V. U. Prabhu and A. Birhane, <span>“Large image datasets: A pyrrhic win for computer vision?”</span> 2020.Available: <a href="https://arxiv.org/abs/2006.16923">https://arxiv.org/abs/2006.16923</a></span>
</span>
</span>, revealed some disturbing results. TinyImages (a dataset of 79 million 32 x 32 pixel colour photos compiled in 2006, by MIT’s Computer Science and Artificial Intelligence Lab for image recognition tasks) contained racist, misogynistic and demeaning labels with corresponding images. Figure <a href="#fig:TinyImages" data-reference-type="ref" data-reference="fig:TinyImages">1.6</a> shows a subset of the data found in TinyImages.</p>
<figure>
<img src="01_Context/figures/Fig_TinyImages.png" id="fig:TinyImages" alt="Figure 1.6: Subset of data in TinyImages exemplifying toxicity in both the images and labels[26]." />
<figcaption aria-hidden="true">Figure 1.6: Subset of data in TinyImages exemplifying toxicity in both the images and labels<span class="citation" data-cites="TinyImages"><a href="#ref-TinyImages" role="doc-biblioref">[26]</a></span>.</figcaption>
</figure>
<p>The problem, unfortunately, does not end here. Many of the datasets used to train and benchmark, not just computer vision but natural language processing tasks, are related. Tiny Images was compiled by searching the internet for images associated with words in WordNet (a machine readable, lexical database, organised by meaning, developed at Princeton), which is where TinyImages inherited its labels from. ImageNet (widely considered to be a turning point in computer vision capabilities) is also based on WordNet and, Cifar-10 and Cifar-100 were derived from TinyImages.</p>
<p>Vision and language datasets are enormous. The time, effort and consideration in collecting the data that forms the foundation of these technologies (compared to that which has gone into advancing the models built on them), is questionable to say the least. Furthermore a dataset can have impact beyond the applications trained on it, because datasets often don’t just die, they evolve. This calls into question the technologies that are in use today, capable of creating persistent representations of our world, and trained on datasets so large they are difficult and expensive to audit.</p>
<p>And there’s plenty of evidence to suggest that this is a problem. For example, in 2013, a study found that Google searches were more likely to return personalised advertisements that were suggestive of arrest records for Black names<span class="citation" data-cites="LatanyaSweeney"><a href="#ref-LatanyaSweeney" role="doc-biblioref">[27]</a></span><span class="marginnote"><span id="ref-LatanyaSweeney" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[27] </span><span class="csl-right-inline">L. Sweeney, <span>“Discrimination in online ad delivery,”</span> <em>SSRN</em>, 2013.</span>
</span>
</span> than White<span class="sidenote-wrapper"><label for="sn-3" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-3" class="margin-toggle"/><span class="sidenote">Suggestive of an arrest record in the sense that they claim to have arrest records specifically for the name that you searched, regardless of whether they do in reality have them.<br />
<br />
</span></span> This doesn’t just result in allocative harms for people applying for jobs for example, it’s denigrating. <a href="https://www.vice.com/en_us/article/j5jmj8/google-artificial-intelligence-bias">Google’s Natural Language API for sentiment analysis is also known to have problems</a>. In 2017, it was assigning negative sentiment to sentences such as “I’m a jew” and “I’m a homosexual” and “I’m black”; neutral sentiment to the phrase “white power” and positive sentiment to the sentences “I’m christian” and “I’m sikh”.</p>
</section>
<section id="under-representation" class="level4 unnumbered">
<h4 class="unnumbered">Under-representation</h4>
<p>In 2015, the New York Times reported, that “<a href="https://www.nytimes.com/2015/03/03/upshot/fewer-women-run-big-companies-than-men-named-john.html">Fewer women run big companies than men named John</a>”, despite this Google’s image search still managed to under-represent women in search results for the word “CEO”. Does this really matter? What difference would an alternate set of search results make? A study the same year found that “people rate search results higher when they are consistent with stereotypes for a career, and shifting the representation of gender in image search results can shift people’s perceptions about real-world distributions.”<span class="citation" data-cites="OccupationImageSearch"><a href="#ref-OccupationImageSearch" role="doc-biblioref">[28]</a></span><span class="marginnote"><span id="ref-OccupationImageSearch" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[28] </span><span class="csl-right-inline">M. Kay, C. Matuszek, and S. A. Munson, <span>“Unequal representation and gender stereotypes in image search results for occupations,”</span> <em>ACM</em>, 2015.</span>
</span>
</span>.</p>
</section>
<section id="ex-nomination" class="level4 unnumbered">
<h4 class="unnumbered">Ex-nomination</h4>
<p>Ex-nomination occurs through invisible means and affects people’s views of the norms within societies. It tends to happen through mechanisms which amplify the presence of some groups and suppress the presence of others. The cultures, beliefs, politics of ex-nominated groups over time become the default. The most obvious example is the ex-nomination of Whiteness and White culture in western society, which might sound like a bizarre statement - what is White culture? But such is the effect of ex-nomination, you can’t describe it, because it is just the norm and everything else is not. Richard Dyer in his book White examines the reproduction and preservation of whiteness in visual media over five centuries, from the depiction of the crucifixion to modern day film. It’s perhaps should not come as a surprise then, when facial recognition software can’t see black faces; or when gender recognition software fails more often than not for black women; or that a generative model that improves the resolution of images, converted a pixelated picture of Barack Obama, into a high-resolution image of a white man.</p>
<p>The ex-nomination of White culture is evident in our language too, in terminology like whitelist and white lie. If you look up white in dictionary and or thesaurus and you’ll find words like innocent and pure, light, transparent, immaculate, neutral. Doing the same for the word black on the other hand, returns very different associations, dirty, soiled, evil, wicked, black magic, black arts, black mark, black humour, blacklist and black is often used as a prefix in describing disastrous events. A similar assessment can be made for gender with women being under-represented in image data and feminine versions of words more often undergoing pejoration (when the meaning or status of a word deteriorates over time).</p>
<p>Members of ex-nominated groups experience a kind of privilege that it is easy to be unaware of. It is a power that comes from being the norm. They have advantages that are not earned, outside of their financial standing or effort, that the ‘equivalent’ person outside the ex-nominated group would not. Their hair type, skin tone, accent, food preferences and more are catered to by every store, product, service and system and it cost less to access them; they see themselves represented in the media and are more often represented in a positive light; they are not subject to profiling or stereotypes; they are more likely to be treated as individuals rather than as representative of (or as exceptions to) a group; they are more often humanised - more likely to be be given the benefit of the doubt, treated with compassion and kindness and thus recover from mistakes; they are less likely to be suspected of crimes; more likely to be trusted financially; they have greater access to opportunities, resources and power and are able to climb financial, social and professional ladders faster. The advantages enjoyed by ex-nominated groups accumulate over time and compound over generations.</p>
</section>
</section>
</section>
<section id="summary" class="level2 unnumbered">
<h2 class="unnumbered">Summary</h2>
<section id="bias-in-machine-learning-1" class="level3 unnumbered">
<h3 class="unnumbered">Bias in Machine learning</h3>
<ul>
<li><p>In this book we use algorithm and model interchangeably. A model can be determined using data, but it need not be. It can simply express an opinion on the relationship between variables. In practice the implementation is an algorithm either way. More precisely, a model is a function or mapping; given a set of input variables (features) it returns a decision or prediction for the target variable.</p></li>
<li><p>Obtaining adequately rich and relevant data is a major limitation of machine learning models.</p></li>
<li><p>At almost every important life event, going to university, getting a job, buying a house, getting sick, decisions are increasingly being made by machines. By construction, these models encode existing societal biases. They not only proliferate but are capable of amplifying them and are easily deployed at scale. Understanding the shortcomings of these models and ensuring such technologies are deployed responsibly are essential if we are to safeguard social progress.</p></li>
</ul>
</section>
<section id="a-philosophical-perspective" class="level3 unnumbered">
<h3 class="unnumbered">A Philosophical Perspective</h3>
<ul>
<li><p>According to uilitarian doctrine, the correct course of action (when faced with a dilemma) is the one that maximises the benefit for the greatest number of people. The doctrine demands that the benefits to all people are are counted equally.</p></li>
<li><p>The approach to training a model (assuming errors in either direction are equally harmful and accurate predictions are equally beneficial), is loosely justified in a utilitarian sense; we optimise our decision process to maximise benefit for the greatest number of people.</p></li>
<li><p>Utilitarianism is a flavour of consequentialism, a branch of ethical theory that holds that consequences are the yardstick against which we must judge the morality of our actions. In contrast deontological ethics judges the morality of actions against a set of rules that define our duties or obligations towards others. Here it is not the consequences of our actions that matter but rather intent.</p></li>
<li><p>There are some practical problems with utilitarianism but perhaps the most significant flaw in utilitarianism for moral reasoning is the omission of justice as a consideration.</p></li>
<li><p>Principles of Justice as Fairness:</p>
<ol>
<li><p><strong>Liberty principle:</strong> Each person has the same indefeasible claim to a fully adequate scheme of equal basic liberties, which is compatible with the same scheme of liberties for all;</p></li>
<li><p><strong>Equality principle:</strong> Social and economic inequalities are to satisfy two conditions:</p>
<ol>
<li><p><strong>Fair equality of opportunity:</strong> The offices and positions to which they are attached are open to all under conditions of fair equality of opportunity;</p></li>
<li><p><strong>Difference principle</strong> They must be of the greatest benefit to the least-advantaged members of society.</p></li>
</ol></li>
</ol>
<p>The principles of justice as fairness are ordered by priority so that fulfilment of the liberty principle takes precedence over the equality principles and fair equality of opportunity takes precedence over the difference principle. In contrast to utilitarianism, justice as fairness introduces a number of constraints that must be satisfied for a decision process to be fair. Applied to a machine learning one might interpret the liberty principle as a requirement of some minimum accuracy level (maximum probability of error) to be set for all members of the population, even if this means the algorithm is less accurate overall. Parallels can be drawn here in machine learning where there is a trade-off between fairness and utility of an algorithm.</p></li>
</ul>
</section>
<section id="a-legal-perspective-1" class="level3 unnumbered">
<h3 class="unnumbered">A Legal Perspective</h3>
<ul>
<li><p>Anti-discrimination laws were born out of long-standing, vast and systemic discrimination against historically oppressed and disadvantaged classes. Such discrimination has contributed to disparities in all measures of prosperity (health, wealth, housing, crime, incarceration) that persist today.</p></li>
<li><p>Legal liability for discrimination against protected classes may be established through both disparate treatment and disparate impact. Disparate treatment (also described as direct discrimination in Europe) refers to both formal differences in the treatment of individuals based on protected characteristics, and the intent to discriminate. Disparate impact (also described as indirect discrimination in Europe) does not consider intent but is concerned with policies and practices that disproportionately impact protected classes.</p></li>
<li><p>Just as the meaning of fairness is subjective, so too is the interpretation of anti-discrimination laws. Two conflicting interpretations are anti-classification and anti-subordination. Anti-classification is a weaker interpretation, that the law is intended to prevent classification of people based on protected characteristics. Anti-subordination is the stronger interpretation that anti-discrimination laws exist to prevent social hierarchies, class or caste systems based on protected features and, that it should actively work to eliminate them where they exist.</p></li>
</ul>
</section>
<section id="a-technical-perspective" class="level3 unnumbered">
<h3 class="unnumbered">A Technical Perspective</h3>
<ul>
<li><p>Identifying bias in data can be tricky. Data can be misleading. An association paradox is a phenomenon where an observable relationship between two variables disappears or reverses after controlling for one or more other variables.</p></li>
<li><p>In order to know which associations (or distributions) are relevant, i.e. the marginal (unconditional) or partial associations (conditional distributions), one must understand the causal nature of the relationships.</p></li>
<li><p>Association paradoxes can also occur for non-collapsible measures of association. Collapsible measures of association are those which can be expressed as the weighted average of the partial measures.</p></li>
</ul>
</section>
<section id="whats-the-harm" class="level3 unnumbered">
<h3 class="unnumbered">What’s the harm?</h3>
<ul>
<li><p>It is important to be clear that in general, machine learning systems are not objective. Data is produced by a necessarily subjective set of decisions. The consistency of algorithms in decision making compared to humans (who make decisions on a case by case basis) is often described as a benefit, but it’s their very consistency that makes them dangerous - capable of discriminating systematically and at scale.</p></li>
<li><p>Classification creates a sense of order and understanding. It enables us to find things more easily, formulate problems neatly and solve them. But classifying people inevitably has the effect of reducing people labels; labels that can result in people being treated as members of a group, rather than individuals.</p></li>
<li><p>Personalisation algorithms that shape our perception of the world in a way that covertly mirror our beliefs can have the effect of trading bridging for bonding capital, the former kind is important in solving global problems that require collective action, such as global warming.</p></li>
<li><p>Targeted political advertising and technologies that enable machines to impersonate humans are powerful tools that can be used as part of orchestrated campaigns of disinformation that manipulate perceptions at an individual level and yet at scale. They are capable of causing great harm to political and social institutions and pose a threat to security.</p></li>
<li><p>An allocative harm happens when a system allocates or withholds an opportunity or resource. Harms of representation occur when systems enforce the subordination of groups through characterizations that affect the perception of them. In contrast to harms of allocation, harms of representation have long-term effects on attitudes and beliefs. They create identities and labels for humans, societies and their cultures. Harms of representation affect our perception of each other and even ourselves. Harms of representation are difficult to quantify. Some types of harms of representation are, stereotyping, (failure of) recognition, denigration, under-representation and ex-nomination.</p></li>
</ul>
</section>
</section>
</section>
<section id="ch_EthicalDev" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Ethical development</h1>
<div class="chapsumm">
<p><strong>This chapter at a glance</strong></p>
<ul>
<li><p>The machine learning cycle - feedback from models to data</p></li>
<li><p>The machine learning development and deployment life cycle</p></li>
<li><p>A practical approach to ethical development and deployment</p></li>
<li><p>A taxonomy of common causes of bias</p></li>
</ul>
</div>
<p>In this chapter, we transition to a more systematic approach to understanding the problem of fairness in decisions making systems. In later chapters we will look at different measures of fairness and bias mitigation techniques but before we discuss and analyse these methods, we review some more practical aspects of responsible model development and deployment. None of the bias mitigation techniques that we will talk about in part three of this book will rectify a poorly formulated, discriminatory machine learning problem or remedy negligent deployment of a predictive algorithm. A model in itself is not the source of unfair or illegal discrimination, models are developed and deployed by people as part of a process. In order to address the problem of unfairness we need to look at the whole system, not just the data or the model.</p>
<p>We’ll start by looking at the machine learning cycle and discuss the importance of how a model is used in the feedback effect it has on data. Where models can be harmful we should expect to have processes in place that aim to avoid common, foreseeable or catastrophic failures. We’ll discuss how to take a proactive rather than reactive approach to managing risks associated with models. We’ll discuss where in the machine learning model development cycle bias metrics and modelling interventions fit. Finally, we’ll classify the most common causes of bias, identifying the parts of the workflow to which they relate.</p>
<p>Our goal is to present problems and interventions schematically, creating a set of references for building, reviewing, deploying and monitoring machine learning solutions that aim to avoid the common pitfalls that result in unfair models. We take a high enough view that the discussion remains applicable to many machine learning applications. The specifics of the framework, can be tailored for a particular use case. Indeed the goal is for the resources in this chapter can be used as a starting point for data science teams that want to develop their own set of standards. Together we will progress towards thinking critically about the whole machine learning cycle, development, validation, deployment and monitoring of machine learning systems. By the end of this chapter we will have a clearer picture of what due diligence in model development and deployment might look like from a practical perspective.</p>
<section id="machine-learning-cycle" class="level2" data-number="2.1">
<h2 data-number="2.1"><span class="header-section-number">2.1</span> Machine Learning Cycle</h2>
<figure>
<img src="02_EthicalDevelopment/figures/Fig_MLCycle.png" id="fig:MLCycle" style="width:65.0%" alt="Figure 2.1: The machine learning cycle" />
<figcaption aria-hidden="true">Figure 2.1: The machine learning cycle</figcaption>
</figure>
<p>Machine learning systems can have long-term and compounding effects on the world around us. In this section we analyse the impact in a variety of different examples to breakdown the mechanisms that determine the nature and magnitude of the effect. In Figure <a href="#fig:MLCycle" data-reference-type="ref" data-reference="fig:MLCycle">2.1</a>, we present the machine learning cycle - a high-level depiction of the interaction between a machine learning solution and the real world. A machine learning system starts with a set of objectives. These can be achieved in a myriad of different ways. The translation of these objectives, into a tractable machine learning problem, consists of a series of subjective decisions; what data we collect to train a model on, what events we predict, what features we use, how we clean and process the data, how we evaluate the model and the decision policy are all choices. They determine the model we create, the actions we take and finally the resulting cycle of feedback on the data.</p>
<p>The most familiar parts of the cycle to most developers of machine learning solutions are on the right hand side; processing data, model selection, training and cross validation and prediction. Each action taken on the basis of our model prediction creates a new world state, which generates new data, which we collect and train our model on, and around it goes again. The actions we take based on our model predictions define how we use the model. The same model used in a different way can result in a very different feedback cycle.</p>
<p>Notice that the world state and data are distinct nodes in in the cycle. Most machine learning models rely on the assumption that the training data is accurate, rich and representative of the population, but this is often not the case. Data is a necessarily subjective representation of the world. The sample may be biased, contain an inadequate collection of features, subjective decisions around how to categorise features into groups, systematic errors or be tainted with prejudice decisions. We may not even be able to measure the true metric we wish to impact. Data collected for one purpose is often reused for another under the assumption that it represents the ground truth when it does not.</p>
<section id="feedback-from-model-to-data" class="level3" data-number="2.1.1">
<h3 data-number="2.1.1"><span class="header-section-number">2.1.1</span> Feedback from Model to Data</h3>
<p>In cases where the ground truth assignment (target variable choice) systematically disadvantages certain classes, actions taken based on predictions from models trained on the data can reinforce the bias and even amplify it. Similarly, decisions made on the basis of results derived from machine learning algorithms, trained on data that under or over-represents disadvantaged classes, can have feedback effects that further skew the representation of those classes in future data. The cycle of training on biased data (which justifies inaccurate beliefs), taking actions in kind, and further generating data that reinforces those biases can become a kind of self-fulfilling prophecy. The good news is that just as we can create pernicious cycles that exaggerate disparities, we can create virtuous ones that have the effect of reducing them. Let’s take two illustrative examples.</p>
<section id="predictive-policing" class="level4 unnumbered">
<h4 class="unnumbered">Predictive Policing</h4>
<p>In the United States, predictive policing has been implemented by police departments in several states including California, Washington, South Carolina, Alabama, Arizona, Tennessee, New York and Illinois. Such algorithms use data on the time, location and nature of past crimes, in order to determine how and where to patrol and thus improve the efficiency with which policing resources are allocated. A major flaw with these algorithms pertains to the data used to train them. It is not of where crimes occurred, but rather where there have been previous arrests. A proxy target variable (arrests) is used in place of the desired target variable (crime). Racial disparities in policing in the US is a well publicised problem. Figure <a href="#fig:drugs" data-reference-type="ref" data-reference="fig:drugs">2.2</a> demonstrates this disparity for policing of drug related crimes. In 2015, an analysis by The Hamilton Project found that at the state level, Blacks were 6.5 times as Whites to be incarcerated for drug-related crimes<span class="citation" data-cites="HamProj"><a href="#ref-HamProj" role="doc-biblioref">[29]</a></span><span class="marginnote"><span id="ref-HamProj" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[29] </span><span class="csl-right-inline"><span>“Rates of drug use and sales, by race; rates of drug related criminal justice measures, by race.”</span> The Hamilton Project, 2015.</span>
</span>
</span> despite drug related crime being more prevalent among Whites. Taking actions based on predictions from an algorithm trained on arrest data will likely amplify existing disparities between under and over-policed neighbourhoods which correlate with race.</p>
<figure>
<img src="02_EthicalDevelopment/figures/Fig_RatesDrugUseSaleRace.png" id="fig:drugs" alt="Figure 2.2: Rates of drug use and sales compared to criminal justice measures by race[29]." />
<figcaption aria-hidden="true">Figure 2.2: Rates of drug use and sales compared to criminal justice measures by race<span class="citation" data-cites="HamProj"><a href="#ref-HamProj" role="doc-biblioref">[29]</a></span>.</figcaption>
</figure>
</section>
<section id="car-insurance" class="level4 unnumbered">
<h4 class="unnumbered">Car insurance</h4>
<p>As a comparative example, let’s consider car insurance. It is well publicised that car insurance companies discriminate against young male drivers (despite age and gender being legally protected characteristics in the countries where these insurance companies operate) since statistically, they are at higher risk of being involved in accidents. Insurance companies act on risk predictions by determining the price of insurance at an individual level - the higher the risk, the more expensive the cost of insurance. What is the feedback effect of this on the data? Of course young men are disadvantaged by having to pay more, but one can see how this pricing structure acts as an incentive to drive safely. It is in the drivers interest to avoid having an accident that would result in an increase in their car insurance premiums. For a high risk driver in particular, an accident could potentially make it prohibitively expensive for them to drive. The feedback effect on the data would be to reduce the disparity in incidents of road traffic accidents among high and low risk individuals.</p>
<p>Along with the difference in the direction of the feedback effects in the examples given above, there is another important distinction to be made in terms of the magnitude of the feedback effect. This is related to how much control the institution making decisions based on the predictions, has over the data. In the predictive policing example the data is entirely controlled by the police department. They decide where to police and who to arrest, ultimately determining the places and people that do (and don’t) end up in the data. They produce the training data, in its entirety, as a result of their actions. Consequently, we would expect the feedback effect of acting on predictions based on the data to be strong and capable of dramatically shifting the distribution of data generated over time. Insurance companies by comparison, have far less influence over the data (consisting individuals involved in road traffic accidents). Though they can arguably encourage certain driving behaviours through pricing, they do not ultimately determine who is and who is not involved in a car accident. As such, feedback effects of risk-related pricing in car insurance are likely to be less strong in comparison.</p>
<div class="lookbox">
<p><strong>Risk related pricing and discrimination</strong></p>
<p>Do you think age and gender based discrimination in car insurance are fair? Why?</p>
</div>
</section>
</section>
<section id="model-use" class="level3" data-number="2.1.2">
<h3 data-number="2.1.2"><span class="header-section-number">2.1.2</span> Model Use</h3>
<p>We’ve seen some examples illustrating how the strength and direction of feedback from models to (future) data can vary. In this section we’ll demonstrate how the same model can have a very different feedback cycle depending on how it is used (i.e. the actions that are taken based on its predictions). A crucial part of responsible model development and deployment then should be clearly defining and documenting the way in which a model is intended to be used and relevant tests and checks that were performed. In addition, considering potential use cases for which one might be tempted to use the model but for which it is not suitable and documenting them can prevent misuse. Setting out the specific use case is an important part of enabling effective and focused analysis and testing in order to understand both its strengths and weaknesses.</p>
<p>The idea that the use case for a product, tool or model should be well understood before release; that it should be validated and thoroughly tested for that use case and further that the potential harms caused (even for unintended uses) should be mitigated is not novel. In fact, many industries have safety standards set by a regulatory body that enshrine these ideas in law. The motor vehicle industry has a rich history of regulation aimed at reducing risk of death or serious injury from road traffic accidents that continues to evolve today. In the early days, protruding knobs and controls on the dash would impale people in collisions. It was not until the 1960s that seatbelts, collapsing steering columns and head restraints became a requirement. Safety testing and requirements have continued to expand to including rear brake lights, a variety of impact crash tests, ISOFIX child car seat anchors among others. There are many more such examples across different industries but it is perhaps more instructive to consider an example that involves the use of models.</p>
<p>Let’s look at an example in the banking industry. Derivatives are financial products in the form of a contract that result in payments to the holder contingent on future events. The details, such as payment amounts, dates and events that lead to them are outlined in the contract. The simplest kinds of derivatives are called vanilla options; if at expiry, the underlying asset is above (call option) or below (put option) a specified limit, the holder receives the difference. In order to price them one must model the behaviour of the underlying asset over time. As the events which result in payments become more elaborate, so does the modelling required to be able to price them, as does the certainty with which they can be priced. In derivatives markets, it is a well understood fact that valuation models are product specific. A model that is suitable for pricing a simple financial instrument will not necessarily be appropriate for pricing a more complex one. For this reason, regulated banks that trade derivatives must validate models specifically for the instruments they will be used to price and document their testing. Furthermore they must track their product inventory (along with the models being used to price them) in order to ensure that they are not using models to price products for which the are inappropriate. Model suitability is determined via an approval process, where approved models have been validated as part of a model review process to some standard of due diligence has been carried out for the specified use case.</p>
<p>Though machine learning models are not currently regulated in this way, it’s easy to draw parallels when it comes to setting requirements around model suitability. But clear consideration of the use case for a machine learning model is not just about making sure that the model performs well for the intended use case. How a predictive model is used, ultimately determines the actions that are taken in kind, and thus the nature of the feedback it has on future data. Just as household appliances come with manuals and warnings against untested / inappropriate / dangerous uses, datasets and models could be required to be properly documented with descriptions, metrics, analysis around use case specific performance and warnings.</p>
<p>It is worth noting that COMPAS<span class="citation" data-cites="ProPub2"><a href="#ref-ProPub2" role="doc-biblioref">[30]</a></span><span class="marginnote"><span id="ref-ProPub2" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[30] </span><span class="csl-right-inline">J. Larson, S. Mattu, L. Kirchner, and J. Angwin, <span>“How we analyzed the COMPAS recidivism algorithm,”</span> <em>ProPublica</em>, 2016.</span>
</span>
</span> was not developed to be used in sentencing. Tim Brennan (the co-founder of Northpointe and co-creator of its COMPAS risk scoring system) himself stated in a court testimony that they “wanted to stay away from the courts”. Documentation<span class="citation" data-cites="COMPASguide"><a href="#ref-COMPASguide" role="doc-biblioref">[31]</a></span><span class="marginnote"><span id="ref-COMPASguide" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[31] </span><span class="csl-right-inline">Northpointe, <em>Practitioners guide to COMPAS core</em>. 2015.</span>
</span>
</span> for the software (dated 2015 two years later) describes it as a risk and needs assessment and case management system. It talks about it being used “to inform decisions regarding the placement, supervision and case management of offenders” and probation officers using the recidivism risk scales to “triage their case loads”. There is no mention of its use in sentencing. Is it reasonable to assume that a model, developed as a case management tool for probation officers could be used to advise judges with regards to sentencing? Napa County, California, uses a similar risk scoring system in the courts. There a Superior Court Judge who trains other judges in evidence-based sentencing cautions colleagues in their interpretation of the scores. He outlines a concrete example of where the model falls short. “A guy who has molested a small child every day for a year could still come out as a low risk because he probably has a job. Meanwhile, a drunk guy will look high risk because he’s homeless. These risk factors don’t tell you whether the guy ought to go to prison or not; the risk factors tell you more about what the probation conditions ought to be.”<span class="citation" data-cites="ProPub2"><a href="#ref-ProPub2" role="doc-biblioref">[30]</a></span></p>
<p>Propublica’s review of COMPAS looked at recidivism risk for more than 10,000 criminal defendants in Broward County, Florida<span class="citation" data-cites="ProPub3"><a href="#ref-ProPub3" role="doc-biblioref">[32]</a></span><span class="marginnote"><span id="ref-ProPub3" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[32] </span><span class="csl-right-inline">J. Larson, <span>“ProPublica analysis of data from broward county, fla.”</span> ProPublica, 2016.</span>
</span>
</span>. Their analysis found the distributions of risk scores for Black and White defendants to be markedly different, with White defendants being more likely to be scored low-risk - see Figure <a href="#fig:COMPAS" data-reference-type="ref" data-reference="fig:COMPAS">2.3</a>.</p>
<figure>
<img src="02_EthicalDevelopment/figures/Fig_Propublica.png" id="fig:COMPAS" style="width:85.0%" alt="Figure 2.3: Comparison of recidivism risk scores for White and Black defendants[32]" />
<figcaption aria-hidden="true">Figure 2.3: Comparison of recidivism risk scores for White and Black defendants<sup><span class="citation" data-cites="ProPub3"><a href="#ref-ProPub3" role="doc-biblioref">[32]</a></span></sup></figcaption>
</figure>
<p>Comparing predicted recidivism rates for over 7,000 of the defendants with the rate that actually occurred over a two-year period, they found the accuracy of the algorithm in predicting recidivism for Black and White defendants to be similar (59% for White and 63% for Black defendants), however the errors revealed a different pattern. They found that Blacks were almost twice as likely as Whites to be labelled as higher risk but not actually re-offend . The errors for White defendants were in the opposite direction; while being more likely to be labelled as low-risk, they more often went on to commit further crimes. See Table <a href="#tbl:COMPAS" data-reference-type="ref" data-reference="tbl:COMPAS">2.1</a>.</p>
<div id="tbl:COMPAS">
<table>
<caption>Table 2.1: COMPAS comparison of risk score errors for White versus Black defendants</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Error type</th>
<th style="text-align: right;">White</th>
<th style="text-align: right;">Black</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Labelled Higher Risk, But Didn’t Re-Offend</td>
<td style="text-align: right;">23.5%</td>
<td style="text-align: right;">44.9%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Labelled Lower Risk, But Did Re-Offend</td>
<td style="text-align: right;">47.7%</td>
<td style="text-align: right;">28.0%</td>
</tr>
</tbody>
</table>
</div>
<p>How might different use cases for the model affect the feedback cycle? Let’s consider some different use cases.</p>
<p>In the courts, the COMPAS recidivism risk score has been used by judges as an aid in determining sentence length - the higher the risk, the longer the sentence. Of course being incarcerated limits ones ability to reoffend but unless the sentence is life, release is inevitable. What impact does a longer sentence have on recidivism? Current research suggests that “The longer and harsher the prison sentence – in terms of less freedom, choice and opportunity for safe, meaningful relationships – the more likely that prisoners’ personalities will be changed in ways that make their reintegration difficult and that increase their risk of re-offending”<span class="citation" data-cites="BBCFPrison"><a href="#ref-BBCFPrison" role="doc-biblioref">[33]</a></span><span class="marginnote"><span id="ref-BBCFPrison" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[33] </span><span class="csl-right-inline">C. Jarrett, <span>“How prison changes people,”</span> <em>BBC Future</em>, May 2018.</span>
</span>
</span>. Now in addition to this consider that as a Black defendant, you are more likely to be incorrectly flagged as high risk. If there was no racial disparity in recidivism rates in the data, we could expect the imbalance in errors to create one. What about crime rates - how do longer sentences impact those? Research shows that it is the certainty, rather than severity of punishment that acts as a deterrent to crime<span class="citation" data-cites="Nagin"><a href="#ref-Nagin" role="doc-biblioref">[34]</a></span><span class="marginnote"><span id="ref-Nagin" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[34] </span><span class="csl-right-inline">D. S. Nagin, <span>“Deterrence in the twenty-first century: A review of the evidence,”</span> <em>Crime and Justice</em>, vol. 42, May 2018.</span>
</span>
</span>. Long-term sentences are particularly ineffective for drug crimes as drug sellers are easily replaced in the community<span class="citation" data-cites="TheSentProj"><a href="#ref-TheSentProj" role="doc-biblioref">[35]</a></span><span class="marginnote"><span id="ref-TheSentProj" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[35] </span><span class="csl-right-inline">M. Mauer, <span>“Long-term sentences: Time to reconsider the scale of punishment,”</span> <em>The Sentencing Project</em>, 2018.</span>
</span>
</span>. On balance, excessive incarceration has negative consequences for public safety because finite resources spent on prison are diverted from policing, drug treatment, preschool programs, or other interventions that might produce crime-reducing benefits.</p>
<div class="lookbox">
<p><strong>Reducing incarceration rates</strong></p>
<p>The US has the highest rate of incarceration in the world, at 0.7% of the population<span class="citation" data-cites="PrisPolInit"><a href="#ref-PrisPolInit" role="doc-biblioref">[36]</a></span><span class="marginnote"><span id="ref-PrisPolInit" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[36] </span><span class="csl-right-inline">P. Wagner and W. Sawyer, <span>“States of incarceration: The global context,”</span> <em>Prison Policy Initiative</em>, 2018.</span>
</span>
</span>. It’s higher than countries with authoritarian governments, those that have recently been locked in civil war and those with murder rates more than twice that in the US. Comparing with countries that have stable democratic governments, the incarceration rate in the US is more than 5 times that of its closest peer - the UK. The US spends $57 billion a year on housing more than 2.2 million people in prison<span class="citation" data-cites="BBCFLongSent"><a href="#ref-BBCFLongSent" role="doc-biblioref">[37]</a></span><span class="marginnote"><span id="ref-BBCFLongSent" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[37] </span><span class="csl-right-inline">B. Lufkin, <span>“The myth behind long prison sentences,”</span> <em>BBC Future</em>, May 2018.</span>
</span>
</span>, almost half of which are private companies that spend significant sums on lobbying the federal government for policies that would further increase incarceration. Some have advocated for the use of risk scores in sentencing in order to reduce the rate of incarceration, the idea being that if the risk scores are low then defendants can be spared prison time. What might the feedback effect be for this use case? What is the impact of the imbalance in error rates? What assumptions are you making to reach this conclusion?</p>
</div>
<p>Alternatively, suppose the software was used as a way to distribute limited rehabilitation resources, allocating them to those defendants that that were deemed to be at the highest risk of re-offending (and thus the most in need of intervention). Assuming the model to be accurate and that rehabilitation decreased the risk of reoffending, we can expect that using this model would serve to reduce existing disparities in recidivism rates between individuals. What about the imbalance in errors? Black defendants would more often erroneously be allocated rehabilitation resources and white defendants erroneously denied.</p>
<p>We have made numerous assumptions in our analysis of the feedback above; rehabilitation consistently reduces the risk of recidivism (regardless of the crime), that the relationship between sentence length and recidivism risk is monotonic and increasing. That two years is a long enough time horizon to consider. Without getting into the weeds, the point here is simply that the same model can have a very different feedback cycle if used in a different way. How a model is used is important and its <em>performance</em> cannot be evaluated in isolation from its use case. A question to ask is, does the action taken on the back of the model serve to push extremes to the centre, or push them further apart? The relationships you have to understand to answer the question, will depend on the specifics of the problem.</p>
</section>
</section>
<section id="sec_ResponseDev" class="level2" data-number="2.2">
<h2 data-number="2.2"><span class="header-section-number">2.2</span> Model Development and Deployment Life Cycle</h2>
<p>In this section we cover the more practical aspects of ethical model development and deployment. We take a take a higher level view of the process by which machine learning systems are created and identify the stages at which we can build in safety considerations. We take inspiration from model risk management in finance where models are ubiquitous. In banking, processes and policies with regard to development, testing, documentation, review, monitoring and reporting of model related valuation risk, have been developed over decades, alongside regulation. Many of the ideas we discuss in this chapter were developed and implemented after the 2008 credit crisis in an effort to improve controls around valuation model risk for derivative products (more on this later).</p>
<p>Before we think about identifying and categorising common causes of harm in machine learning applications, it will be helpful to outline the workflow through which machine learning models might be developed and deployed responsibly. Figure <a href="#fig:Workflow" data-reference-type="ref" data-reference="fig:Workflow">2.4</a> does exactly this.</p>
<figure class="fullwidth">
<img src="02_EthicalDevelopment/figures/Fig_Workflow.png" id="fig:Workflow" alt="Figure 2.4: Fairness aware machine learning system development, deployment and management workflow." />
<figcaption aria-hidden="true">Figure 2.4: Fairness aware machine learning system development, deployment and management workflow.</figcaption>
</figure>
<section id="model-governance-standards" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1"><span class="header-section-number">2.2.1</span> Model Governance Standards</h3>
<p>At the top, overarching the entire workflow, we have the model governance standards. These essentially outline the processes, roles and responsibilities that constitute the development, deployment and management of the machine learning system. It defines and documents a set of standards for the activities that constitute each stage of the depicted workflow. More on this later.</p>
</section>
<section id="problem-formulation" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2"><span class="header-section-number">2.2.2</span> Problem Formulation</h3>
<p>Below this, the life cycle of a machine learning system starts in top left corner with the formulation of the problem. This segment of the development process includes setting objectives, gathering data, analysing and processing it, determining a target variable, relevant features and metrics that indicate success (and failure) of the model (in training, evaluating and monitoring the deployed model). This process should include consulting with experts in the problem domain. The goal here is to understand the problem, data and impact of potential solutions for all the stakeholders. The arrows show that the problem formulation process is an iterative one where ideally domain experts, data collection and processing all inform each other in the creation of a tractable machine learning problem.</p>
<p>An assessment should be made with regards to how appropriate the data is for the model use case. Understanding the provenance of the data (who collected it, how it was collected and for what purpose) is important. Is it representative of the population the model built on it intends to serve? Exploratory data analysis (EDA) should include understanding if there is bias and or discrimination in the data. In particular understanding how is the target variable distributed for different subgroups of the population and what the nature of the resulting machine learning cycle might be for the intended and unintended use cases. Is there strong correlation between protected features and other variables?</p>
<p>Problem formulation should also consider the proposed materiality of the associated risk. What’s the worst that can happen? How might the model be misused of misinterpreted? Would a disclaimer (what this model doesn’t tell you...) be appropriate? How many individuals would be exposed to the model? Is the model within risk appetite (as defined in the model governance standards)? Having a way to understand and compare the risks posed by different models/applications is useful in ensuring the appropriate amount of resource and scrutiny is applied at all stages of the development, deployment and maintenance life cycle.</p>
</section>
<section id="model-development" class="level3" data-number="2.2.3">
<h3 data-number="2.2.3"><span class="header-section-number">2.2.3</span> Model Development</h3>
<p>Once the problem is well understood and represented in the data the next broad segment is developing a model. This includes splitting the data into training and testing sets, evaluating the model against its objectives and consequently refining the data, model, evaluation metrics or other aspects. The splitting of data may be more complex, depending on the cross validation approach, but for simplicity we omit specific details in Figure <a href="#fig:Workflow" data-reference-type="ref" data-reference="fig:Workflow">2.4</a>. Part of model development and validation process should be to understand the model’s limitations - where predictions might be unreliable, what it can and cannot be used for. The process of testing and analysing model output for performance should include analysis for discrimination and fairness. How are predictions and errors distributed for different subgroups of the population? How does the model output distribution differ from the training data? Again, model development is an iterative process and the data, metrics, training objectives, post-processing steps and more will evolve as the developers’ understanding of the problem improves.</p>
</section>
<section id="model-owners" class="level3" data-number="2.2.4">
<h3 data-number="2.2.4"><span class="header-section-number">2.2.4</span> Model Owners</h3>
<p>For applications deemed ready for deployment, the documentation for the data and model analysis and implementation is submitted to the model owners for review. So who are these model owners? There are often many people involved in the development and deployment of a machine learning system (one would hope, at least two in general) and the model governance standards should specify which of them plays what role in deciding when a solution is ready to be deployed. Each of the model owners will have different (potentially conflicting) concerns. Model owners represent the different stakeholders of the risk associated with the model and collectively they are accountable, though for potentially differing aspects of it. These might include for example,</p>
<ul>
<li><p><strong>Product owners</strong> that will use the system to make decisions.</p></li>
<li><p><strong>Domain experts</strong> that may have had input in the development of the solution (legal, domain or application specific council) and/or may be responsible for dealing with cases for which the model is deemed inappropriate (a radiologist for a pneumonia detector for example).</p></li>
<li><p><strong>Model developers</strong> that were involved in the construction of the model from collecting the data to building the model.</p></li>
<li><p><strong>Independent model validators</strong> that provide adversarial challenge around the modelling and implementation.</p></li>
<li><p><strong>Engineers</strong> that might be responsible for ensuring that infrastructure (for example, data collection, storage, post-deployment monitoring and reporting) requirements can be met.</p></li>
</ul>
</section>
<section id="approval-process" class="level3" data-number="2.2.5">
<h3 data-number="2.2.5"><span class="header-section-number">2.2.5</span> Approval Process</h3>
<p>Together model owners determine if the model is approved for deployment or not. For the sake of brevity, and to emphasize the right of the model owners to reject proposed solutions, we describe the situation where the model is not approved, as it being rejected. In reality, rejecting a model need not mean that it is scrapped. Model owners may for example require further analysis or other changes to be made before it is resubmitted for approval. In any organisation, ideally the values, mission and objectives are well enough understood by the members, that a solution being scrapped at the last hurdle would be a rare event. The kinds of issues that would result in rejection should generally be caught at an earlier stage of the model development workflow. Model owners will also be responsible for monitoring the model post-deployment, periodic re-review of the risks and failure postmortems that determine what changes are required when issues arise, including amendments to the model governance standards themselves. The model governance standards might be interpreted as a contract between the model owners that describes their commitments, individually and collectively in managing the risk.</p>
</section>
<section id="management-of-deployed-models" class="level3" data-number="2.2.6">
<h3 data-number="2.2.6"><span class="header-section-number">2.2.6</span> Management of Deployed Models</h3>
<p>Ensuring the necessary reporting mechanisms are in place so the decision system can be monitored both for validity and exposure, should be a pre-deployment requirement. This kind of risk tracking can be used as a control, if say limits can be defined which reflect risk appetite. Limits might be set based on how well understood the risks associated with a product (the longer a model is monitored, the more information we have about it) are and what mitigation strategies might be in place, for example.</p>
<p>Importantly the post-deployment cycle of Figure <a href="#fig:Workflow" data-reference-type="ref" data-reference="fig:Workflow">2.4</a> (like the machine learning cycle in Figure <a href="#fig:MLCycle" data-reference-type="ref" data-reference="fig:MLCycle">2.1</a>, at the start of the chapter) includes separate nodes for the model predictions and actions taken. Selbst et al.<span class="citation" data-cites="FairAbs"><a href="#ref-FairAbs" role="doc-biblioref">[38]</a></span><span class="marginnote"><span id="ref-FairAbs" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[38] </span><span class="csl-right-inline">A. D. Selbst, D. Boyd, S. A. Friedler, S. Venkatasubramanian, and J. Vertesi, <span>“Fairness and abstraction in sociotechnical systems,”</span> in <em>Proceedings of the conference on fairness, accountability, and transparency</em>, 2019, pp. 59–68. doi: <a href="https://doi.org/10.1145/3287560.3287598">10.1145/3287560.3287598</a>.</span>
</span>
</span>, describe five traps that one might fall into, even while attempting to create fair machine learning applications. In particular, they describe the <em>framing trap</em>, in which one might unwittingly ensure that an algorithm meets some narrow fairness criterion on outcomes or errors (over the <em>algorithmic frame</em>) but fail to consider its impact in the real world. For example, failing to be sufficiently transparent about the weaknesses of it which leads to it erroneously being prioritised over the judgement of human experts. Or we might fail to consider the longer term impacts on the sociopolitical landscape (over the <em>sociotechnical frame</em>) in determining something as complicated as fairness. If the actions taken off the back of the predictions include human judgement or interpretation, this should also be captured as part of monitoring the model. Are people using the model in ways that were not anticipated or is it having an adverse affect in some other way? Finally we include human experts in the loop again at the stage where predictions are acted upon. Human experts might for example be consulted in cases where the model is understood to produce less reliable predictions, or via an appeals process that is built into the decision system.</p>
<p>Processes and procedures for managing remedial work in the event of failures could be specified as part of the model governance standards. One of the issues with machine learning solutions is that when there are failures (say, a photo or sentence is labelled in an offensive way), the easiest response is an ad hoc rule based approach to ‘fixing’ the specific issue that occurred - the “if this, then do something else” solution, so to speak. But this kind of action isn’t sufficient to address the root of the problem. Remedial work will typically require more resource and planning to fix. A failure should prompt a re-review. Having a more robust process around dealing with failures when they occur, should mean that not only is action is taken in a timely manner, but also that meaningful changes are made as a result of them and that work is appropriately prioritised.</p>
<p>Failure post-mortems that focus on understanding the weaknesses of the model governance process (not the failure of individuals) could also be a means for improving them. Once in production, periodic re-reviews of the model are a means to catch risks that may have been missed the first time around. The frequency of re-reviews can depend on the risk level of the model/application in question if these are being tracked.</p>
</section>
<section id="measuring-fairness" class="level3" data-number="2.2.7">
<h3 data-number="2.2.7"><span class="header-section-number">2.2.7</span> Measuring Fairness</h3>
<p>Bias and fairness metrics are essentially calculated on data. There are two stages at which we’ll be interested in measuring bias and or fairness in evaluating our machine learning system. The relevant nodes are coloured red in Figure <a href="#fig:Workflow" data-reference-type="ref" data-reference="fig:Workflow">2.4</a>.</p>
<ol>
<li><p><strong>Model input:</strong> The training data, during the <em>data evaluation</em> stage.</p></li>
<li><p><strong>Model output:</strong> The predictions produced by our model, that is the <em>model evaluation</em> stage.</p></li>
</ol>
<p>Our chosen fairness evaluation metrics calculated on the training data and model output will in general not be the same. By comparing the two, we can evaluate how well the model is replicating relationships in the data.</p>
</section>
<section id="bias-mitigation-techniques" class="level3" data-number="2.2.8">
<h3 data-number="2.2.8"><span class="header-section-number">2.2.8</span> Bias Mitigation Techniques</h3>
<p>There are three stages at which one can intervene in the development of machine learning model mapping to mitigate bias and they are categorised accordingly. Relevant nodes coloured green in Figure <a href="#fig:Workflow" data-reference-type="ref" data-reference="fig:Workflow">2.4</a>.</p>
<ol>
<li><p><strong>Pre-processing</strong> techniques modify the historical data on which the model is trained (at the <em>data pre-process</em> stage).</p></li>
<li><p><strong>In-processing</strong> techniques alter the training process or objective (at the <em>model training</em> stage).</p></li>
<li><p><strong>Post-processing</strong> techniques take a trained model/s and modify or combine the output (at the <em>model post-process</em> stage).</p></li>
</ol>
</section>
</section>
<section id="sec_ProcessPolicy" class="level2" data-number="2.3">
<h2 data-number="2.3"><span class="header-section-number">2.3</span> Responsible Model Development and Deployment</h2>
<p>In this section we examine a fairness aware development, deployment and management policies for a sociotechnical system. For the most part, the ideas are similar to those concerned with effective model risk management; one that acknowledges that models are fallible and accordingly sets standards for development, deployment, monitoring and maintenance. The intention being, to prevent foreseeable failures and mitigate the associated risks. The main difference is that we consider ethical risk as a central component of the risks that must be managed. Of course predictive performance is an important consideration in being fair (it’s hard to imagine a model that is no better than guessing in making material decisions for people, as being fair) but predictive performance does not guarantee fairness. Viewing model evaluation through an ethical lens requires a more holistic assessment of the system, it’s purpose, reliability and impact; not just for the business, but for all those exposed to or affected by it - society at large.</p>
<p>We’ll address some of the problems that can’t be solved through the kinds of model mapping interventions we’ll talk about in this book. Another fair machine learning trap described by Selbst et al.<span class="citation" data-cites="FairAbs"><a href="#ref-FairAbs" role="doc-biblioref">[38]</a></span> is the <em>formalism trap</em>, in which one fails to account for the full meaning of complex social concepts, such as fairness, which can’t be formalised with mathematical equations. In chapter <a href="#ch_GroupFairness" data-reference-type="ref" data-reference="ch_GroupFairness">3</a> we’ll show that under such formalisms, a universally fair classifier is precluded by irreconcilable definitions. Fairness might more naturally be established <em>procedurally</em> (as often it is in law). Furthermore, social concepts are deeply <em>contextual</em>, and thus do not lend themselves well to abstraction (a core principal in mathematics which enables portability of solutions). Social concepts evolve over time, as cultural norms shift, therefore <em>contestability</em> is key, as it provides an avenue for change and challenge. These are qualities of a system rather than an equation and cannot be resolved through algorithmic interventions. They require people to do the right thing, and for organisations to define what they consider the right thing to be.</p>
<section id="policy" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1"><span class="header-section-number">2.3.1</span> Policy</h3>
<p>In industry, where innovation demands taking risks and time is money, how do we ensure the proper amount of care and attention is applied when creating products that have the potential for harm? Historically, the answer has been to impose rules that slow the process down, by requiring steps which prioritise safety over other concerns. In order to do this, one must first determine and define a safety standard. In Figure <a href="#fig:Workflow" data-reference-type="ref" data-reference="fig:Workflow">2.4</a>, overarching the whole process is a set of model governance standards. These essentially define that standard. They describe the process through which systems are developed and approved for deployment, and the standard to which systems are tested and evaluated.</p>
<p>In the financial sector, major banks (that are considered to be of systemic importance to a nations financial stability) are subjected to greater scrutiny by the central bank and regulators. An example of this might be requiring them to publish results of <a href="https://www.bankofengland.co.uk/stress-testing/2021/key-elements-of-the-2021-stress-test">solvency stress tests</a>. The currency might be social rather than financial for sociotechnical systems but the principal should be the same.</p>
<section id="prioritisation" class="level4 unnumbered">
<h4 class="unnumbered">Prioritisation</h4>
<p>Products which are of systemic importance to the sociopolitical landscape should have sufficient and appropriate resources (relative to those of the risk generating activities) to manage and mitigate their ethical risk. For applications that carry high risk of harm, risk functions should act as gatekeepers for model deployment and use.</p>
</section>
<section id="model-governance-standards-1" class="level4 unnumbered">
<h4 class="unnumbered">Model governance standards</h4>
<p>Though relatively new terminology in machine learning circles, the concept of model governance has existed for decades. For large financial institutions (which depend on vast numbers of proprietary models), operating and maintaining a model governance framework is a central part of model risk management and a regulatory requirement. The regulatory landscape of the financial sector is considerably more mature than that of other industries and the frameworks used to handle the associated risks have been developed and refined over time. It is therefore instructive to look at how such institutions manage their model risk and consider how these might be applied to sociotechnical systems.</p>
<p>So what does responsible and ethical machine learning development and deployment look like? In reality there is no one size fits all answer. As we’ve noted before, sociotechnical systems are context dependent. The answer can depend on a whole multitude of factors.</p>
<ul>
<li><p><strong>Domain:</strong> Different domains will have different legal and ethical concerns for example employment versus say social media.</p></li>
<li><p><strong>The number and complexity of the models being used by the business:</strong> A large organisation that uses or tests hundreds of models and composes them to make decisions and create new products (such as Microsoft) would benefit greatly from infrastructure and methodologies for measuring the materiality of the associated risks that would enable prioritisation of work related to mitigating them. In contrast, for a business based on a single model that automates a specific task (such as tagging images), this would be less of a concern.</p></li>
<li><p><strong>Cost of errors:</strong> Where the stakes are high, for example self driving cars, pre-deployment testing will need to be extensive and prescribed in order to reduce the probability of making mistakes. Well defined and mandatory processes will play an important role - checklists, contingency planning, detailed logging for postmortems and more. For these types of applications we would want authority over model use to be distributed to risk functions which determine when the product is approved for deployment and have the power to decommission them. For a wake word detector (think "Hey Siri", "Okay Google" and "Alexa") a lower standard would be accepted by most.</p></li>
</ul>
<p>Given this, how does one approach the problem of responsible development? Step zero is to create a set of model governance standards, the purpose of which is to clearly define and communicate what responsible model development and deployment looks like for your specific application, use case, domain, business, principles and values.</p>
<p>What are the kinds of questions we might want our model governance standards to answer?</p>
<ul>
<li><p>Why is the work important? What kinds of events or uses of your models are you trying to avoid (or are outside of the organisation’s risk appetite)? What legislation is the company subject to? What are the consequences of failures? What are the values of the company that you want to protect?</p></li>
<li><p>Who is responsible? What are the roles that must be fulfilled to deploy monitor and manage the risks. Who are the stakeholders or model owners and what is their remit? Who is accountable?</p></li>
<li><p>What are model owners responsible for? What technology is covered by the standard. What kind of expertise are required to be able to report, understand and manage the risks? What are the questions each stakeholder must answer? What are the responsibilities of those experts at the various stages of the model development and deployment life cycle? What authority do they have in relation to determining if the model is fit for deployment? Who decides what?</p></li>
<li><p>How do you manage the risk? What are the rules, processes and requirements that ensure the companies values are maintained, people are treated fairly, the legal requirements are fulfilled and risks are appropriately managed? How do the stakeholders work together? For example some roles might need to be independent while others work alongside one another. What are the requirements around training data (documentation, review, storage, privacy, consent and such)? What are the requirements around modelling (documentation, testing, monitoring and such)? What are the processes around proposing, reviewing, testing, deploying, monitoring model related risks? For example, frequency of risk reviews, forums for discussion and monitoring. What are the processes and requirements in place for (specific foreseeable types of) failures? Are there stakeholder specific templates or check-lists that ensure particular questions get answered at specific points in the model development and deployment life cycle?</p></li>
</ul>
<p>The list of questions above is by no means exhaustive but a good starting point. Creating a set of model governance standards is about planning. Machine learning systems can be complicated and have many points of failure: problem formulation, data collection, data processing, modelling, implementation, interpretation. The only way to reduce the risk of failures is to be organised, deliberate and plan for them. Creating a set of standards does exactly that. Where the systems we build have real world consequences, the preparation, planning and process around development, review, analysis, deployment and monitoring of them should reflect that. Ensuring that the right questions get asked at the right time, knowing who is responsible for answering them and being prepared to address problems is a core part of developing and deploying models ethically.</p>
<p>Finally, we note that the benefits of having excellent model governance standards with well defined goals, processes, roles and responsibilities won’t be realised if in practice they are not followed. In large organisations, consistency can be a challenge. The role of internal audit is to provide objective feedback on the risks, systems, processes and compliance at an executive level. From a model governance perspective the role of auditors is to ensure that there are good processes in place and that the processes are being followed. Internal audit’s role is independent of the business up to the executive level. All functions within the business are required to cooperate with internal auditors and provide unfettered access to information requested. Internal audit does not contribute to the improvement of or compliance to processes directly. Their role is to , assess and report back to senior leadership. In a risk management context, internal audit are considered to be the <em>third line of defence</em>. We shall come to the first and second lines shortly.</p>
</section>
<section id="risk-assessment" class="level4 unnumbered">
<h4 class="unnumbered">Risk Assessment</h4>
<p>In order to manage risk it must be identified. Any algorithm, no matter how simple, carries the risk of implementation errors or bugs and thus should at the very least be subject to unit testing and independent code review before being deployed. For organisations with more complicated risk profiles, an important component of managing risk is having a system to measure and track it. Having a way to compare risk level across products and or product classes, even if comparisons are coarse, enables some degree of risk appropriate prioritisation and resource allocation in managing them. Risk can be estimated in many different ways and exactly how it is measured will depend on the details of the application. Broadly speaking it should consider both the severity of the event and likelihood. What’s important is not the exact value but rather the ability to compare risks across products, applications or indeed any other lines along which a business is organised. Metrics that capture things like the scale on which the model is being used, predictive performance, training data quality/representativeness, model complexity, potential for harm and more could potentially be used to coarsely judge the risk posed by different applications. Model governance standards can define risk bands or metrics if they are application specific enough.</p>
</section>
</section>
<section id="risk-controls" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2"><span class="header-section-number">2.3.2</span> Risk Controls</h3>
<p>In this section we return to the workflow and see how the policies, discussed above, feed into the development, deployment and management of a decisions system. Problem formulation is the first key step in developing a machine learning solution and an especially pivotal one in ethical risk assessment. The problem formulation stage plays perhaps the largest role in determining what the end product will actually be. It is the stage at which the model objectives, requirements, target variable and training data are determined.</p>
<section id="deployment-bias" class="level4 unnumbered">
<h4 class="unnumbered">Deployment Bias</h4>
<p>As part of problem formulation one should examine the machine learning cycle in the context of the biases in the data and consider the nature (direction and strength) of the feedback of resulting actions on future data. It’s important to consider other ways in which the model might be used (other than that intended) and understand the feedback cycle in those cases. How the model might be misused/misinterpreted? Are there ways in which it should not be used? Documenting these types of considerations is an essential step in preventing deployment bias; that is, systematic errors resulting through inappropriate model use or misinterpretation of model results. As creators of technologies which affect society at large, documenting our work might be interpreted as a civic duty. We consider documentation to be an essential part of a dataset and model without which it is incomplete and potentially harmful. As such we classify lack of documentation as a model issue.</p>
<p>Repurposing data of models is a risky thing to do and is often the source of bias in models. A good example of this was uncovered by researchers from Berkeley in 2019. They discovered racial bias in an algorithm used to make important health-care determinations for millions of Americans <span class="citation" data-cites="Obermeyer"><a href="#ref-Obermeyer" role="doc-biblioref">[39]</a></span><span class="marginnote"><span id="ref-Obermeyer" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[39] </span><span class="csl-right-inline">Z. Obermeyer, B. Powers, C. Vogeli, and S. Mullainathan, <span>“Dissecting racial bias in an algorithm used to manage the health of populations,”</span> <em>Science</em>, vol. 366, pp. 447–453, Oct. 2019, doi: <a href="https://doi.org/10.1126/science.aax2342">10.1126/science.aax2342</a>.</span>
</span>
</span>. The algorithm was being used to identify patients that would benefit from high-risk care management programs, which improve patient outcomes and reduce healthcare costs for patients with complex healthcare needs. The researchers found that Black patients who had the same risk scores as White patients were far less healthier and thus less likely to be selected for the programs. The bias was the result of data documenting healthcare costs being used to predict healthcare needs.</p>
<p>A thorough examination of ethical issues demands consideration of a diversity of voices, which is well known to be lacking in technology. This is the stage at which it is important to consider who is affected by the technology, consult with them and ensure their views are incorporated in the understanding of the problem and design of a potential solution. Who are the human experts? People who would have valuable insight and opinions on the potential impact of the model you plan on building? Who does the model advantage and who does it disadvantage? Want to use machine learning to help manage diabetes? What are the interests of the health insurance company funding the development? Have you consulted with diabetics in addition to specialist physicians? What are their concerns? What is the problem from the different perspectives? Would a model be able to help or are there simpler solutions?</p>
</section>
<section id="independent-model-validation" class="level4 unnumbered">
<h4 class="unnumbered">Independent Model Validation</h4>
<p>In any system that is vulnerable to costly errors, unit testing and pre-deployment independent review is a well established method of preventing costly foreseeable failures. Whether it’s a completely new solution built from scratch or a modification to an existing solution that’s being deployed, an independent review process is an important element of responsible model development. Below we describe the responsibilities of two separate roles, the model developers and the model validators.</p>
<p>The model developers role is to translate the business problem into a tractable machine learning problem and create a solution. They will work with the business and receive input from other necessary domain experts relevant to the application to develop a possible solution. This will include tasks such as acquiring and interpreting data that is relevant for the problem, determining a target variable, model objectives, performance measures, fairness measures and more. In terms of preventing failures, model developers are considered the <em>first line of defence</em>. The responsibility of developing a model responsibly lies, in the first instance, with them. The model developers should aim to create a model they believe to be production ready and more specifically, fulfils the requirements specified in the model governance standards.</p>
<p>As part of the pre-deployment process, the model should be reviewed. Model validators will have a similar skill set to model developers but their goal is different to that of the model developers. Where the developers primary objective is to create a solution to the business problem that meets a standard which will be approved by model owners, the role of a model validator is to critique that solution and expose problems with it - the more the better. Their role is to adversarially challenge the solution. They might challenge performance claims (error, bias, fairness) by changing the data or metrics, or demonstrate problems with the model by comparing with an alternative solution. The goal is to expose model weaknesses and demonstrate the limits of its validity in testing and documentation. The model validator might devise mitigation strategies for identified risks. Such strategies might include setting model usage limits (that might trigger a re-review for example) or additional monitoring requirements. They might for example identify additional cases when human review might be required or reject the proposed solution entirely if the problems with the model are great enough. The role of the reviewer could be thought of as something akin to a hacker but with the advantage of having the keys in the form of model documentation (provided by the developers). The model reviewer in pre-deployment can act as a gatekeeper.</p>
<p>Note that in our terminology, the model is simply a mapping. It need not be learned by calibration to historic data. Any algorithm where the decision being made is important enough should be treated as such and proper precautions should be taken. For an algorithm which will be used in production, no matter how simple, this should mean being subject to code review and unit testing that demonstrates its validity in some well chosen cases. A good example of where this would have been valuable came up in December 2020 when a bug in an algorithm, meant that <a href="https://www.npr.org/sections/coronavirus-live-updates/2020/12/18/948176807/stanford-apologizes-after-vaccine-allocation-leaves-out-nearly-all-medical-resid">Stanford Hospital Residents were not correctly prioritised for the COVID-19 vaccine</a>, despite working with COVID-19 patients daily. The algorithm did not apparently account for the fact that Resident doctors had a blank ‘location’ field in the data. We might never know the details of how it was implemented and tested but it hard to imagine such a bungle passed any decent unit test.</p>
<p>The model review process acts as the <em>second line of defence</em>. To be effective, the model reviewer’s role must be independent of the model developer’s to some extent. What does independence mean? We mentioned the distinct goals of their roles and this is important. The validator should not drive the development of a solution approach or model but instead focus on critique. In reality, it’s easy to see that the iterative nature of model development might mean that amendments addressing criticisms of the solution may get rolled into it’s development at multiple stages, blurring the lines between critique and collaboration. From an efficiency perspective, it might make sense for the solution to be reviewed at several critical stages of the development process making the overall process indeed more collaborative. If there’s a problem with the data that was missed, ideally the developer would want to fix it before going on to build and train a model on it. One of the challenges then is how to preserve independence between the roles, and ensure that the value of having adversarial criticism in preventing failures, is not lost in collaboration. How best to preserve independence will depend on the specifics and is something that should be determined within the model governance standards. In a bank, the model developers and validators are required (by the regulator) to serve under different business functions (the trading desk versus risk management). They have different reporting lines up to executive level, and work in physically separate locations.</p>
</section>
<section id="monitoring" class="level4 unnumbered">
<h4 class="unnumbered">Monitoring</h4>
<p>Post-deployment monitoring is an important part of responsible model development and deployment. Analysis should not stop once the model is deployed. Decisions on what to monitor and necessary feedback mechanisms should be determined during development. It’s important to understand if the model is performing in line with expectations (based on pre-deployment testing and analysis). Is the data coming out of the model more or less biased than the data going in? Distributional shifts should be of particular concern where the actions taken based on predictions have a strong impact on the composition of future data.</p>
</section>
<section id="domain-expertise" class="level4 unnumbered">
<h4 class="unnumbered">Domain Expertise</h4>
<p>In section <a href="#sec_SimpsParadox" data-reference-type="ref" data-reference="sec_SimpsParadox">1.4</a> we spoke of the importance of domain knowledge in interpreting causal relationships in data. Consulting domain experts at the problem formulation stage can yield considerable ethical risk reducing benefits. Incorporating more diverse perspectives on a problem will surely result in a better design that will benefit a broader cross-section of society. Given that models are simplified representations of real world systems and we know that they will make errors, responsible development should build in processes for anticipating and dealing with such cases and, where appropriate, deferring to the judgement of a human expert.</p>
</section>
</section>
</section>
<section id="common-causes-of-harm" class="level2" data-number="2.4">
<h2 data-number="2.4"><span class="header-section-number">2.4</span> Common Causes of Harm</h2>
<p>There are many ways in which machine learning solutions can result in harm. In this section we present a taxonomy of common causes and provide examples. At the end of the section, we’ll relate the causes in our taxonomy to the corresponding stages of the model development and deployment life cycle (discussed earlier), indicating where consideration and intervention could prevent them from arising. The goal is for this to serve as a good starting point as a practical reference for developing fairer models. For practising data scientists it could be helpful as a standard to compare our current practices against, avoid common pitfalls and hopefully help ensure we perform an appropriate level of due diligence before releasing our work. In our taxonomy, we aim to layout both the points at which issues arise and the various points at which one could assess and intervene. For this reason, the table may appear to contain duplications of the same problem viewed from different perspectives. This is intentional. Often different parts of an application are developed independently.<span class="sidenote-wrapper"><label for="sn-4" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-4" class="margin-toggle"/><span class="sidenote">It’s not uncommon for example (thanks to unprecedented growth in data markets), for a model to be built by one organisation, based on data collected by another.<br />
<br />
</span></span> Taking this approach is beneficial since it provides multiple opportunities to see and remedy the same problems.</p>
<p>Before presenting this taxonomy, it’s worth being clear that, in reality, there is no agreed upon terminology that describes the different types of issues that can arise or agreed upon framework for developing machine learning solutions that factor in ethical safety concerns (since regulation surrounding algorithmic decision systems is still in the process of being shaped). Indeed, developing one is the subject of recent research, <span class="citation" data-cites="ConsClfn"><a href="#ref-ConsClfn" role="doc-biblioref">[40]</a></span><span class="marginnote"><span id="ref-ConsClfn" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[40] </span><span class="csl-right-inline">B. d’Alessandro, C. O’Neil, and T. LaGatta, <span>“Conscientious classification: A data scientist’s guide to discrimination-aware classification,”</span> <em>Big Data</em>, vol. 5, no. 2, pp. 120–134, 2017, doi: <a href="https://doi.org/10.1089/big.2016.0048">10.1089/big.2016.0048</a>.</span>
</span>
</span>, <span class="citation" data-cites="BiasFramework"><a href="#ref-BiasFramework" role="doc-biblioref">[41]</a></span><span class="marginnote"><span id="ref-BiasFramework" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[41] </span><span class="csl-right-inline">H. Suresh and J. Guttag, <span>“A framework for understanding sources of harm throughout the machine learning life cycle,”</span> 2021.</span>
</span>
</span>, <span class="citation" data-cites="DS4DS"><a href="#ref-DS4DS" role="doc-biblioref">[42]</a></span><span class="marginnote"><span id="ref-DS4DS" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[42] </span><span class="csl-right-inline">T. Gebru <em>et al.</em>, <span>“Datasheets for datasets.”</span> 2020.Available: <a href="https://arxiv.org/abs/1803.09010">https://arxiv.org/abs/1803.09010</a></span>
</span>
</span>, <span class="citation" data-cites="MC4MR"><a href="#ref-MC4MR" role="doc-biblioref">[43]</a></span><span class="marginnote"><span id="ref-MC4MR" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[43] </span><span class="csl-right-inline">M. Mitchell <em>et al.</em>, <span>“Model cards for model reporting,”</span> <em>Proceedings of the Conference on Fairness, Accountability, and Transparency</em>, 2019, doi: <a href="https://doi.org/10.1145/3287560.3287596">10.1145/3287560.3287596</a>.</span>
</span>
</span>. The word bias itself has many definitions and even in a given context can have multiple valid interpretations. Different practitioners would likely describe the same type of bias differently. Causes of bias in machine learning applications are often numerous and overlapping, thus difficult to attribute to a single source or prescribe a single solution for. The most appropriate remedy itself will be very much context dependent and different practitioners will choose different approaches.</p>
<p>In creating this taxonomy, we take inspiration from that described by d’Alessandro et. al.<span class="citation" data-cites="ConsClfn"><a href="#ref-ConsClfn" role="doc-biblioref">[40]</a></span>, in which the <em>model</em> or algorithm (function mapping <span class="math inline">\(f\)</span> from features <span class="math inline">\((\boldsymbol{X}, \boldsymbol{Z})\)</span> to predictions <span class="math inline">\(\hat{Y}\)</span>), is distinguished from the larger <em>system</em> (people, infrastructure, processes, policies and risk controls) through which it is developed, deployed and managed. Evidence based medicine provides a rich terminology for different mechanisms through which systematic errors can be introduced in data and has perhaps the most comprehensive set of definitions and classification of bias types. This in itself can provide an important reference in determining which kinds of biases model developers should be aware of and we include some of them here. Table <a href="#tbl:Taxonomy" data-reference-type="ref" data-reference="tbl:Taxonomy">2.2</a> summarises our taxonomy of common causes of harm in machine learning systems.</p>
<div id="tbl:Taxonomy">
<table>
<caption>Table 2.2: Taxonomy of common causes of harm in machine learning systems.</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Element</th>
<th style="text-align: left;">Failure</th>
<th style="text-align: left;">Issue Type</th>
<th style="text-align: left;">Issue Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td rowspan="11" style="text-align: left;">System</td>
<td rowspan="4" style="text-align: left;">Policy</td>
<td rowspan="2" style="text-align: left;">Prioritisation</td>
<td style="text-align: left;">Failure to allocate appropriate/sufficient resource</td>
</tr>
<tr class="even">
<td style="text-align: left;">Failure to distribute power to manage conflicts of interest</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Governance</td>
<td style="text-align: left;">Failure to set or comply with application specific standards</td>
</tr>
<tr class="even">
<td style="text-align: left;">Risk assessment</td>
<td style="text-align: left;">Failure to identify and manage model related risk</td>
</tr>
<tr class="odd">
<td rowspan="7" style="text-align: left;">Controls</td>
<td style="text-align: left;">Deployment bias</td>
<td style="text-align: left;">Inappropriate model use / misinterpretation of model results</td>
</tr>
<tr class="even">
<td rowspan="3" style="text-align: left;">Independent model validation</td>
<td style="text-align: left;">Data appropriateness and preparation</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Modelling approach and implementation</td>
</tr>
<tr class="even">
<td style="text-align: left;">Model evaluation metrics (pre and post deployment)</td>
</tr>
<tr class="odd">
<td rowspan="2" style="text-align: left;">Monitoring</td>
<td style="text-align: left;">Poor monitoring of model validity and impact</td>
</tr>
<tr class="even">
<td style="text-align: left;">Poor monitoring of risk exposure</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Domain expertise</td>
<td style="text-align: left;">Non deference to human domain expert</td>
</tr>
<tr class="even">
<td rowspan="18" style="text-align: left;">Model</td>
<td rowspan="7" style="text-align: left;">Data</td>
<td style="text-align: left;">Historical bias</td>
<td style="text-align: left;">Data records wrongful discrimination</td>
</tr>
<tr class="odd">
<td rowspan="3" style="text-align: left;">Measurement bias</td>
<td style="text-align: left;">Quality of data varies across protected classes</td>
</tr>
<tr class="even">
<td style="text-align: left;">Measurement process varies across protected classes</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Recording proxies for immeasurable / ill defined variables</td>
</tr>
<tr class="even">
<td style="text-align: left;">Representation bias</td>
<td style="text-align: left;">Data not representative of target population</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Low support</td>
<td style="text-align: left;">Insufficient data for minority classes</td>
</tr>
<tr class="even">
<td style="text-align: left;">Documentation</td>
<td style="text-align: left;">Failure to adequately document</td>
</tr>
<tr class="odd">
<td rowspan="11" style="text-align: left;">Misspecification</td>
<td style="text-align: left;">Aggregation bias</td>
<td style="text-align: left;">Failure to model differences of type</td>
</tr>
<tr class="even">
<td rowspan="3" style="text-align: left;">Target variable</td>
<td style="text-align: left;">Target variable subjectivity</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Proxy target variable learning</td>
</tr>
<tr class="even">
<td style="text-align: left;">Heterogeneous target variable</td>
</tr>
<tr class="odd">
<td rowspan="2" style="text-align: left;">Features</td>
<td style="text-align: left;">Inclusion of protected features without control variables</td>
</tr>
<tr class="even">
<td style="text-align: left;">Inclusion of protected feature proxies (redlining)</td>
</tr>
<tr class="odd">
<td rowspan="2" style="text-align: left;">Cost function</td>
<td style="text-align: left;">Failure to specify asymmetric error costs</td>
</tr>
<tr class="even">
<td style="text-align: left;">Omitted discrimination penalties</td>
</tr>
<tr class="odd">
<td rowspan="2" style="text-align: left;">Evaluation bias</td>
<td style="text-align: left;">Poor choice of evaluation metrics</td>
</tr>
<tr class="even">
<td style="text-align: left;">Test data not representative of the target</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Documentation</td>
<td style="text-align: left;">Failure to adequately document</td>
</tr>
</tbody>
</table>
</div>
<p>In section <a href="#sec_ProcessPolicy" data-reference-type="ref" data-reference="sec_ProcessPolicy">2.3</a> we discussed a framework for responsible development and deployment of models. We summarise important elements of that discussion under <strong>system issues</strong> in our taxonomy of harms. The idea is that if having a process in place could avoid certain types of harms, then not having them is a failure of the system surrounding the model. In this section we discuss common causes of discrimination that relate directly to the model. We categorise these as originating from failures related to one of two sources:</p>
<ol>
<li><p><strong>Data issues</strong> refer to harms that arises as a direct result of issues with the data</p></li>
<li><p><strong>Misspecification</strong> refers to harms that arise through misspecification of the underlying problem in the modelling of it.</p></li>
</ol>
<p>The latter is an extension of the notion of model misspecification in statistics where the functional form of a model does not adequately reflect observed behaviour.</p>
<p>Before discussing our taxonomy for modelling issues, we address a point of contention in the machine learning community - that models are not biased, bias comes from data. The notion that bias is simply an artifact of data rather than a model is not uncommon among machine learning scholars and practitioners. In this book we’ve already discussed numerous examples of biased machine learning models, so where does this idea come from? In more theoretical disciplines a model is interpreted as being the parametric form. Under this definition of a model, different values of the parameters then don’t change what we consider to be our model. For example, the term <em>linear model</em> describes a family of models. More practical disciplines view a model as a function mapping - provided with input, the model returns output. By this definition of a model, if the parameters change, so does the function and thus the model. From a practical perspective then it’s clear that a model can discriminate since if the data documents historic discrimination, we would expect the trained model to reproduce it.</p>
<p>The idea that bias is a data problem, rather than a modelling one is at best a gross oversimplification of the problem and at worst misleading. It implies that in general, after training, a model will perfectly reproduce the joint distribution of the variables in data. Anyone who’s ever trained a model on real world data knows, is patently false. It suggests that models and data are independent when, in practice, they ought not be. Model development is an iterative process. The modelling choices we make can depend on the data and our model results should in turn influence our training data. Treating data and modelling as independent entities diminishes the responsibility of model developers in addressing the problem of biased and unfair applications. It ignores the very practical nature of developing models and the societal impact they can have. For sociotechnical systems, the objectives must surely extend beyond utility. We consider defining those wider objectives and incorporating them part of the modelling process and thus failing to consider them a modelling problem.</p>
<section id="data-issues" class="level3" data-number="2.4.1">
<h3 data-number="2.4.1"><span class="header-section-number">2.4.1</span> Data Issues</h3>
<p>When it comes to bias, data driven medicine provides a rich vocabulary for the different types. We mention three here.</p>
<section id="historical-bias" class="level4 unnumbered">
<h4 class="unnumbered">Historical Bias</h4>
<p>Historical bias arises as a result of differences between accepted societal values and cultural norms and those captured by data. These need not be a result of errors in the data. Even if data perfectly represents some world state, it can still capture a reality which society deems unfair. Training a model on such data will naturally lead to similarly unfair predictions. Historical bias can manifest itself in data in numerous ways, through unfair outcomes recorded in the data, differing data quality across groups and under or over-representation of groups to name just a few. Take medical data where racial and gender disparities in diagnosis and treatment are well publicised as the <em>health gap</em>. There is a growing body of research across the US and Europe that exposes systematic under-treatment and misdiagnosis of pain in women (<span class="citation" data-cites="CalderoneSexRoles"><a href="#ref-CalderoneSexRoles" role="doc-biblioref">[44]</a></span><span class="marginnote"><span id="ref-CalderoneSexRoles" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[44] </span><span class="csl-right-inline">K. L. Calderone, <span>“The influence of gender on the frequency of pain and sedative medication administered to postoperative patients,”</span> <em>Sex Roles</em>, vol. 23, pp. 713–725, 1990, doi: <a href="https://doi.org/10.1007/BF00289259">https://doi.org/10.1007/BF00289259</a>.</span>
</span>
</span>, <span class="citation" data-cites="SexAnalgesic"><a href="#ref-SexAnalgesic" role="doc-biblioref">[45]</a></span><span class="marginnote"><span id="ref-SexAnalgesic" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[45] </span><span class="csl-right-inline">E. H. C. MD <em>et al.</em>, <span>“Gender disparity in analgesic treatment of emergency department patients with acute abdominal pain,”</span> <em>Academic Emergency Medicine</em>, vol. 15, pp. 414–418, May 2008, doi: <a href="https://doi.org/10.1111/j.1553-2712.2008.00100.x">https://doi.org/10.1111/j.1553-2712.2008.00100.x</a>.</span>
</span>
</span>, <span class="citation" data-cites="GirlPain"><a href="#ref-GirlPain" role="doc-biblioref">[46]</a></span><span class="marginnote"><span id="ref-GirlPain" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[46] </span><span class="csl-right-inline">D. E. Hoffmann and A. J. Tarzian, <span>“The girl who cried pain: A bias against women in the treatment of pain,”</span> <em>SSRN</em>, 2001, doi: <a href="http://dx.doi.org/10.2139/ssrn.383803">http://dx.doi.org/10.2139/ssrn.383803</a>.</span>
</span>
</span>) and Black patients (despite prescription drug abuse being more prevalent among White Americans), <span class="citation" data-cites="RacialBiasPain"><a href="#ref-RacialBiasPain" role="doc-biblioref">[47]</a></span><span class="marginnote"><span id="ref-RacialBiasPain" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[47] </span><span class="csl-right-inline">K. M. Hoffman, S. Trawalter, J. R. Axt, and M. N. Oliver, <span>“Racial bias in pain assessment and treatment recommendations, and false beliefs about biological differences between blacks and whites,”</span> <em>Proceedings of the National Academy of Sciences</em>, vol. 113, no. 16, pp. 4296–4301, 2016, doi: <a href="https://doi.org/10.1073/pnas.1516047113">10.1073/pnas.1516047113</a>.</span>
</span>
</span>.</p>
</section>
<section id="measurement-bias" class="level4 unnumbered">
<h4 class="unnumbered">Measurement Bias</h4>
<p>Measurement bias refers to non-random noise in measurements across groups. This can occur if for example, there are geographic disparities in services provided by an institution or the quantity and quality of the measuring instruments that mean the accuracy and completeness of records vary by location (and other highly correlated variables like race). In some cases institutions can systematically fail to produce accurate and timely records for certain groups. For example, in medical data, where more frequent misdiagnosis of rare diseases for women leads to a longer lag before accurate diagnosis. In particular, 12 compared to 20 months for Crohn’s disease (despite the disease being more prevalent among women) and 16 compared to 4 years for Ehlers-Danlos syndrome<span class="citation" data-cites="EU12K"><a href="#ref-EU12K" role="doc-biblioref">[48]</a></span><span class="marginnote"><span id="ref-EU12K" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[48] </span><span class="csl-right-inline"><span>“The voice of 12,000 patients: Experiences and expectations of rare disease patients on diagnosis and care in europe.”</span> 2009.</span>
</span>
</span>. Systematic delays in diagnosis for protected groups mean that for any given snapshot in time, the medical records for more frequently misdiagnosed groups are less accurate.</p>
<p>Another way in which measurement bias can manifest is if the measurement process varies across groups, for example where the level of scrutiny varies across groups. Predictive policing discussed earlier provides an example of this where there are existing disparities in the level of policing across neighbourhoods. But in practice any process (algorithmic or otherwise) which seeks to identify a behaviour or property (good or bad), but where disproportionate attention is allocated to some subgroup will result in disproportionately more instances of that behaviour or property being observed among members of that group. The result is induced correlation in the data, even in cases where there may in reality be none. One must be careful of making the assumption that where no observation was made the behaviour or property did not exist. The result can be a cycle that continually amplifies the association. Since data often measures and records features which are in fact noisy proxies for the true variables of interest, measurement bias includes cases where use of proxies leads to systematic errors.</p>
</section>
<section id="representation-bias" class="level4 unnumbered">
<h4 class="unnumbered">Representation bias</h4>
<p>Representation bias occurs as a result of biased sampling from the target population. It can be observed as differences in the prevalence of groups when comparing the target population and the sample data. Under-represented classes are exposed to higher error rates; a problem which arises as a result of ‘low support’, that is a smaller pool of data points to train the model on. Looked at from the perspective of the majority class which dominates the aggregate error, the algorithm is naturally incentivised to focus learning characteristics of majority classes.</p>
<p>One of the drivers behind big data initiatives is the plummeting cost of collection and storage data. Companies and institutions are able to train models that better target individuals, reducing costs and boosting profits. However, data collection methods often fail to adequately capture historically disadvantaged classes of people that are less engaged in data generating ecosystems. A good example of this, given by Barocas &amp; Selbst<span class="citation" data-cites="BarocasSelbst"><a href="#ref-BarocasSelbst" role="doc-biblioref">[4]</a></span> is that of the phone app <a href="https://www.boston.gov/transportation/street-bump">Street Bump</a>, which was developed by the City of Boston to reduce the cost and time taken to find (and consequently repair) pot holes. The app uses data generated by the accelerometers and GPS of Boston residents’ smart phones as they drive. Once a pothole is located it is automatically added to the city’s system to schedule a repair. One can see easily see how this method of data collection might fail to adequately capture data from poorer neighbourhoods, where car and smart phone ownership are less prevalent; neighbourhoods which probably correlate with race and are already likely to suffer from lack of investment.</p>
<p>In the extreme case of under-representation, there is no support, that is to say, no data points to train on at all. This can be a problem when say studies of symptoms or clinical trials for drugs have no representation for certain groups among which symptoms or drug effectiveness may well vary. A good example of this is diabetes, the impact of the disease and effectiveness of drugs for which have historically most often been measured on samples with few to no hispanic individuals in datasets at all.</p>
</section>
<section id="low-support" class="level4 unnumbered">
<h4 class="unnumbered">Low support</h4>
<p>Low support may lead to undesirably high errors for some groups even in the absence of representation bias, since minority classes naturally have fewer data points to train on. This is a particular problem for individuals belonging to multiple disadvantaged classes, for example Black women, which are often overlooked when studies seek to meet fairness metric targets.</p>
</section>
<section id="documentation" class="level4 unnumbered">
<h4 class="unnumbered">Documentation</h4>
<p>Documentation of datasets is an essential step in avoiding data misuse or misinterpretation of variables or relationships in the data due to lack of domain knowledge. Documentation should evidence that model governance standards were met. Summaries that explain the provenance of the data (who collected the data, for what purpose, what population was sampled from and how, limitations of the data, clear explanation of the target variables (including consideration of use cases for which it would not appropriate for), breakdown of the demographics and the variables by sensitive features pointing out classes that are not well represented. Documentation that is standardised through use of a template could ensure some level of consistency.</p>
</section>
</section>
<section id="misspecification" class="level3" data-number="2.4.2">
<h3 data-number="2.4.2"><span class="header-section-number">2.4.2</span> Misspecification</h3>
<section id="aggregation-bias" class="level4 unnumbered">
<h4 class="unnumbered">Aggregation Bias</h4>
<p>Aggregation bias occurs when heterogeneous groups are modelled as homogeneous. In this case we are assuming the same model is appropriate for all groups when in fact it is not, it is a failure to recognise differences in type. There are many examples of this is medical models for diagnosis or that measure the effectiveness of treatments. Historically much of medical research is based on data that over-represents White men. Diseases that manifest differently across gender or race are more often misdiagnosed or less effectively treated. Take autism spectrum disorder (AUD) for example, in 2016 research estimated that autism is four times more prevalent in boys than girls. However more recent research has suggested that a contributing factor maybe that autism more often goes undiagnosed in women because studies of the disorder have historically been focused on male subjects. The most notable difference between autistic males and females is how the social (rather than behavioural) symptoms manifest. It is thought that women, especially at the high-functioning end of the spectrum, are more likely to camouflage their symptoms.</p>
</section>
<section id="target-variable-selection" class="level4 unnumbered">
<h4 class="unnumbered">Target Variable Selection</h4>
<p>One of the challenges in developing a machine learning is the translation of the underlying problem by defining a target variable - something which can be observed, measured and recorded or obtained easily (from a third party vendor), and that accurately reflects the variable we wish to predict. While there are relatively uncontentious examples that machine learning solutions lend themselves well to (spam detection for emails or on-base or slugging percentage for major league baseball player valuation) for many problems the translation is non-trivial and subjective. Take a job applicant filter for example, that aims to find the most promising applicants. The attributes that one might consider to be held by an applicant that make them promising are likely to be described differently by different people even if they work in the same team. Even if two individuals agree on the attributes, it’s likely they’ll weigh the attributes differently based on their experiences and preferences. Different choices will result in the different kinds of biases infiltrating our algorithm.</p>
<p>Often when data on the variable we want to affect doesn’t really exist we use a proxy. In 2018, Amazon was forced to scrap a recruitment tool it spent four years developing. The algorithm rated resumes of potential employees and was trained on 10 years worth of resumes submitted by job applicants. The exact details of the algorithm were not publicised but based on the training data, it is likely that the proxy variable they used was some measure of how the candidates had performed in the hiring process previously. Thus predicting who they would have hired in the past (given their historical and existing biases) rather than who was the best applicant. The problem with such systems is that often they end up being how we define the thing that it’s actually a proxy for.</p>
<p>Issues can also arise when defining a heterogeneous target variable, where a range of different events are coarsely grouped into a single outcome. This is a form of aggregation bias where the issue specifically concerns the target. This might happen for example where the event of particular interest is rare and by including more events in the target the predictive accuracy of the model increases as it has more data to learn from. D’Alessandro et. al<span class="citation" data-cites="ConsClfn"><a href="#ref-ConsClfn" role="doc-biblioref">[40]</a></span> provide a useful example in predictive policing where the model developer is initially interested in predicting violent crime but ends up incorporating petty crimes (which happen much more frequently) in the target variable in pursuit of a more accurate model. The model then ends up trying to learn the features of a more nebulous concept of crime ignoring important differences between different types. Another example might be building a gender recognition system and only recognising people as one of two genders<span class="citation" data-cites="GenderShades"><a href="#ref-GenderShades" role="doc-biblioref">[25]</a></span>.</p>
</section>
<section id="feature-selection" class="level4 unnumbered">
<h4 class="unnumbered">Feature selection</h4>
<p>In an ideal world we would train a machine learning model on a sufficiently large dataset consisting of a rich set of features that actually influence the target variable rather than simply being correlated to it. More often than not, the reality is rather different. Comprehensive data can be expensive and difficult to collect. Factors that influence the target variable might not be easily measured or be measurable at all, while data containing more erroneous indicators might simply be cheaper to obtain or more readily available. This is a common way in which bias against protected classes can enter our model.</p>
<p>The inclusion of protected features without control variables might arise because a protected feature appears to be predictive of the target variable where explanatory variables are not known or available. Of course in cases where using protected characteristics as inputs to an algorithm would lead to disparate treatment liability, this is not a problem one is typically faced with, but it’s worth reiterating the importance of controlling for confounding variables, in drawing conclusions about relationships between features from observational data (see section <a href="#sec_SimpsParadox" data-reference-type="ref" data-reference="sec_SimpsParadox">1.4</a>).</p>
<p>Inclusion of protected feature proxies, as is the case with redlining, is perhaps a more common problem. One where protected features are not used as inputs to the model, but features which are predictive of them are. Historically employers have taken the reputation of the university that applicants graduated from as a strong indicator of the calibre of the candidate. But many of the most reputable universities have very low rates of non-White/Asian students in attendance. A hiring process which is strongly influenced by the university from which the applicant graduated, can erroneously disadvantage racial groups that are less likely to have attended them. While the university an applicant graduated from, might correlate to some degree with success in a particular role, it is not in itself the driver. An algorithm that directly takes into account the skills and competencies required for the role would be more predictive and simultaneously less biased. Given the cost of collecting comprehensive data, one might argue that higher error rates for some classes would be financially justified (rational prejudice).</p>
</section>
<section id="cost-function" class="level4 unnumbered">
<h4 class="unnumbered">Cost function</h4>
<p>A critical consideration in how we specify our model is the cost function. It is how we evaluate our model in training and essentially determines the model (parameters) we end up with. The cost function can be interpreted as an expression of our model objectives and so provides a natural route to addressing discrimination concerns. A common failure in the design of classification models is proper accounting of the costs of the different types of classification errors (false negative versus false positives). If the harm caused by the different types of misclassification are asymmetric, the cost matrix should reflect this asymmetry.</p>
<p>More broadly (for both regression and classification), it is important to consider the contribution from each sample in the training data to the cost function in training. Upsampling (or simply up-weighting, depending on the learning algorithm you are using) is a valuable tool to keep in mind and can alleviate a number of the issues discussed above, that are common sources of bias. Let’s take the issue of low support. By upsampling minority classes, one can increase the importance of reducing errors for those data points, relative to other more abundant classes, during learning. Though it’s worth noting that it cannot resolve issues relating to a lack of richness of representation for classes with low support. Another case in which upsampling can help is that discussed in relation to definition of a heterogeneous target variable. By upsampling data points that correspond to the primary event of interest (violent crime in the example we discussed above), one can again increase the importance of the model fitting to those data points.</p>
<p>For an algorithm that solves a problem in a regulated domain, it would make sense for the absence of discrimination to be a model objective along with utility. This can be achieved by use of a penalty term in the cost function which relates to discrimination in the resulting predictions (just as we have terms that relate to the error or overfitting). Essentially the idea is similar to that of regularisation to avoid overfitting. We introduce an additional hyper-parameter to tune, which represents the strength of the penalty for discrimination in our cost. We will discuss this and upsampling in more detail when we discuss bias mitigation techniques, in part three of the book.</p>
</section>
<section id="evaluation-bias" class="level4 unnumbered">
<h4 class="unnumbered">Evaluation bias</h4>
<p>Evaluation bias arises when evaluating a model’s performance. There are two main components here, the metrics chosen to describe the model’s performance and the benchmark dataset on which they are calculated. Choosing either inappropriately will result in our evaluation metric inaccurately reflecting the efficacy of our model. For sociotechnical problems in particular choosing good metrics requires domain knowledge - the wider political, legal, social and historical context is important when defining what success and failure look like. For example, if building a gender recognition system, one should not simply think of the performance on the specific task but also the wider infrastructural systems which might find the technology useful. Where should we set the bar for such a technology? That should surely depend on how the technology is used after the prediction is made? Are there controls around model use? Should there be? What kinds of risk level does the model present? What might be the impact of the prediction being incorrect? When would an error be fair? What kind of examples would you expect your system to get wrong and why? What do they gave in common? Are they represented in the benchmark dataset? By asking these kinds of questions, when deciding what success looks like, it’s hard to imagine thinking that minimising the mean squared error on a conveniently available dataset would be sufficient.</p>
<p>One approach might be to set accuracy thresholds across all (skin colour) phenotype and gender combinations <span class="citation" data-cites="GenderShades"><a href="#ref-GenderShades" role="doc-biblioref">[25]</a></span>. This would be one way of thinking about success in a way that incorporates <em>some</em> of our societal values of equality. The gender recognition software we talked about in the previous chapter suffered from evaluation bias on both counts. The benchmark datasets used were not representative of the target population and the metrics that were chosen, failed to expose the models poor performance on darker skinned women. The problem of evaluation bias arising from poor choice of testing/benchmark data is often the result of trying to objectively compare performance across models and can lead to overfitting to said benchmark data.</p>
</section>
<section id="documentation-1" class="level4 unnumbered">
<h4 class="unnumbered">Documentation</h4>
<p>Documentation for models (as for datasets) can have a significant impact when it comes to avoiding model misuse (a model use it is not appropriate/approved for) and ensuring model limitations are well understood. It can reduce the risk of misinterpretation of variables as suitable proxies for other variables. Clear explanation of the model, testing that was performed, on what subgroups of the data can make it easier to know which tests might be missing that would offer insight into the validity of the model. Documentation should evidence that the model governance standards have been met. Descriptions of the data and model, motivation behind subjective decisions that were made to arrive at the solution (how to process the data, what features were used/ignored and why, model type, cost function, sample weights, bias and success metrics), known data/model issues, how the model was tested, what it’s limitations are, what it should and should not be used for with justification. Documentation of the model should provide enough detail to be able to re-implement the model, reproduce results and justify the solution approach. Documentation that is standardised through use of a template could ensure some level of consistency and efficiency across domains and applications. Recent research discusses the matter specifically for publicly released datasets<span class="citation" data-cites="DS4DS"><a href="#ref-DS4DS" role="doc-biblioref">[42]</a></span> and machine learning models<span class="citation" data-cites="MC4MR"><a href="#ref-MC4MR" role="doc-biblioref">[43]</a></span>. They suggest standardised analysis which for example demonstrates the performance of the algorithm for different subgroups of the population and requirements for proving efficacy for conjunctions of sensitive characteristics also.</p>
</section>
</section>
</section>
<section id="linking-common-causes-of-harm-to-the-workflow" class="level2" data-number="2.5">
<h2 data-number="2.5"><span class="header-section-number">2.5</span> Linking Common Causes of Harm to the Workflow</h2>
<p>In Figure <a href="#fig:Taxonomy" data-reference-type="ref" data-reference="fig:Taxonomy">2.5</a> we provide a visual summary of the taxonomy in Table <a href="#tbl:Taxonomy" data-reference-type="ref" data-reference="tbl:Taxonomy">2.2</a>, the goal being that it might be useful as a reference for teams developing machine learning technologies. Since failures of policy do not relate to any particular part of the model development and deployment life cycle but rather all of it, we omit these.</p>
<figure class="fullwidth">
<img src="02_EthicalDevelopment/figures/Fig_Taxonomy.png" id="fig:Taxonomy" alt="Figure 2.5: Taxonomy of common causes of bias in machine learning models together with the stages of the model development and deployment life cycle they relate to." />
<figcaption aria-hidden="true">Figure 2.5: Taxonomy of common causes of bias in machine learning models together with the stages of the model development and deployment life cycle they relate to.</figcaption>
</figure>
<p>At the top of Figure <a href="#fig:Taxonomy" data-reference-type="ref" data-reference="fig:Taxonomy">2.5</a> we have a simplified version of the model development and deployment life cycle. Below this, the causes of harm are displayed in boxes which span the parts of the lifecycle to which they relate. We use colour to separate different categories of failures and curly brackets to group issues by type.</p>
</section>
<section id="summary-1" class="level2 unnumbered">
<h2 class="unnumbered">Summary</h2>
<section id="machine-learning-cycle-1" class="level3 unnumbered">
<h3 class="unnumbered">Machine learning cycle</h3>
<ul>
<li><p>Machine learning solutions can have long-term and compounding effects on the world around us. Figure <a href="#fig:MLCycle" data-reference-type="ref" data-reference="fig:MLCycle">2.1</a> illustrates the interaction between a machine learning solution and the real world.</p></li>
<li><p>The translation of a given problem and objectives into a tractable machine learning problem, requires a series of subjective choices. Choices around what data to train the model on, what events to predict, what features to use, how to clean and process the data, how to evaluate the model and what the decision policy should be will all determine the model we create, the actions we take and ultimately the cycle we end up with.</p></li>
<li><p>Data is a necessarily subjective representation of the world. The sample may be biased, contain an inadequate collection of features, subjective decisions around how to categorise features into groups, systematic errors or be tainted with prejudice decisions. We may not even be able to measure the true metric we wish to impact. Data collected for one purpose is often reused for another under the assumption that it represents the ground truth when it does not.</p></li>
<li><p>In cases where the ground truth (target variable) assignment systematically disadvantages certain classes, actions taken based on predictions from models trained on the data are capable of reinforcing and further amplifying the bias.</p></li>
<li><p>Decisions made on the basis of results derived from machine learning algorithms trained on data that under or over-represents certain classes can have feedback effects that further skew the representation of those classes in future data.</p></li>
<li><p>The actions we take based on our model predictions define how we use the model. The same model used in a different way can result in a very different feedback cycle.</p></li>
<li><p>The magnitude of the feedback effect will depend how much control the institution making decisions based on the predictions, has over the data the training data.</p></li>
<li><p>Just as we can create pernicious machine learning cycles that exaggerate disparities, we can also create virtuous ones that have the effect of reducing disparities. Therefore it’s important to consider the whole machine learning cycle when formulating a machine learning problem</p></li>
</ul>
</section>
<section id="model-development-and-deployment-life-cycle" class="level3 unnumbered">
<h3 class="unnumbered">Model development and deployment life cycle</h3>
<ul>
<li><p>Figure <a href="#fig:Workflow" data-reference-type="ref" data-reference="fig:Workflow">2.4</a> depicts the model development, deployment and monitoring life cycle at a high level. Overarching the entire workflow, are the <strong>model governance standards</strong>. These essentially outline the processes, roles and responsibilities that constitute the development, deployment and management of the machine learning system. It defines and documents a set of standards for the activities that constitute each stage of the workflow.</p></li>
<li><p><strong>Problem formulation:</strong> Translating a business problem into a machine learning one.</p>
<ul>
<li><p>The problem formulation stage plays a pivotal role in what the end product will actually be. It is the stage at which the model objectives, requirements, target variable and training data are determined and it is the stage at which perhaps the most important ethical question (whether the model should be built at all) must be answered.</p></li>
<li><p>Consider who is affected by the technology, consult with them and ensure their views are understood and incorporated in the understanding of the problem and design of a potential solution.</p></li>
<li><p>Assess the materiality of the risk. What’s the worst that can happen? How likely is such a failure? How many people are exposed to the model?</p></li>
<li><p>Examine the machine learning cycle in the context of the biases in the data and consider the nature (direction and strength) of the feedback of resulting actions on future data.</p></li>
<li><p>Consider other ways in which the model might be used (other than that intended) and the corresponding feedback cycle in those cases. How the model might be misused?</p></li>
</ul></li>
<li><p><strong>Independent model validation:</strong> An independent review process is an important element of responsible model development. This means that pre-deployment there are two separate data science roles, model development (designing a solution) and the model validation (critical assessment of the solution).</p></li>
<li><p><strong>Model development:</strong> The model developers role is to translate the business problem into a tractable machine learning problem and create a model solution.</p>
<ul>
<li><p>The model developer will work with the business and receive input from other necessary domain experts relevant to the application to develop a possible solution.</p></li>
<li><p>The model developer should document the solution. Documentation should include descriptions of the data and model, justification of the approach, known issues and limitations, model testing (biases as well as performance), what the model should not be used for and why. Templates are a good way of standardising documentation.</p></li>
<li><p>In terms of preventing failures, the model developer is the <em>first line of defence</em>. The responsibility of developing a model responsibly and ethically lies, in the first instance, with them.</p></li>
</ul></li>
<li><p><strong>Model validation:</strong> The role of a model validator is to criticise the proposed solution.</p>
<ul>
<li><p>The model validator will identify and expose issues with the problem formulation, data and data processing. They will verify the model performance metrics (error, bias, fairness), look for model weaknesses and demonstrate them through testing. They may also devise mitigation strategies for identified risks.</p></li>
<li><p>The role of the reviewer might be thought of as a hacker but with the advantage of having access to the model documentation (provided by the model developer). They also act as a gate keeper.</p></li>
<li><p>The model reviewer must also document their analysis, testing and critique and recommendations regarding the solution.</p></li>
<li><p>The model reviewer acts as the <em>second line of defence</em>.</p></li>
</ul></li>
<li><p><strong>Model approval:</strong> The model owners collectively determine if a solution is ready for deployment.</p>
<ul>
<li><p>Model owners act as the final stage gate keepers before deployment. They will each have been involved in different aspects of the development and deployment of the machine learning system.</p></li>
<li><p>In effect, the model owners represent the different stakeholders of the risk associated with the model and collectively they are accountable, though for potentially differing aspects of it.</p></li>
<li><p>They will also be responsible for monitoring the model and risk materiality post-deployment and ensuring that periodic re-review, failure processes and post-mortems occur and are effective.</p></li>
<li><p>The model governance standards might be interpreted as a contract between the model owners that describes their commitments, individually and collectively in managing the risk.</p></li>
</ul></li>
<li><p><strong>Monitoring of deployed models:</strong> The world is dynamic and the risk associated with models evolves with it. Deployed models should be monitored to understand if they are behaving in line with expectations. The metrics which should be reported to model owners should be identified pre-deployment by the model developer and validator.</p></li>
<li><p><strong>Risk materiality tracking:</strong> As model usage increases so does the associated risk. As part of monitoring, metrics that give an indication of the risk associated with the model is should be reported to the model owners.</p></li>
<li><p><strong>Periodic re-review:</strong> The pre-deployment independent review of the model is just the first. Thereafter, periodic re-reviews of the model are a means to catch risks that may have been missed the first time around. The frequency of re-reviews will depend on the risk level of the model/application in question.</p></li>
<li><p><strong>Failure event process:</strong> Processes and procedures in the event of failures should be specified as part of the model governance standards, in particular what steps should be taken by which model owner. Having a robust process around dealing with failures when they occur should mean that action is taken in a timely manner and that meaningful changes are made as a result of them.</p></li>
<li><p><strong>Failure post-mortems:</strong> A post-mortem should focus on understanding the weaknesses of the model governance process (not the failure of individuals) that contributed to it and appropriately prioritise any changes required to remedy them.</p></li>
<li><p><strong>Measuring bias:</strong> Bias and fairness metrics are essentially calculated on data; the data going into our model (training data) and the data coming out of it (the predictions produced by our model); the data evaluation and model evaluation stages.</p></li>
<li><p><strong>Bias mitigation techniques:</strong> There are three stages at which one can intervene to mitigate bias when developing a machine learning model labelled <em>data pre-process</em>, <em>model training</em> and <em>model post-process</em> in Figure <a href="#fig:Workflow" data-reference-type="ref" data-reference="fig:Workflow">2.4</a>. We categorise them accordingly:</p>
<ul>
<li><p><strong>Pre-processing techniques</strong> modify to the historical data on which the model is trained, the idea being that fair/unbiased data will result in a fair/unbiased model once trained.</p></li>
<li><p><strong>In-processing techniques</strong> alter the training process or objective in order to create model with fairer/less biased predictions.</p></li>
<li><p><strong>Post-processing techniques</strong> take a trained model and modify the output such that the resulting predictions are fairer/less biased.</p></li>
</ul></li>
</ul>
</section>
<section id="responsible-model-development-and-deployment" class="level3 unnumbered">
<h3 class="unnumbered">Responsible model development and deployment</h3>
<section id="model-governance-standards-2" class="level4 unnumbered">
<h4 class="unnumbered">Model governance standards</h4>
<ul>
<li><p>Machine learning systems can be complicated and have many points of failure: problem formulation, the data collection, data processing, modelling, implementation, deployment. The only way to reduce the risk of failures is to be organised, deliberate and plan for them. Creating a set of standards does exactly that. They make sure the right questions get asked at the right time and that there is clarity around who is responsible for what.</p></li>
<li><p>The purpose of creating a set of model governance standards is to clearly define and communicate what responsible model development and deployment looks like for your specific application, domain, business, principles and values. It essentially documents and communicates the why, who, what and how of your model risk management approach.</p>
<ul>
<li><p><strong>Why is the work important?</strong> What kinds of events are you trying to avoid? What are the consequences of failures? What are the values of the company that you want to protect?</p></li>
<li><p><strong>Who is responsible?</strong> Who are the stakeholders? Who is accountable for managing the various identified risks?</p></li>
<li><p><strong>What are they responsible for?</strong> What are their roles/expertise? What authority do they have in relation to determining if the model is fit for deployment?</p></li>
<li><p><strong>How do you manage the risk?</strong> What are the policies, processes and requirements that ensure the companies values are maintained, people are treated fairly, the legal requirements are fulfilled and the model risks are appropriately managed? How do the stakeholders work together?</p></li>
</ul></li>
<li><p>In large companies that carry lots of model risk it can be difficult to ensure there is consistency in standards of due diligence in model development and deployment across the board. The role of internal audit is to provide independent and objective feedback on the risks, systems, processes and compliance at an executive level. From a model governance perspective they determine if that there are good processes in place and that the processes are being followed. From a risk management perspective internal audit’s role constitutes the <em>third line of defence</em>.</p></li>
</ul>
</section>
</section>
<section id="common-causes-of-harm-1" class="level3 unnumbered">
<h3 class="unnumbered">Common causes of harm</h3>
<ul>
<li><p>Table <a href="#tbl:Taxonomy" data-reference-type="ref" data-reference="tbl:Taxonomy">2.2</a> summarises the taxonomy of common causes of bias in a machine learning system.</p></li>
<li><p>Figure <a href="#fig:Taxonomy" data-reference-type="ref" data-reference="fig:Taxonomy">2.5</a> summarises common causes of bias in the context of the model development and deployment workflow, indicating both the stages of the workflow to which they relate and their categorisation within the taxonomy.</p></li>
</ul>
</section>
</section>
</section>
<section id="part-ii-measuring-bias" class="level1 unnumbered">
<h1 class="unnumbered">Part II Measuring Bias</h1>
<p>“To measure is to know. If you cannot measure, you cannot improve." Lord Kelvin.</p>
<p>“When a measure becomes a target, it ceases to be a measure." Goodhart’s Law.</p>
</section>
<section id="ch_GroupFairness" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Group Fairness</h1>
<div class="chapsumm">
<p><strong>This chapter at a glance</strong></p>
<ul>
<li><p>Group fairness concepts and metrics</p></li>
<li><p>Comparing different group fairness metrics</p></li>
<li><p>Incompatibility of group fairness criteria</p></li>
<li><p>Weaknesses of group fairness criteria</p></li>
</ul>
</div>
<p>The term <em>group fairness</em> is used to describe a class of metrics that are used to measure discrimination or bias across specific subgroups of a population, in a given decision process (algorithmic or otherwise). In this chapter we will introduce group fairness metrics in a structured way, and familiarise ourselves with the terminology for well known metrics. We will compare and analyse the different categories of groups fairness metrics in terms of their assumptions, interpretation and potential implications. We’ll prove that the different classes of metrics cannot be satisfied simultaneously except in some degenerate cases. The goal of this chapter, is to develop a deeper understanding of different group fairness metrics, that will enable us to make more educated decisions about which metrics might offer particularly valuable insights for a given problem.</p>
<p>At the implementation level, all group fairness metrics indicate the extent to which, some <em>statistical property</em> differs between different <em>subgroups</em> of a population. The <em>subgroups</em> are typically determined by the values of <em>protected characteristics</em> such as gender or ethnicity. We might also describe these as <em>sensitive features</em>. Partitions of the population could be defined by a single feature or logical conjunctions of multiple sensitive features if we are interested in <em>intersectional fairness</em>. For example, if we were considering both race and gender simultaneously, one group of the partition might be Black women, another White men, and so on (more about this later). The <em>statistical property</em> we’ll be interested in comparing will depend on our beliefs about what fairness should mean in the context of the problem.</p>
<p>We broadly classify group fairness criteria into two types; those comparing <em>outcomes</em> across groups and those comparing <em>errors</em>. We discussed examples of both in chapter <a href="#ch_Background" data-reference-type="ref" data-reference="ch_Background">1</a>. Recall that in section <a href="#sec_SimpsParadox" data-reference-type="ref" data-reference="sec_SimpsParadox">1.4</a>, we compared outcomes (acceptance rates) for male and female applicants to Berkeley as an example of Simpson’s rule. In section <a href="#sec_harms" data-reference-type="ref" data-reference="sec_harms">1.5</a>, we discussed Gender Shades, a project that compared the errors (or equivalently accuracy) of a set of gender recognition systems, across subgroups defined by skin tone and gender. We’ll see how in general group fairness criterion can be understood as independence constraints on the joint distributions of the non-sensitive features <span class="math inline">\(X\)</span>, sensitive features, <span class="math inline">\(Z\)</span>, the target variable <span class="math inline">\(Y\)</span> and predicted target <span class="math inline">\(\hat{Y}\)</span> (or rather <span class="math inline">\(P\)</span> for a classification problem where we want our fairness criteria to hold for all thresholds). For brevity, we will express all constraints in terms of <span class="math inline">\(\hat{Y}\)</span>, but keep in mind that for classification problems we might want to instead impose it on the score <span class="math inline">\(P\)</span>. We will introduce the necessary mathematical notation as required throughout this book. A summary is provided in appendix <a href="#app_Notation" data-reference-type="ref" data-reference="app_Notation">A</a>.</p>
<section id="sec_BalOut" class="level2" data-number="3.1">
<h2 data-number="3.1"><span class="header-section-number">3.1</span> Comparing Outcomes</h2>
<p>First we look at fairness constraints on the relationship between the sensitive features <span class="math inline">\(Z\)</span>, and the predicted target <span class="math inline">\(\hat{Y}\)</span> (or rather <span class="math inline">\(Y\)</span> if we are interested in understanding our data rather than our model output). We’ll discuss two fairness criteria. In the first we require the outcome <span class="math inline">\(\hat{Y}\)</span>, to be marginally (unconditionally) independent of the sensitive features <span class="math inline">\(Z\)</span>. In the second we are essentially trying to establish cause; we require the outcome <span class="math inline">\(\hat{Y}\)</span> to be independent of the sensitive features <span class="math inline">\(Z\)</span> when conditioned on all other (non-sensitive) features <span class="math inline">\(X\)</span>. We’ll describe the latter as the twin test, that is <span class="math inline">\(\hat{Y}\)</span> and <span class="math inline">\(Z\)</span> being independent ceteris paribus (all else, or rather all other variables <span class="math inline">\(X\)</span>, being equal).</p>
<section id="independence" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1"><span class="header-section-number">3.1.1</span> Independence</h3>
<p>Of all fairness criteria, independence is undoubtedly the most well known. It requires the target variable to be unconditionally (marginally) <em>independent</em> of the sensitive feature, that is, <span class="math inline">\(\hat{Y} \bot Z\)</span>. This is true if and only if (<span class="math inline">\(\Leftrightarrow\)</span>), the probability distribution of the target variable <span class="math inline">\(f_{\hat{Y}}(y)\)</span>, is the same for all values of the sensitive feature <span class="math inline">\(Z\)</span>; that is, <span class="math inline">\(f_{\hat{Y}|Z}(y)=f_{\hat{Y}}(y)\)</span>. For a discrete target variable we require <span class="math display">\[\hat{Y} \bot Z \quad \Leftrightarrow \quad \mathbb{P}(\hat{Y}=\hat{y}|Z=z) = \mathbb{P}(\hat{Y}=\hat{y}) \quad \forall \quad y \in \mathcal{Y}, \quad z \in \mathcal{Z},\]</span> or <span class="math inline">\(\mathbb{P}(\hat{y}|z)=\mathbb{P}(\hat{y})\)</span> in our abbreviated notation.</p>
<p>Recall that for the 1973 Berkeley admissions example in section <a href="#sec_SimpsParadox" data-reference-type="ref" data-reference="sec_SimpsParadox">1.4</a>, we looked at independence criterion, by comparing acceptance rates across the sensitive feature gender. Independence has been interpreted as addressing disparate impact<span class="citation" data-cites="DispMistreat"><a href="#ref-DispMistreat" role="doc-biblioref">[49]</a></span><span class="marginnote"><span id="ref-DispMistreat" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[49] </span><span class="csl-right-inline">M. B. Zafar, I. Valera, M. Gomez Rodriguez, and K. P. Gummadi, <span>“Fairness beyond disparate treatment &amp; disparate impact,”</span> <em>Proceedings of the 26th International Conference on World Wide Web</em>, 2017, doi: <a href="https://doi.org/10.1145/3038912.3052660">10.1145/3038912.3052660</a>.</span>
</span>
</span>, since we are only interested in the relationship between the outcome and sensitive feature. Independence criterion might be interpreted as a strong expression of fairness as equality; the belief or assumption that any differences in the target between subgroups, are a direct result of structural injustice<span class="citation" data-cites="GrpIndConflict"><a href="#ref-GrpIndConflict" role="doc-biblioref">[50]</a></span><span class="marginnote"><span id="ref-GrpIndConflict" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[50] </span><span class="csl-right-inline">R. Binns, <span>“On the apparent conflict between individual and group fairness.”</span> arXiv, 2019. doi: <a href="https://doi.org/10.48550/ARXIV.1912.06883">10.48550/ARXIV.1912.06883</a>.</span>
</span>
</span>. It’s important to acknowledge that if independence is not satisfied by the data, imposing it on a model implies a level of distrust in the data or model. Independence metrics provide valuable insight and in some cases trying to achieve independence might even make sense as a corrective measure; but if differences are large it should naturally lead us to question the suitability of the data and modelling of the problem before introducing technical interventions.</p>
<section id="measures-of-independence" class="level4 unnumbered">
<h4 class="unnumbered">Measures of independence</h4>
<p>In this section we will define a range of fairness metrics derived from the notion of independence. Along the way, we will familiarise ourselves with some of the terminology used to describe them. In the equations that follow, we provide metrics that quantify the extent of the relationship between our model output <span class="math inline">\(\hat{Y}\)</span> and sensitive feature <span class="math inline">\(Z\)</span>, but we could equally well replace the predicted target variable <span class="math inline">\(\hat{Y}\)</span>, with the actual target <span class="math inline">\(Y\)</span> to assess our data under the same criterion.</p>
<p><strong>Mutual information</strong>, denoted <span class="math inline">\(I\)</span>, is popular in information theory for measuring dependence between random variables.</p>
<div id="eq:MutualInfo">
<table>
<colgroup>
<col style="width: 90%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
I(\hat{Y},Z) = \sum_{z \in \mathcal{Z}} \,\, \int_{\hat{y} \in \mathcal{Y}}
f_{\hat{Y},Z}(\hat{y},z) \log \frac{f_{\hat{Y},Z}(\hat{y},z)}
{f_{\hat{Y}}(\hat{y})\mathbb{P}(z)}\,\mathrm{d}\hat{y}.\]</span></td>
<td style="text-align: right;">(3.1)</td>
</tr>
</tbody>
</table>
</div>
<p>It is equal to zero, if and only if the joint distribution of <span class="math inline">\(Z\)</span> and <span class="math inline">\(\hat{Y}\)</span> is equal to the product of their marginal distributions, that is if <span class="math inline">\(f_{\hat{Y},Z}(\hat{y},z)=f_{\hat{Y}}(\hat{y})\mathbb{P}(z)\)</span>. Therefore, two variables which have zero mutual information are independent. The <strong>normalised prejudice index</strong><span class="citation" data-cites="Fukuchi"><a href="#ref-Fukuchi" role="doc-biblioref">[51]</a></span><span class="marginnote"><span id="ref-Fukuchi" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[51] </span><span class="csl-right-inline">K. Fukuchi, J. Sakuma, and T. Kamishima, <span>“Prediction with model-based neutrality,”</span> <em>IEICE TRANS. INF. &amp; SYS.</em>, vol. E98–D, no. 8, 2015, doi: <a href="https://doi.org/10.1587/transinf.2014EDP7367">10.1587/transinf.2014EDP7367</a>.</span>
</span>
</span> divides mutual information by a normalising factor so that the resulting value falls between zero and one:</p>
<div id="eq:npi">
<table>
<colgroup>
<col style="width: 90%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
r_{\text{npi}} = \frac{I(\hat{Y},Z)}{\sqrt{S(\hat{Y})S(Z)}},\]</span></td>
<td style="text-align: right;">(3.2)</td>
</tr>
</tbody>
</table>
</div>
<p>where <span class="math inline">\(S\)</span> is the entropy,</p>
<div id="eq:entropyY">
<table>
<colgroup>
<col style="width: 90%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
S(Y) = -\int_{y \in \mathcal{Y}} f_Y(y) \log f_Y(y)\,\mathrm{d}y,\]</span></td>
<td style="text-align: right;">(3.3)</td>
</tr>
</tbody>
</table>
</div>
<p>and</p>
<div id="eq:entropyZ">
<table>
<colgroup>
<col style="width: 90%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
S(Z) = -\sum_{z \in \mathcal{Z}} \mathbb{P}(z) \log \mathbb{P}(z),\]</span></td>
<td style="text-align: right;">(3.4)</td>
</tr>
</tbody>
</table>
</div>
<p>In the above equation we assume a continuous target variable, for classification problems we can replace the integrals in equations (3.1) and (3.4) with summations. <a href="#GF_NPI">Implemention</a> in appendix <a href="#sec_app_GFSolutions" data-reference-type="ref" data-reference="sec_app_GFSolutions">D.1</a>.</p>
<p>A simple relaxation of independence requires only the mean predicted target variable (rather than the full distribution) to be equal for all values of the sensitive feature, for example, <span class="math display">\[\mathbb{E}(\hat{Y} | Z=0) = \mathbb{E}(\hat{Y} | Z=1).\]</span> We could measure the extent of the disparity by taking the ratio or the difference of the expectations; the latter is more commonly used. The <strong>mean difference</strong> (illustrated in Figure <a href="#fig:distributions" data-reference-type="ref" data-reference="fig:distributions">3.1</a>) which (as the name suggests) looks at the difference between the mean predictions for different partitions of the population based on the sensitive feature <span class="math inline">\(Z\)</span>, <span class="math display">\[d = \mathbb{E}(\hat{Y} | Z=0) - \mathbb{E}(\hat{Y} | Z=1).\]</span></p>
<figure>
<img src="03_GroupFairness/figures/Fig_distributions.png" id="fig:distributions" alt="Figure 3.1: Visualisation of the mean difference for a continuous target variable." />
<figcaption aria-hidden="true">Figure 3.1: Visualisation of the mean difference for a continuous target variable.</figcaption>
</figure>
<p>Taking the simplest example of discrete binary classifier where we have a binary sensitive feature. We can write the requirement of independence as, <span class="math display">\[\mathbb{P}(\hat{Y}=1 | Z=1) = \mathbb{P}(\hat{Y}=1 | Z=0).\]</span> This criterion goes by many names in research literature - <strong>demographic parity</strong>, <strong>statistical parity</strong> and <strong>parity impact</strong>, among others. We can quantify the disparity by looking at the difference or the ratio of the acceptance rates for each sensitive feature. Both are straightforward to calculate given the 2 <span class="math inline">\(\times\)</span> 2 contingency table (Table <a href="#tbl:independence" data-reference-type="ref" data-reference="tbl:independence">3.1</a>), which summarises the observed relationship between the sensitive feature and outcome. Each cell of the contingency table shows the number of examples satisfying the conditions given in the corresponding row and column headers. So for example, <span class="math inline">\(n_{01}\)</span> is the number of data points for which <span class="math inline">\(Z=0\)</span> and <span class="math inline">\(\hat{Y}=1\)</span>.</p>
<div id="tbl:independence">
<table>
<caption>Table 3.1: Contingency table for prediction against the sensitive feature.</caption>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;"><span class="math inline">\(\hat{Y}=1\)</span></th>
<th style="text-align: right;"><span class="math inline">\(\hat{Y}=0\)</span></th>
<th style="text-align: right;">Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(Z=1\)</span></td>
<td style="text-align: right;"><span class="math inline">\(n_{11}\)</span></td>
<td style="text-align: right;"><span class="math inline">\(n_{10}\)</span></td>
<td style="text-align: right;"><span class="math inline">\(n_{Z=1}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(Z=0\)</span></td>
<td style="text-align: right;"><span class="math inline">\(n_{01}\)</span></td>
<td style="text-align: right;"><span class="math inline">\(n_{00}\)</span></td>
<td style="text-align: right;"><span class="math inline">\(n_{Z=0}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Total</td>
<td style="text-align: right;"><span class="math inline">\(n_{\hat{Y}=1}\)</span></td>
<td style="text-align: right;"><span class="math inline">\(n_{\hat{Y}=0}\)</span></td>
<td style="text-align: right;"><span class="math inline">\(n\)</span></td>
</tr>
</tbody>
</table>
</div>
<p>In bio-medical sciences, the <strong>risk difference</strong>: <span class="math display">\[d = \mathbb{P}(\hat{Y}=1 | Z=0) - \mathbb{P}(\hat{Y}=1 | Z=1)
  = \frac{n_{11}}{n_{Z=0}} - \frac{n_{01}}{n_{Z=1}},\]</span> measures the impact of treatment (or risk factors), <span class="math inline">\(Z\)</span> on outcome, <span class="math inline">\(\hat{Y}\)</span>. In discrimination literature, it has been described as the <strong>discrimination score</strong> and <strong>statistical parity difference</strong> among others. Note that if <span class="math inline">\(\hat{Y}=1\)</span> is the advantageous outcome and <span class="math inline">\(Z=1\)</span> is the advantaged group, we would expect <span class="math inline">\(d\)</span> to be negative. The algorithm is fair when <span class="math inline">\(d=0\)</span>. The further from zero, the greater the disparity. A modified version of this metric is the <strong>normalised difference</strong><span class="citation" data-cites="Zliobaite"><a href="#ref-Zliobaite" role="doc-biblioref">[52]</a></span><span class="marginnote"><span id="ref-Zliobaite" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[52] </span><span class="csl-right-inline">I. Zliobaite, <span>“On the relation between accuracy and fairness in binary classification.”</span> 2015.Available: <a href="https://arxiv.org/abs/1505.05723">https://arxiv.org/abs/1505.05723</a></span>
</span>
</span> which divides the risk difference by it’s maximal value, thus ensuring the normalised difference is bounded between plus and minus one.</p>
<div class="lookbox">
<p><strong>Statistical Parity Difference Maximum</strong></p>
<div id="eq:dmax">
<table>
<colgroup>
<col style="width: 90%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
d_{\max} = \min\left\{ \frac{\mathbb{P}(\hat{Y}=1)}{\mathbb{P}(Z=1)},
                       \frac{\mathbb{P}(\hat{Y}=0)}{\mathbb{P}(Z=0)} \right\}
= \min\left\{ \frac{n_{\hat{Y}=1}}{n_{Z=1}},
              \frac{n_{\hat{Y}=0}}{n_{Z=0}} \right\},\]</span></td>
<td style="text-align: right;">(3.5)</td>
</tr>
</tbody>
</table>
</div>
<p><a href="#GF_SPDmax">Proof</a> in appendix <a href="#sec_app_GFSolutions" data-reference-type="ref" data-reference="sec_app_GFSolutions">D.1</a>.</p>
</div>
<p>Alternatively, we could instead take the ratio as a measure of discrimination: <span class="math display">\[r = \frac{\mathbb{P}(\hat{Y}=1 | Z=0)}{\mathbb{P}(\hat{Y}=1 | Z=1)}
  = \frac{n_{11}/n_{Z=0}}{n_{01}/n_{Z=1}}.\]</span> In biomedical sciences this measure is called the <strong>risk ratio</strong>. It is used to measure the strength of association between treatment (or risk factors), <span class="math inline">\(Z\)</span>, and outcome, <span class="math inline">\(\hat{Y}\)</span>. It has been described in discrimination aware machine learning literature as the <strong>impact ratio</strong> or <strong>disparate impact ratio</strong>. The algorithm is fair if <span class="math inline">\(r=1\)</span>. If we take <span class="math inline">\(Z=1\)</span> to be the advantaged group the value is bounded between zero and one. The Equal Employment Opportunity Commission (EEOC) have used this measure in their guidelines for identifying discrimination in employment selection processes<span class="citation" data-cites="US-EEOC"><a href="#ref-US-EEOC" role="doc-biblioref">[53]</a></span><span class="marginnote"><span id="ref-US-EEOC" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[53] </span><span class="csl-right-inline">U. S. E. E. O. Commission, <span>“Questions and answers to clarify and provide a common interpretation of the uniform guidelines on employee selection procedures,”</span> <em>Federal Register</em>, vol. 44, no. 43, 1979.</span>
</span>
</span>. As a rule of thumb, the EEOC determine that a company’s selection system is having an adverse impact on a particular group if the selection rate for that group is less than four-fifths (or 80%) that of the most advantaged group, that is, the impact ratio is less than 0.8 where <span class="math inline">\(Z=0\)</span> is the most advantaged group (for which the acceptance rate is the highest).</p>
<p>The <strong>elift ratio</strong><span class="citation" data-cites="Pedreschi"><a href="#ref-Pedreschi" role="doc-biblioref">[54]</a></span><span class="marginnote"><span id="ref-Pedreschi" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[54] </span><span class="csl-right-inline">D. Pedreschi, S. Ruggieri, and F. Turini, <span>“Discrimination-aware data mining,”</span> in <em>Proceedings of the ACM SIGKDD international conference on knowledge discovery and data mining</em>, 2008, pp. 560–568. doi: <a href="https://doi.org/10.1145/1401890.1401959">10.1145/1401890.1401959</a>.</span>
</span>
</span> is similar to the impact ratio but instead of comparing acceptance rates for protected groups to each other, we compare them to the overall/mean acceptance rate: <span class="math display">\[\frac{\mathbb{P}(\hat{Y}=1 | Z=0)}{\mathbb{P}(\hat{Y}=1)}.\]</span></p>
<p>In theory, any measure of association suitable for the data types can be used as a metric to understand the magnitude of discrimination in our data or predictions. The <strong>odds ratio</strong> (popular in natural, social and biomedical sciences) is the ratio of the odds of a positive prediction for each group. We can write it as: <span class="math display">\[\frac{\mathbb{P}(\hat{Y}=1 | Z=1)\mathbb{P}(\hat{Y}=0 | Z=0)}
         {\mathbb{P}(\hat{Y}=0 | Z=1)\mathbb{P}(\hat{Y}=1 | Z=0)}
  = \frac{n_{11}n_{00}}{n_{10}n_{01}}.\]</span> The odds ratio is equal to one when there is no discrimination. In the case where <span class="math inline">\(\hat{Y}=1\)</span> in the advantaged outcome and <span class="math inline">\(Z=1\)</span> is the privileged group, the odds ratio is always greater than or equal to one. Recall that the odds ratio is not a collapsible measure (see section <a href="#sec_collapsibility" data-reference-type="ref" data-reference="sec_collapsibility">1.4.3</a>).</p>
<p>As mentioned earlier, independence metrics can be evaluated on both the data and the model. A common problem in machine learning is that existing biases in the data can be exaggerated if protected groups are minorities in the population. By comparing metrics for the data with those of our model output, we can understand if our model is inadvertently introducing biases that do not originate from the data.</p>
<p>It might seem intuitive already, that independence can only be satisfied by a model (optimising for predictive performance), if the target variable <span class="math inline">\(Y\)</span> and sensitive feature <span class="math inline">\(Z\)</span> are in fact independent<span class="sidenote-wrapper"><label for="sn-5" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-5" class="margin-toggle"/><span class="sidenote">We’ll prove this to be true in section <a href="#sec_Impossible" data-reference-type="ref" data-reference="sec_Impossible">3.3</a>, for the case where our variables are binary.<br />
<br />
</span></span>. If this is not the case, then satisfying independence for one’s model, will not permit the theoretically perfect solution <span class="math inline">\(\hat{Y}=Y\)</span> (should your model be able to achieve it). We would also then naturally, expect that the stronger the relationship between the sensitive feature and target, the greater the trade-off with utility in satisfying independence criterion.</p>
<p>Independence does not guarantee fairness in a broader sense. Consider a simple hypothetical example where, there are discrepancies between credit card approval rates for men and women at the population level, which disappear once you control for (the confounding variable) income. The underlying issue appears to be the fact that women, generally earn less than men. If the lender was to enforce independence between gender and its loan approval rate by, for example, setting lower income requirements for women than men, this could conceivably lead to higher default rates among women. Clearly a less than desirable solution which, arguably, doesn’t address the underlying problem. Furthermore, it might be argued that satisfying independence could lead to less fair outcomes from a different perspective; that a man and woman who were the same in all other respects (features) would receive different outcomes. In the next chapter we’ll talk about individual fairness which reconciles these differences in perspective by requiring the specification of a task specific similarity metric for individuals.</p>
<p>It is important to note that the assumption of independence does not allow for confounding variables (discussed in section <a href="#sec_SimpsParadox" data-reference-type="ref" data-reference="sec_SimpsParadox">1.4</a>). Suppose we want to measure the relationship between the sensitive feature and outcome using one of the above metrics. A natural solution to the problem of confounding variables, is to control for them (assuming you have them recorded in your dataset and your data is representative of the population). Next, we consider the case where we condition on all the non-sensitive variables <span class="math inline">\(X\)</span>.</p>
</section>
</section>
<section id="sec_CondIndep" class="level3" data-number="3.1.2">
<h3 data-number="3.1.2"><span class="header-section-number">3.1.2</span> The Twin Test</h3>
<p>The twin test tries to establish cause (of differing treatment across protected groups), by conditioning on all other features. Because of this, it has been interpreted as avoiding disparate treatment<span class="citation" data-cites="DispMistreat"><a href="#ref-DispMistreat" role="doc-biblioref">[49]</a></span>. While legally speaking such proof is not required to establish liability (as discussed in section <a href="#sec_AppLaw" data-reference-type="ref" data-reference="sec_AppLaw">1.3.2</a>), the twin test provides a useful tool for evaluating feature specific discrimination in models. In this case, our fairness criterion requires the predicted target variable to be independent of the sensitive features when conditioned on all other features. This is true, if and only if, the probability distribution of <span class="math inline">\(Y\)</span> conditioned on <span class="math inline">\(X\)</span> is the same, for all values of the sensitive feature <span class="math inline">\(Z\)</span>; <span class="math display">\[\hat{Y} \bot Z | X \quad \Leftrightarrow \quad
f_{\hat{Y}|X}(\hat{y}, z; x) = f_{\hat{Y}|X}(\hat{y}; x).\]</span></p>
<p>Suppose we wish to establish a causal connection between the decision or outcome and an individual’s membership in some protected group. Typically, in a human decision process which is subjective, proving a causal connection difficult (a problem addressed by judicial systems). In the case where a decision is made purely on the basis of a deterministic algorithm (which one has access to and need only be in the form of a black box), making this causal connection is easier. Imagine a ‘counterfactual’ world in which for every individual in this world (say John Doe) there exists an identical twin in the counterfactual world which differers only by the sensitive feature (say Jane Doe). If a deterministic model produces predictions that are different for for John and Jane, we have established the individual’s sensitive feature as the reason.</p>
<p>With this approach, establishing cause with a model becomes straight forward. We conduct a randomized experiment, sampling for <span class="math inline">\(X\)</span>. The individuals for which we check the model output need not exist, we can simply fabricate them, and compare the target distributions. What if the model is not deterministic but rather makes randomised predictions for a given <span class="math inline">\(X\)</span>? This makes things a bit more complicated because neither John nor Jane Doe get the same model prediction at every trial, so it’s not enough to check the outcome for a single John and Jane for each Doe. This makes the test computationally more expensive. We need to compute the target <span class="math inline">\(\hat{Y}\)</span> a sufficiently large number of times to obtain the distribution for each value of <span class="math inline">\(X\)</span>. For a dataset, the twin test is less reliable. Without access to the (potentially human and thus non-deterministic) algorithm by which it was produced, we have no way of producing counterfactual twins that don’t exist, making sample size a potential issue.</p>
<p>The counterfactual approach to establishing the fairness of our model, we can consider all independence metrics described above have natural extensions which are conditioned on <span class="math inline">\(X\)</span> as well as <span class="math inline">\(Z\)</span>. So for example we define the <strong>causal mean difference</strong> as <span class="math display">\[\mathbb{E}(\hat{Y} | Z=1, X=x) - \mathbb{E}(\hat{Y} | Z=0, X=x).\]</span> and the <strong>observed mean difference</strong> as <span class="math display">\[\mathbb{E}(Y | Z=1, X=x) - \mathbb{E}(Y | Z=0, X=x).\]</span></p>
</section>
</section>
<section id="sec_BalErr" class="level2" data-number="3.2">
<h2 data-number="3.2"><span class="header-section-number">3.2</span> Comparing Errors</h2>
<p>In this section we learn about fairness criteria which compare model errors across groups, rather than outcomes. A fundamental assumption here is that the training data is fair and just and that there exists a ground truth to compare our model to. We discussed earlier in the chapter how independence and twin test constraints have been interpreted as avoiding disparate impact and disparate treatment respectively. Analogously, criteria on model errors have been described as avoiding <strong>disparate mistreatment</strong><span class="citation" data-cites="DispMistreat"><a href="#ref-DispMistreat" role="doc-biblioref">[49]</a></span> in the literature.</p>
<section id="independent-errors" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1"><span class="header-section-number">3.2.1</span> Independent Errors</h3>
<p>Independence in errors (or equivalently predictive performance), is the next strongest fairness criterion after independence, <span class="math inline">\((\hat{Y}-Y) \bot Z\)</span>.</p>
<p>A relaxation of this criterion compares only the mean error <span class="math inline">\(\mathbb{E}(\hat{Y} - Y)\)</span> for the groups (rather than the full distributions). This essentially tells us if essentially if our model is over or underestimating the target <span class="math inline">\(Y\)</span> (or score <span class="math inline">\(P\)</span> for classification) on average. For classification problems it provides a measure of the number of false positives compared to false negatives. If <span class="math inline">\(\mu\)</span> is positive there are more false positives and vice versa. Another way to look at the mean error is a measure of luck or opportunity. Depending on whether it is preferable to have a model under or overestimate <span class="math inline">\(Y\)</span> determines which direction is lucky (given greater opportunity) versus unlucky. For regression models <strong>Balanced residuals</strong><span class="citation" data-cites="BalRes"><a href="#ref-BalRes" role="doc-biblioref">[55]</a></span><span class="marginnote"><span id="ref-BalRes" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[55] </span><span class="csl-right-inline">T. Calders, A. Karim, F. Kamiran, W. Ali, and X. Zhang, <span>“Controlling attribute effect in linear regression,”</span> 2013. doi: <a href="https://doi.org/10.1109/ICDM.2013.114">10.1109/ICDM.2013.114</a>.</span>
</span>
</span> takes the difference of the mean errors, <span class="math display">\[d_{\text{err}} = \mathbb{E}(\hat{Y} - Y | Z=1) - \mathbb{E}(\hat{Y} - Y | Z=0).\]</span> This can be calculated for <span class="math inline">\(n=n_0+n_1\)</span> data points as, <span class="math display">\[d_{\text{err}} = \frac{1}{n_1}\sum_{i|z_i=1}(\hat{y}_i-y_i).
               - \frac{1}{n_0}\sum_{i|z_i=0}(\hat{y}_i-y_i)\]</span></p>
<p>For a classification problem a relaxation of this criterion compares only the error rates (or equivalently accuracy) for all groups. The direction of the error is effectively assumed to be inconsequential. To derive a measure of fairness from this criterion we could (as before) take the difference, or the ratio. The <strong>error rate difference</strong> is given by, <span class="math display">\[\mathbb{P}(\hat{Y}\neq Y | Z=1) - \mathbb{P}(\hat{Y}\neq Y | Z=0).\]</span> The <strong>error rate ratio</strong> is given by <span class="math display">\[\frac{\mathbb{P}(\hat{Y}\neq Y | Z=0)}{\mathbb{P}(\hat{Y}\neq Y | Z=1)}.\]</span> For a binary classifier, false positives and false negatives will typically have different implications and associated costs which are ignored when comparing error rates. Table <a href="#tbl:CMErrMetrics" data-reference-type="ref" data-reference="tbl:CMErrMetrics">3.2</a> summarises terminology for the different types of error rates for a binary classification model. Table <a href="#tbl:CMPerfMetrics" data-reference-type="ref" data-reference="tbl:CMPerfMetrics">3.3</a> summarises terminology for the equivalent predictive performance metrics.</p>
<div id="tbl:CMErrMetrics">
<table>
<caption>Table 3.2: Summary of error rate metrics for a binary classifier</caption>
<thead>
<tr class="header">
<th colspan="2" style="text-align: center;"></th>
<th colspan="2" style="text-align: center;">Ground Truth</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td colspan="2" style="text-align: center;"></td>
<td style="text-align: center;"><span class="math inline">\(y=1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(y=0\)</span></td>
<td style="text-align: center;">Error Rate Type</td>
</tr>
<tr class="even">
<td rowspan="4" style="text-align: center;">Prediction</td>
<td rowspan="2" style="text-align: center;"><span class="math inline">\(\hat{y}=1\)</span></td>
<td rowspan="2" style="text-align: center;">True Positive</td>
<td rowspan="2" style="text-align: center;">False Positive<br>Type I Error</td>
<td rowspan="2" style="text-align: center;">False Discovery Rate<br><span class="math inline">\(\mathbb{P}(\hat{y}\neq y|\hat{y}=1)\)</span></td>
</tr>
<tr class="odd">
</tr>
<tr class="even">
<td rowspan="2" style="text-align: center;"><span class="math inline">\(\hat{y}=0\)</span></td>
<td rowspan="2" style="text-align: center;">False Negative<br>Type II Error</td>
<td rowspan="2" style="text-align: center;">True Negative</td>
<td rowspan="2" style="text-align: center;">False Omission Rate<br><span class="math inline">\(\mathbb{P}(\hat{y}\neq y|\hat{y}=0)\)</span></td>
</tr>
<tr class="odd">
</tr>
<tr class="even">
<td colspan="2" rowspan="2" style="text-align: center;">Error Rate Type</td>
<td rowspan="2" style="text-align: center;">False Negative Rate<br><span class="math inline">\(\mathbb{P}(\hat{y}\neq y|y=1)\)</span></td>
<td rowspan="2" style="text-align: center;">False Positive Rate<br><span class="math inline">\(\mathbb{P}(\hat{y}\neq y|y=0)\)</span></td>
<td rowspan="2" style="text-align: center;">Error Rate<br><span class="math inline">\(\mathbb{P}(\hat{y}\neq y)\)</span></td>
</tr>
<tr class="odd">
</tr>
</tbody>
</table>
</div>
<div id="tbl:CMPerfMetrics">
<table>
<caption>Table 3.3: Summary of predictive performance metrics for a binary classifier</caption>
<thead>
<tr class="header">
<th colspan="2" style="text-align: center;"></th>
<th colspan="2" style="text-align: center;">Ground Truth</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td colspan="2" style="text-align: center;"></td>
<td style="text-align: center;"><span class="math inline">\(y=1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(y=0\)</span></td>
<td style="text-align: center;">Metric</td>
</tr>
<tr class="even">
<td rowspan="4" style="text-align: center;">Prediction</td>
<td rowspan="2" style="text-align: center;"><span class="math inline">\(\hat{y}=1\)</span></td>
<td rowspan="2" style="text-align: center;">True Positive</td>
<td rowspan="2" style="text-align: center;">False Positive<br>Type I Error</td>
<td rowspan="2" style="text-align: center;">Positive Predictive Value<sup>a</sup><br><span class="math inline">\(\mathbb{P}(\hat{y}=y|\hat{y}=1)\)</span></td>
</tr>
<tr class="odd">
</tr>
<tr class="even">
<td rowspan="2" style="text-align: center;"><span class="math inline">\(\hat{y}=0\)</span></td>
<td rowspan="2" style="text-align: center;">False Negative<br>Type II Error</td>
<td rowspan="2" style="text-align: center;">True Negative</td>
<td rowspan="2" style="text-align: center;">Negative Predictive Value<br><span class="math inline">\(\mathbb{P}(\hat{y}=y|\hat{y}=0)\)</span></td>
</tr>
<tr class="odd">
</tr>
<tr class="even">
<td colspan="2" rowspan="2" style="text-align: center;">Metric</td>
<td rowspan="2" style="text-align: center;">True Positive Rate<sup>b</sup><br><span class="math inline">\(\mathbb{P}(\hat{y}=y|y=1)\)</span></td>
<td rowspan="2" style="text-align: center;">True Negative Rate<br><span class="math inline">\(\mathbb{P}(\hat{y}=y|y=0)\)</span></td>
<td rowspan="2" style="text-align: center;">Accuracy<br><span class="math inline">\(\mathbb{P}(\hat{y}=y)\)</span></td>
</tr>
<tr class="odd">
</tr>
</tbody>
</table>
</div>
<div class="tablenotes">
<p><sup>a</sup> Positive Predictive Value = Precision</p>
<p><sup>b</sup> True Positive Rate = Recall</p>
</div>
<p>Fairness criteria that compare error distributions (or equivalently predictive performance metrics) across groups can be broken down into conditional independence constraints on the joint distributions of the sensitive features, <span class="math inline">\(Z\)</span>, the target feature <span class="math inline">\(Y\)</span> and predicted target <span class="math inline">\(\hat{Y}\)</span>. <em>Separation</em> conditions on <span class="math inline">\(Y\)</span> (the columns of the confusion matrix) requiring the false negative and false positive (or equivalently the true positive and true negative) rates to be independent of protected group membership. <em>Sufficiency</em> conditions on <span class="math inline">\(\hat{Y}\)</span> (the rows of the confusion matrix) requiring the false discovery and false omission (or equivalently positive predictive value and negative predictive value) rates to be independent of protected group membership. Let’s take a closer look at them.</p>
</section>
<section id="separation" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2"><span class="header-section-number">3.2.2</span> Separation</h3>
<p>Separation requires the predicted target variable to be independent of the sensitive feature, conditioned on the target variable, that is, <span class="math inline">\(\hat{Y} \bot (Z|Y)\)</span>. We can say that the predicted target <span class="math inline">\(\hat{Y}\)</span>, is <em>separated</em> from the sensitive feature <span class="math inline">\(Z\)</span>, by the target variable <span class="math inline">\(Y\)</span>. The corresponding graphical model for separation criteria is shown in Figure <a href="#fig:separation" data-reference-type="ref" data-reference="fig:separation">3.2</a>.</p>
<figure>
<img src="03_GroupFairness/figures/Fig_separation.png" id="fig:separation" style="width:50.0%" alt="Figure 3.2: Graphical model for separation." />
<figcaption aria-hidden="true">Figure 3.2: Graphical model for separation.</figcaption>
</figure>
<p>So, for a fixed value of the target variable, there should be no difference in the distribution of the predicted target variable, across different values of the sensitive feature. That is, <span class="math display">\[\mathbb{P}(\hat{y}|y, z) = \mathbb{P}(\hat{y}|y).\]</span> Unlike independence, separation, allows for dependence between the predicted target variable and the sensitive feature but only to the extent that it exists between the actual target variable and the sensitive feature.</p>
<p>For a binary classifier where we have a single sensitive binary feature. We can write this requirement (most well known as <strong>equalised odds</strong><span class="citation" data-cites="EqOfOp"><a href="#ref-EqOfOp" role="doc-biblioref">[56]</a></span><span class="marginnote"><span id="ref-EqOfOp" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[56] </span><span class="csl-right-inline">M. Hardt, E. Price, and N. Srebro, <span>“Equality of opportunity in supervised learning.”</span> 2016.Available: <a href="https://arxiv.org/abs/1610.02413">https://arxiv.org/abs/1610.02413</a></span>
</span>
</span>) as two conditions, <span class="math display">\[\begin{aligned}
\mathbb{P}(\hat{Y}=1 | Z=1, Y=1) &amp; = \mathbb{P}(\hat{Y}=1 | Z=0, Y=1), \\
\mathbb{P}(\hat{Y}=1 | Z=1, Y=0) &amp; = \mathbb{P}(\hat{Y}=1 | Z=0, Y=0).
\end{aligned}\]</span> Recall that <span class="math inline">\(\mathbb{P}(\hat{Y}=1 | Y=1)\)</span> is the true positive rate (<span class="math inline">\(TPR\)</span>) of the classifier and <span class="math inline">\(\mathbb{P}(\hat{Y}=1 | Y=0)\)</span> is the false positive rate (<span class="math inline">\(FPR\)</span>). We see then that separation requires the true positive rate, and the false positive rate, to be the same for all values of the sensitive feature.</p>
<p>Let’s think about what this means in the context of a recidivism risk predictor used in sentencing. Separation requires the proportion of (false positive) errors among those that did not in fact reoffend, and (false negative) errors among those that did to be the same across protected groups. This was essentially Propublica’s criticism of COMPAS, that the model overestimated the risk presented by Black defendants (demonstrated by their higher false positive rate) and underestimated the risk presented by White defendants (demonstrated by their higher false negative rate).</p>
<p>Two related metrics are the average odds difference and average odds error. The <strong>average odds difference</strong> measures the magnitude of unfairness as the average of the difference in true positive rate and false positive rate, <span class="math display">\[\frac{1}{2}
[ TPR_{Z=0} - TPR_{Z=1} + FPR_{Z=0} - FPR_{Z=1} ].\]</span> The <strong>average odds error</strong> measures the magnitude of unfairness as the average of the absolute difference in true positive rate and false positive rate, <span class="math display">\[\frac{1}{2}
[ |TPR_{Z=0} - TPR_{Z=1}| + |FPR_{Z=0} - FPR_{Z=1}| ].\]</span></p>
<p>A relaxed version of equalised odds, called <strong>equal opportunity</strong><span class="citation" data-cites="EqOfOp"><a href="#ref-EqOfOp" role="doc-biblioref">[56]</a></span>, requires only the true positive rates to be the same across all groups, assuming a positive prediction is the advantageous or lucky outcome<span class="sidenote-wrapper"><label for="sn-6" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-6" class="margin-toggle"/><span class="sidenote">Recall Rawl’s second principle of justice as fairness (fair equality of opportunity) discussed in section <a href="#sec_FairnessJustice" data-reference-type="ref" data-reference="sec_FairnessJustice">1.2</a>.<br />
<br />
</span></span>. If the reverse is true (i.e. the negative prediction is the advantageous outcome), we would instead want the true negative rates to be equal. For our recidivism risk predictor, this would mean ensuring that for defendants which did not reoffend have the same probability of being flagged low risk. Said another way, we want defendants that were in fact low risk to be given equal opportunity to be marked low risk across protected groups. A metric which uses this as a criterion to measure unfairness is <strong>equal opportunity difference</strong> which takes the difference in true positive rates across groups, that is, <span class="math display">\[TPR_{Z=0} - TPR_{Z=1}.\]</span></p>
</section>
<section id="sufficiency" class="level3" data-number="3.2.3">
<h3 data-number="3.2.3"><span class="header-section-number">3.2.3</span> Sufficiency</h3>
<p><em>Sufficiency</em> requires the sensitive feature <span class="math inline">\(Z\)</span> and target variable <span class="math inline">\(Y\)</span> to be independent, conditional on the predicted target variable <span class="math inline">\(\hat{Y}\)</span>, that is, <span class="math inline">\(Y \bot (Z|\hat{Y})\)</span>. We can say that the predicted target <span class="math inline">\(\hat{Y}\)</span> is <em>sufficient</em> for the sensitive feature <span class="math inline">\(Z\)</span>. That is to say, given <span class="math inline">\(\hat{Y}\)</span>, <span class="math inline">\(Z\)</span> provides no additional information. The corresponding graphical model for sufficiency criteria is shown in Figure <a href="#fig:sufficiency" data-reference-type="ref" data-reference="fig:sufficiency">3.3</a>.</p>
<figure>
<img src="03_GroupFairness/figures/Fig_sufficiency.png" id="fig:sufficiency" style="width:50.0%" alt="Figure 3.3: Graphical model for sufficiency." />
<figcaption aria-hidden="true">Figure 3.3: Graphical model for sufficiency.</figcaption>
</figure>
<p>It should hopefully be straightforward to see that sufficiency requires the false omission rate and false discovery rate (see Table <a href="#tbl:CMErrMetrics" data-reference-type="ref" data-reference="tbl:CMErrMetrics">3.2</a>) to be equal across protected groups.</p>
<div class="lookbox">
<p><strong>Sufficiency</strong></p>
<p>Sufficiency is satisfied if and only if the false omission rate and false discovery rate are equal for all groups. <a href="#GF_Suff">Proof</a> in appendix <a href="#sec_app_GFSolutions" data-reference-type="ref" data-reference="sec_app_GFSolutions">D.1</a>.</p>
</div>
<p>Sufficiency requires the probability of of an error for a given prediction to be the same across protected groups. Let’s think about what this means for our binary recidivism risk calculator. Sufficiency requires that for a given prediction (high/low risk), the probability of error (predicting high risk for those that did not reoffend/predicting low risk for those that did) is independent of protected group membership.</p>
<p>Comparing sufficiency to separation we note that <span class="math inline">\(Y\)</span> and <span class="math inline">\(\hat{Y}\)</span> are reversed in the graphical models (and conditional independence constraints). In the graphical model for separation, the data is upstream of the model output; for sufficiency, we assume the model is upstream of the data. In reality of course, the world is more complicated. In the previous chapter we discussed the machine learning cycle - specifically the fact that including a model in the decision making process impacts future data, which, when used to retrain our model, creates feedback loops. In imposing both separation and sufficiency (by requiring independent errors), we accept that our causal model is more complicated - like that shown in Figure <a href="#fig:SepSuf" data-reference-type="ref" data-reference="fig:SepSuf">3.4</a> a).</p>
<figure>
<img src="03_GroupFairness/figures/Fig_SepSuf.png" id="fig:SepSuf" style="width:90.0%" alt="Figure 3.4: Alternative causal models." />
<figcaption aria-hidden="true">Figure 3.4: Alternative causal models.</figcaption>
</figure>
<p>We tend to think of sensitive features as immutable facts, but in reality they are not. Over time, we can expect a progressive society to become more inclusive, recognising more subgroups that previously did not exist. If we accept that environmental factors, and even our target or prediction, can affect the sensitive categories we fall into, the graph then perhaps begins to look more like Figure <a href="#fig:SepSuf" data-reference-type="ref" data-reference="fig:SepSuf">3.4</a> b), at which point, there’s little value to be found in graphical models, because everything is connected. But it’s worth realising just how much more complicated reality typically is, especially when decisions that can shape lives are at stake.</p>
<section id="sufficiency-and-calibration-by-group" class="level4 unnumbered">
<h4 class="unnumbered">Sufficiency and Calibration by Group</h4>
<p>As one might expect, satisfying separation or sufficiency does not require as great a sacrifice of utility as independence (assuming <span class="math inline">\(Y\not\perp Z\)</span>). Neither separation nor sufficiency are necessarily satisfied by the utility optimal solution. Of the two sufficiency, imposes a weaker constraint on our model. To understand why, we explore another interpretation of sufficiency which intuitively explains why it might be satisfied implicitly through the training process<span class="citation" data-cites="ImplicitFairness"><a href="#ref-ImplicitFairness" role="doc-biblioref">[57]</a></span><span class="marginnote"><span id="ref-ImplicitFairness" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[57] </span><span class="csl-right-inline">L. T. Liu, M. Simchowitz, and M. Hardt, <span>“The implicit fairness criterion of unconstrained learning.”</span> 2019.Available: <a href="https://arxiv.org/abs/1808.10013">https://arxiv.org/abs/1808.10013</a></span>
</span>
</span>. Let us look at sufficiency criteria in terms of the classification score <span class="math inline">\(P\)</span>, <span class="math display">\[\mathbb{P}(Y=1 | P=p, Z=1) = \mathbb{P}(Y=1 | P=p, Z=0) \quad \forall \, p\]</span> We say that a classifier score is calibrated if <span class="math display">\[\mathbb{P}(Y=1 | P=p) = p \quad \forall \, p.\]</span> Essentially, this is the requirement that the proportion of data points assigned the score <span class="math inline">\(p\)</span>, which did in fact have a positive outcome <span class="math inline">\(Y=1\)</span>, should be equal to the score <span class="math inline">\(p\)</span>. The score <span class="math inline">\(p\)</span> can then be interpreted, at the population level, as the probability that the a positive prediction <span class="math inline">\(\hat{Y}=1\)</span> would be correct<span class="sidenote-wrapper"><label for="sn-7" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-7" class="margin-toggle"/><span class="sidenote">For the score to be interpretable as this probability at the individual level, we would need to satisfy the stronger criteria <span class="math inline">\(P=\mathbb{E}[Y|X]\)</span>.<br />
<br />
</span></span>.</p>
<p>From the definitions above we can see that if our classifier scores are calibrated for all groups, sufficiency is automatically satisfied. If our model satisfies sufficiency but not calibration by group, we can calibrate our model score through a simple transformation. We simply pick a value for <span class="math inline">\(Z\)</span>, <span class="math inline">\(Z=1\)</span> say, and then calculate the mapping, <span class="math display">\[\mathbb{P}(Y=1|P=p, Z=1) = f(p).\]</span> We then transform all our scores to new scores (which satisfy calibration by group) by applying the inverse mapping <span class="math inline">\(f^{-1}(P)\)</span>. The resulting model is both sufficient and calibrated. It’s worth noting that the developers of COMPAS were able to demonstrate that their model did satisfy calibration by group. In a later review, researchers crowd sourced human risk assessors via Amazon Mechanical Turk it was found that COMPAS was "no more accurate or fair than predictions made by people with little or no criminal justice expertise. In addition, despite COMPAS’s collection of 137 features, the same accuracy can be achieved with a simple linear classifier with only two features"<span class="citation" data-cites="COMPAS-SciAdv"><a href="#ref-COMPAS-SciAdv" role="doc-biblioref">[58]</a></span><span class="marginnote"><span id="ref-COMPAS-SciAdv" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[58] </span><span class="csl-right-inline">J. Dressel and H. Farid, <span>“The accuracy, fairness, and limits of predicting recidivism,”</span> <em>Science Advances</em>, vol. 4, no. 1, p. eaao5580, 2018, doi: <a href="https://doi.org/10.1126/sciadv.aao5580">10.1126/sciadv.aao5580</a>.</span>
</span>
</span>.</p>
<p>There are some obvious advantages of comparing errors rather than outcomes. Note that unlike criteria comparing outcomes they do not preclude the theoretically perfect solution, <span class="math inline">\(\hat{Y}=Y\)</span>. The criteria also preclude large differences in error rates for different groups that are typical when disadvantaged classes are minorities suffering from low support. It’s worth reiterating that criteria comparing errors assume that the relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(Z\)</span> prescribed by the training data is fair. Depending on the context of the problem one might prioritise equalising one type of error over another. For example, in pretrial risk assessment we might choose to prioritise ensuring equal false positive rates if we believe that it is preferable to set free a guilty defendant than incarcerate an innocent one. As another example, let’s take the infamous <a href="https://en.wikipedia.org/wiki/Stop-and-frisk_in_New_York_City">NYPD stop-and-frisk program</a> where pedestrians were stopped, interrogated and searched on ‘reasonable’ suspicion of carrying contraband. In this case we might want to ensure false discovery rates are equal across groups to ensure we are not disproportionately targeting particular minority groups.</p>
</section>
</section>
</section>
<section id="sec_Impossible" class="level2" data-number="3.3">
<h2 data-number="3.3"><span class="header-section-number">3.3</span> Incompatibility Between Fairness Criteria</h2>
<p>So far in this chapter we have learned a range of different group fairness criteria and seen how each of them can be viewed as imposing different constraints on the joint distributions of our variables <span class="math inline">\(X\)</span>, <span class="math inline">\(Z\)</span>, <span class="math inline">\(Y\)</span> and <span class="math inline">\(\hat{Y}\)</span>. In this section we will prove that these fairness criteria can be restrictive enough to mean that satisfying more than one of them is impossible, except in some degenerate cases. For a useful recap of the rules of probability (which we will use in our proofs), see in Appendix <a href="#app_ProbRules" data-reference-type="ref" data-reference="app_ProbRules">C</a>.</p>
<section id="independence-versus-sufficiency" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1"><span class="header-section-number">3.3.1</span> Independence versus Sufficiency</h3>
<div class="lookbox">
<p><strong>Independence versus Sufficiency</strong></p>
<p>Independence (<span class="math inline">\(Z \bot \hat{Y}\)</span>) and sufficiency (<span class="math inline">\(Z \bot Y | \hat{Y}\)</span>) can only be simultaneously satisfied if the sensitive feature, <span class="math inline">\(Z\)</span> and the target variable <span class="math inline">\(Y\)</span> are independent (<span class="math inline">\(Z \bot Y\)</span>).</p>
</div>
<p>To prove this we consider the conditional distribution <span class="math inline">\(Z|Y,\hat{Y}\)</span>.</p>
<div id="eq:IndSuf1">
<table>
<colgroup>
<col style="width: 90%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[\begin{aligned}
\textrm{Independence: } Z \bot \hat{Y} \quad
&amp; \Rightarrow\quad \mathbb{P}(z|y,\hat{y}) = \mathbb{P}(z|y) \nonumber\\
\textrm{Product rule} \quad
&amp; \Rightarrow\quad \mathbb{P}(z|y) = \frac{\mathbb{P}(z,y)}{\mathbb{P}(y)}\nonumber\\
&amp; \Rightarrow\quad \mathbb{P}(z|y,\hat{y}) = \frac{\mathbb{P}(z,y)}{\mathbb{P}(y)}.
\end{aligned}\]</span></td>
<td style="text-align: right;">(3.6)</td>
</tr>
</tbody>
</table>
</div>
<p>Applying Sufficiency, followed by independence gives,</p>
<div id="eq:IndSuf2">
<table>
<colgroup>
<col style="width: 90%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[\begin{aligned}
\textrm{Sufficiency: } Z \bot Y | \hat{Y} \quad
&amp; \Rightarrow\quad \mathbb{P}(z|y,\hat{y}) = \mathbb{P}(z|\hat{y})\nonumber\\
\textrm{Independence: } Z \bot \hat{Y} \quad
&amp; \Rightarrow\quad \mathbb{P}(z|\hat{y}) = \mathbb{P}(z)\nonumber\\
&amp; \Rightarrow\quad \mathbb{P}(z|y,\hat{y}) = \mathbb{P}(z).
\end{aligned}\]</span></td>
<td style="text-align: right;">(3.7)</td>
</tr>
</tbody>
</table>
</div>
<p>Equating (3.6) and (3.7) and rearranging gives, <span class="math display">\[\mathbb{P}(z,y) = \mathbb{P}(z)\mathbb{P}(y).\]</span> Thus, <span class="math inline">\(Z\)</span> and <span class="math inline">\(Y\)</span> must be independent.</p>
</section>
<section id="independence-versus-separation" class="level3" data-number="3.3.2">
<h3 data-number="3.3.2"><span class="header-section-number">3.3.2</span> Independence versus Separation</h3>
<div class="lookbox">
<p><strong>Independence versus Separation</strong></p>
<p>In the case that <span class="math inline">\(Y\)</span> is binary, independence (<span class="math inline">\(Z \bot \hat{Y}\)</span>) and separation (<span class="math inline">\(Z \bot \hat{Y} | Y\)</span>) criteria can only be simultaneously satisfied if either <span class="math inline">\(\hat{Y} \bot Y\)</span> or <span class="math inline">\(Y \bot Z\)</span>.</p>
</div>
<p>To prove this we consider the distribution of <span class="math inline">\(\hat{Y}\)</span>.</p>
<div id="eq:IndSep1">
<table>
<colgroup>
<col style="width: 90%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[\begin{aligned}
\textrm{Sum rule:} \quad &amp; \Rightarrow \quad \mathbb{P}(\hat{y})
= \sum_{y\in\mathcal{Y}}  \mathbb{P}(\hat{y}, y).\nonumber\\
\textrm{Product rule} \quad &amp; \Rightarrow \quad \mathbb{P}(\hat{y})
= \sum_{y\in\mathcal{Y}} \mathbb{P}(\hat{y}|y) \mathbb{P}(y).
\end{aligned}\]</span></td>
<td style="text-align: right;">(3.8)</td>
</tr>
</tbody>
</table>
</div>
<div id="eq:IndSep2">
<table>
<colgroup>
<col style="width: 90%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[\begin{aligned}
\textrm{Conditioning on }Z \quad \Rightarrow \quad \mathbb{P}(\hat{y}|z)
= \sum_{y\in\mathcal{Y}} \mathbb{P}(\hat{y}|y, z) \mathbb{P}(y|z).\nonumber\\
\textrm{Independence: } \hat{Y} \bot Z \quad \Rightarrow \quad \mathbb{P}(\hat{y})
= \sum_{y\in\mathcal{Y}} \mathbb{P}(\hat{y}|y) \mathbb{P}(y|z).
\end{aligned}\]</span></td>
<td style="text-align: right;">(3.9)</td>
</tr>
</tbody>
</table>
</div>
<p>Equating (3.8) and (3.9) and rearranging gives,</p>
<div id="eq:IndSep3">
<table>
<colgroup>
<col style="width: 90%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
\sum_{y\in\mathcal{Y}} \mathbb{P}(\hat{y}|y) [\mathbb{P}(y)-\mathbb{P}(y|z)] = 0\]</span></td>
<td style="text-align: right;">(3.10)</td>
</tr>
</tbody>
</table>
</div>
<p>For binary <span class="math inline">\(Y\)</span>, <span class="math inline">\(\mathcal{Y}=\{0,1\}\)</span>. Denoting <span class="math inline">\(\mathbb{P}(y)=p_y\)</span> and <span class="math inline">\(\mathbb{P}(y|z) = q_y\)</span>, then <span class="math inline">\(p_1 = 1-p_0\)</span> and <span class="math inline">\(q_1 = 1-q_0\)</span>. Substituting into (3.10) gives, <span class="math display">\[\begin{aligned}
&amp; \phantom{[}\mathbb{P}(\hat{y}|Y=0)(p_0-q_0)+\mathbb{P}(\hat{y}|Y=1)[1-p_0-(1-q_0)] = 0 \\
\Leftrightarrow \quad &amp; [\mathbb{P}(\hat{y}|Y=0)-\mathbb{P}(\hat{y}|Y=1)](p_0-q_0) = 0
\end{aligned}\]</span> which is true if and only if, <span class="math display">\[\begin{aligned}
&amp;\textrm{either }
  &amp; \mathbb{P}(\hat{y}|Y=0) = \mathbb{P}(\hat{y}|Y=1) \quad
    &amp; \Leftrightarrow \quad \hat{Y} \bot Y,\\
&amp; \textrm{or }
  &amp; p_0=q_0 \quad \Leftrightarrow \quad \mathbb{P}(Y=0) = \mathbb{P}(Y=0|z) \quad
    &amp; \Leftrightarrow \quad Y \bot Z.
\end{aligned}\]</span></p>
</section>
<section id="separation-versus-sufficiency" class="level3" data-number="3.3.3">
<h3 data-number="3.3.3"><span class="header-section-number">3.3.3</span> Separation versus Sufficiency</h3>
<div class="lookbox">
<p><strong>Separation versus Sufficiency I</strong></p>
<p>In the case where all events in the joint distribution of <span class="math inline">\(Z\)</span>, <span class="math inline">\(Y\)</span> and <span class="math inline">\(\hat{Y}\)</span> have non zero probability, separation (<span class="math inline">\(Z \bot \hat{Y} | Y\)</span>) and sufficiency (<span class="math inline">\(Z \bot Y | \hat{Y}\)</span>) can only be simultaneously be satisfied if the sensitive feature, <span class="math inline">\(Z\)</span> is independent of both the target variable <span class="math inline">\(Y\)</span> and the predicted target <span class="math inline">\(\hat{Y}\)</span>, that is if <span class="math inline">\(Z \bot Y\)</span> and <span class="math inline">\(Z \bot \hat{Y}\)</span>.</p>
</div>
<p>To prove this we consider the conditional distribution <span class="math inline">\(\mathbb{P}(z|y,\hat{y})\)</span>.</p>
<div id="eq:SepSuf1">
<table>
<colgroup>
<col style="width: 90%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[\begin{aligned}
\textrm{Separation: } Z \bot \hat{Y} | Y \quad
&amp; \Rightarrow \quad \mathbb{P}(z|y,\hat{y}) = \mathbb{P}(z|y) \nonumber\\
\textrm{Sufficiency: } Z \bot Y | \hat{Y} \quad
&amp; \Rightarrow \quad \mathbb{P}(z|y,\hat{y}) = \mathbb{P}(z|\hat{y}) \nonumber\\
&amp; \Rightarrow \quad \mathbb{P}(z|y) = \mathbb{P}(z|\hat{y}).
\end{aligned}\]</span></td>
<td style="text-align: right;">(3.11)</td>
</tr>
</tbody>
</table>
</div>
<div id="eq:SepSuf2">
<table>
<colgroup>
<col style="width: 90%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[\begin{aligned}
\textrm{Product rule: } \quad\phantom{\Rightarrow} \mathbb{P}(z,y) &amp; = \mathbb{P}(z|y) \mathbb{P}(y)\nonumber\\
(3.11) \qquad\quad \Rightarrow \quad \mathbb{P}(z,y) &amp; = \mathbb{P}(z|\hat{y}) \mathbb{P}(y).
\end{aligned}\]</span></td>
<td style="text-align: right;">(3.12)</td>
</tr>
</tbody>
</table>
</div>
<p><span class="math display">\[\begin{aligned}
\textrm{Sum rule: } \quad \phantom{\Rightarrow}\mathbb{P}(z) &amp; = \sum_{y\in\mathcal{Y}} \mathbb{P}(z,y)\\
(3.12) \quad\,\, \Rightarrow \quad
\mathbb{P}(z) &amp; = \sum_{y\in\mathcal{Y}} \mathbb{P}(z|\hat{y}) \mathbb{P}(y)
\end{aligned}\]</span> If all events have non-zero probability, we can move <span class="math inline">\(\mathbb{P}(z|\hat{y})\)</span> outside of the summation,</p>
<div id="eq:SepSuf3">
<table>
<colgroup>
<col style="width: 90%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
\mathbb{P}(z) = \mathbb{P}(z|\hat{y})\]</span></td>
<td style="text-align: right;">(3.13)</td>
</tr>
</tbody>
</table>
</div>
<p>Thus showing that <span class="math inline">\(Z\)</span> and <span class="math inline">\(\hat{Y}\)</span> must be independent. Equating (3.11) and (3.13) shows that <span class="math inline">\(Z\)</span> and <span class="math inline">\(Y\)</span> must also be independent.</p>
<div class="lookbox">
<p><strong>Separation versus Sufficiency II</strong></p>
<p>In the case where <span class="math inline">\(Y\)</span> is binary, separation and sufficiency can only be satisfied simultaneously if the sensitive feature is independent of the target variable, or the model has an accuracy of 100% (<span class="math inline">\(\hat{Y}=Y\)</span>) or 0% (<span class="math inline">\(\hat{Y}=1-Y\)</span>).</p>
</div>
<p>Consider the case where <span class="math inline">\(Y\)</span> is binary. Separation requires all groups to have the same true positive rate (recall or <span class="math inline">\(TPR\)</span>) and the same false positive rate (<span class="math inline">\(FPR\)</span>). On the other hand, sufficiency requires all groups to have the same positive predictive value (precision or <span class="math inline">\(PPV\)</span>) and the same negative predictive value (<span class="math inline">\(NPV\)</span>). A problem is evident at this point. For a fixed number of data points, the confusion matrix for a binary classifier only has three degrees of freedom but satisfying both separation and sufficiency introduces four constraints which requires four degrees of freedom in order be able to satisfy them. We can write the positive and negative predictive values in terms of the true positive and false positive rates.</p>
<div class="lookbox">
<p><strong>Predictive Values</strong></p>
<p>We can write the positive and negative predictive values in terms of the true and false positive rates as follows,</p>
<div id="eq:PPV">
<table>
<colgroup>
<col style="width: 90%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
PPV = \frac{p TPR}{p TPR + (1-p)FPR}\]</span></td>
<td style="text-align: right;">(3.14)</td>
</tr>
</tbody>
</table>
</div>
<p>and</p>
<div id="eq:NPV">
<table>
<colgroup>
<col style="width: 90%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
NPV = \frac{(1-p)(1-FPR)}{p(1-TPR) + (1-p)(1-FPR)}\]</span></td>
<td style="text-align: right;">(3.15)</td>
</tr>
</tbody>
</table>
</div>
<p>where <span class="math inline">\(p=\mathbb{P}(Y=1)\)</span>. <a href="#GF_PredVal">Proof</a> in appendix <a href="#sec_app_GFSolutions" data-reference-type="ref" data-reference="sec_app_GFSolutions">D.1</a>.</p>
</div>
<p>For separation to hold the true positive rate (<span class="math inline">\(TPR\)</span>) and false positive rate (<span class="math inline">\(FPR\)</span>) must be constant across all values of the sensitive features. For sufficiency to hold the positive predictive value (<span class="math inline">\(PPV\)</span>) and negative predictive value (<span class="math inline">\(NPV\)</span>) must be constant across all values of the sensitive features. For brevity we shall denote <span class="math inline">\(p_a=\mathbb{P}(Y=1|Z=a)\)</span>.</p>
<div class="lookbox">
<p><strong>Separation versus Sufficiency</strong></p>
<p>For separation and sufficiency to hold we must have</p>
<div id="eq:PRT1">
<table>
<colgroup>
<col style="width: 90%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
FPR (p_a-p_b) TPR = 0\]</span></td>
<td style="text-align: right;">(3.16)</td>
</tr>
</tbody>
</table>
</div>
<p>and</p>
<div id="eq:PRT2">
<table>
<colgroup>
<col style="width: 90%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
(1-FPR) (p_a-p_b) (1-TPR) = 0\]</span></td>
<td style="text-align: right;">(3.17)</td>
</tr>
</tbody>
</table>
</div>
<p>for any pair of groups <span class="math inline">\(Z=a\)</span> and <span class="math inline">\(Z=b\)</span>. <a href="#GF_SepVsSuff">Proof</a> in appendix <a href="#sec_app_GFSolutions" data-reference-type="ref" data-reference="sec_app_GFSolutions">D.1</a>.</p>
</div>
<p>Equations (3.16) and (3.17) can only be simultaneously satisfied in 3 cases:</p>
<ol>
<li><p><span class="math inline">\(p_a=p_b \, \forall \, a, b\)</span> in which case <span class="math inline">\(Y \bot Z\)</span>,</p></li>
<li><p><span class="math inline">\(FPR=0\)</span> and <span class="math inline">\(TPR=1\)</span> in which case <span class="math inline">\(Y=\hat{Y}\)</span>,</p></li>
<li><p><span class="math inline">\(FPR=1\)</span> and <span class="math inline">\(TPR=0\)</span> in which case <span class="math inline">\(Y=1-\hat{Y}\)</span>.</p></li>
</ol>
</section>
</section>
<section id="concluding-remarks" class="level2" data-number="3.4">
<h2 data-number="3.4"><span class="header-section-number">3.4</span> Concluding Remarks</h2>
<p>We’ve seen that in general, for a binary classifier, there are only a few cases in which it is possible to satisfy more than one of the three group fairness criterion simultaneously. It’s a useful exercise to summarise our findings, because this will provide some clues as to how we might go about improving our model/s of fairness. Table <a href="#tbl:Compatibility" data-reference-type="ref" data-reference="tbl:Compatibility">3.4</a> provides such a summary.</p>
<div id="tbl:Compatibility">
<table>
<caption>Table 3.4: Group fairness metrics compatibility summary.</caption>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Comparing</th>
<th style="text-align: left;">Name</th>
<th style="text-align: left;">Criterion</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Strong</td>
<td rowspan="4" style="text-align: left;"><span class="math inline">\(\displaystyle \hat{Y}\bot Z\quad
\left\{ \rule[-3.3em]{0pt}{7em} \right.\)</span></td>
<td style="text-align: left;">Outcomes</td>
<td style="text-align: left;">Independence</td>
<td style="text-align: left;"><span class="math inline">\(\displaystyle \hat{Y}\bot Z\)</span></td>
<td style="text-align: left;"></td>
<td rowspan="2" style="text-align: left;"><span class="math inline">\(\displaystyle
\left.\rule[-1.3em]{0pt}{2.8em} \right\}\quad\hat{Y}\bot Y\)</span> or <span class="math inline">\(\displaystyle |Y|&gt;|Z|\)</span></td>
</tr>
<tr class="even">
<td rowspan="2" style="text-align: center;"><span class="math inline">\(\displaystyle \Bigg\uparrow\)</span></td>
<td style="text-align: left;">Errors</td>
<td style="text-align: left;">Separation</td>
<td style="text-align: left;"><span class="math inline">\(\displaystyle \hat{Y}\bot Z|Y\)</span></td>
<td rowspan="3" style="text-align: left;"><span class="math inline">\(\displaystyle
\left.\rule[-2.5em]{0pt}{5.4em} \right\}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><span class="math inline">\(\displaystyle \hat{Y}=Y\)</span> or <span class="math inline">\(\displaystyle \hat{Y} = 1-Y\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">Weak</td>
<td style="text-align: left;">Errors</td>
<td style="text-align: left;">Sufficiency</td>
<td style="text-align: left;"><span class="math inline">\(\displaystyle Y\bot Z|\hat{Y}\)</span></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
</div>
<p>There are two types of fairness metrics, those comparing outcomes or predictions <span class="math inline">\(\hat{Y}\)</span>, and those comparing errors. We can further bisect the latter into separation and sufficiency. The criteria are ordered from strong to weak; by this we are referring to the trade-off with utility in satisfying it. Independence or statistical parity is the strongest criterion. There is a larger gap between separation and sufficiency, because separation imposes a more similar constraint to independence. We still want the sensitive feature to be independent of the prediction, but only when conditioned on the actual outcome <span class="math inline">\(Y\)</span>. Sufficiency is almost implicitly satisfied just by training or calibrating our model. All three criteria compare some joint distribution over the prediction, target and sensitive feature. The brackets either side of the table show which sets of criteria can be satisfied and how.</p>
<p>Independence and sufficiency (at the top and bottom of Table <a href="#tbl:Compatibility" data-reference-type="ref" data-reference="tbl:Compatibility">3.4</a> respectively) are the furthest apart; they can only be satisfied if the actual outcome <span class="math inline">\(Y\)</span> is independent of <span class="math inline">\(Z\)</span>. This says that all sensitive subgroups must be equally represented in both the accepted and rejected groups in the data. But if <span class="math inline">\(Y\)</span> is independent of <span class="math inline">\(Z\)</span>, we can satisfy all three criteria. It makes sense that the gold standard for fairness is representation, because fairness is aspirational.</p>
<p>Separation and sufficiency are the next closest together. If we could only satisfy two of the three criteria, these are the ones we’d choose, because together they give us independent errors. This is only possible, if the target and prediction are exactly the same, or exactly the opposite; that is, if the error is always exactly zero, or exactly one. This makes sense, because independent errors, does not prohibit the target <span class="math inline">\(Y\)</span> or prediction <span class="math inline">\(\hat{Y}\)</span> from depending on <span class="math inline">\(Z\)</span>. Rather, it’s okay for them to depend on <span class="math inline">\(Z\)</span>, as long as their difference doesn’t. For a binary target, there are only two ways of satisfying this constraint.</p>
<p>Lastly, we can satisfy independence and separation if the prediction <span class="math inline">\(\hat{Y}\)</span> is independent of the target (which sounds like a terrible model) or if the target <span class="math inline">\(Y\)</span> has more degrees of freedom than <span class="math inline">\(Z\)</span>. So for a binary sensitive feature, we need three or more possible outcomes, to satisfy both independence and separation. Note that if we have infinitely possible outcomes, as in the case of a continuous target, we can definitely satisfy both these criteria, because <span class="math inline">\(Z\)</span> is certainly finite in size (limited to a finite number of subgroups). Furthermore, if the target is continuous, that would help us to satisfy independence of errors, without requiring equal representation. So increasing the degrees of freedom in our target seems like a promising path.</p>
<p>There is one particular issue with group fairness metrics. That is, that <strong>equalising statistical properties at the group level, does not guarantee fair treatment at an individual level</strong>. Let’s return to our applicant filter with the sensitive feature gender. Independence requires that acceptance rates are equal for male and female applicants. Suppose model acceptance rates are lower for female applicants. To ensure we satisfy the independence fairness criterion, we could just randomly select female applicants that were rejected and instead accept them until the acceptance rates matched. In fact this kind of approach can be used to satisfy any group fairness criterion. Clearly this method will likely result in some <em>undeserving</em> female applicants being accepted. Although this approach would be able to satisfy the fairness criterion, the resulting algorithm would likely be considered unfair.</p>
<p>It’s worth noting that although the approach of randomly selecting female applicants to accept might seem unnecessarily naive, there can be cases, (particularly when there are multiple protected characteristics that intersect) where protected groups are so small that models simply do not have enough training data to be able to make accurate predictions for them. In such cases a model could conceivably be, not much better than guessing for individuals in those groups. Even if we supposedly take a smarter approach of say, choosing the individuals closest to the decision boundary (rather than choosing them randomly) this would be equivalent to choosing a different acceptance threshold for women, in which case we would be using a different criterion to determine acceptance for male and female applicants (which are in all other respects similar), which could be viewed as unfair, despite satisfying independence. In the next chapter we’ll talk about <em>individual fairness</em> which resolves these difficulties by specifying the modelling problem in such a way that the notions of fairness and utility are entirely orthogonal.</p>
</section>
<section id="summary-2" class="level2 unnumbered">
<h2 class="unnumbered">Summary</h2>
<section id="group-fairness" class="level3 unnumbered">
<h3 class="unnumbered">Group fairness</h3>
<ul>
<li><p>The term group fairness is used to describe a class of metrics that are used to measure discrimination or bias across specific subgroups of a population, in a given decision process. At the implementation level, all group fairness metrics indicate the extent to which, some statistical property differs between different groups.</p></li>
<li><p>In general group fairness criterion and measures can be derived from independence constraints on the joint distributions of the non-sensitive features <span class="math inline">\(X\)</span>, sensitive features, <span class="math inline">\(Z\)</span>, the target feature <span class="math inline">\(Y\)</span> and predicted target <span class="math inline">\(\hat{Y}\)</span>.</p></li>
<li><p>Group fairness criteria can be broadly classified into two types; those that compare outcomes and those comparing errors.</p></li>
</ul>
<div id="tbl:GFairSumm">
<table>
<caption>Table 3.5: Group fairness metrics summary.</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Comparing</th>
<th colspan="2" style="text-align: center;">Outcomes</th>
<th colspan="2" style="text-align: center;">Errors</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Criterion</td>
<td style="text-align: center;">Independence</td>
<td style="text-align: center;">Twin Test</td>
<td style="text-align: center;">Separation</td>
<td style="text-align: center;">Sufficiency</td>
</tr>
<tr class="even">
<td style="text-align: left;">Constraint</td>
<td style="text-align: center;"><span class="math inline">\(\hat{Y}\bot Z\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\hat{Y}\bot Z|X\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\hat{Y}\bot Z|Y\)</span></td>
<td style="text-align: center;"><span class="math inline">\(Y\bot Z|\hat{Y}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Measures</td>
<td style="text-align: center;">Disparate impact</td>
<td style="text-align: center;">Disparate treatment</td>
<td colspan="2" style="text-align: center;">Disparate mistreatment</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="comparing-outcomes" class="level3 unnumbered">
<h3 class="unnumbered">Comparing Outcomes</h3>
<div id="tbl:GFOutSumm">
<table>
<caption>Table 3.6: Group fairness metrics comparing outcomes.</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Category</th>
<th style="text-align: left;">Criterion</th>
<th style="text-align: left;">Definition</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td rowspan="9" style="text-align: left;">Independence<br><span class="math inline">\(\hat{Y}\bot Z\)</span></td>
<td style="text-align: left;">Mutual information</td>
<td style="text-align: left;"><span class="math inline">\(\displaystyle I(\hat{Y},Z)=\sum_{z\in\mathcal{Z}}\,\,\int_{\hat{y} \in \mathcal{Y}} f_{\hat{Y},Z}(\hat{y},z) \log \frac{f_{\hat{Y},Z}(\hat{y},z)} {f_{\hat{Y}}(\hat{y})\mathbb{P}(z)}\,\mathrm{d}\hat{y}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Normalised prejudice index</td>
<td style="text-align: left;"><span class="math inline">\(\displaystyle r_{\text{npi}} = \frac{I(\hat{Y},Z)}{\sqrt{S(\hat{Y})S(Z)}}, \quad S(Z) = -\sum_{z\in\mathcal{Z}} \mathbb{P}(z)\log\mathbb{P}(z)\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Mean difference</td>
<td style="text-align: left;"><span class="math inline">\(\displaystyle d
    = \mathbb{E}(\hat{Y} | Z=0) - \mathbb{E}(\hat{Y} | Z=1)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Statistical parity<sup>a</sup></td>
<td style="text-align: left;"><span class="math inline">\(\displaystyle \mathbb{P}(\hat{Y}=1 | Z=1) = \mathbb{P}(\hat{Y}=1 | Z=0)\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Risk difference<sup>b</sup></td>
<td style="text-align: left;"><span class="math inline">\(\displaystyle d = \mathbb{P}(\hat{Y}=1 | Z=0) - \mathbb{P}(\hat{Y}=1 | Z=1)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Normalised difference</td>
<td style="text-align: left;"><span class="math inline">\(\displaystyle \bar{d} = \frac{d}{d_{\max}}, \quad d_{\max} = \min\left\{ \frac{\mathbb{P}(\hat{Y}=1)}{\mathbb{P}(Z=1)}, \frac{\mathbb{P}(\hat{Y}=0)}{\mathbb{P}(Z=0)} \right\}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Risk ratio<sup>c</sup></td>
<td style="text-align: left;"><span class="math inline">\(\displaystyle r = \frac{\mathbb{P}(\hat{Y}=1 | Z=0)}{\mathbb{P}(\hat{Y}=1 | Z=1)}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Elift ratio</td>
<td style="text-align: left;"><span class="math inline">\(\displaystyle \frac{\mathbb{P}(\hat{Y}=1 | Z=0)}{\mathbb{P}(\hat{Y}=1)}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Odds ratio</td>
<td style="text-align: left;"><span class="math inline">\(\displaystyle \frac{\mathbb{P}(\hat{Y}=1 | Z=1)\mathbb{P}(\hat{Y}=0 | Z=0)}
       {\mathbb{P}(\hat{Y}=0 | Z=1)\mathbb{P}(\hat{Y}=1 | Z=0)}\)</span></td>
</tr>
<tr class="even">
<td rowspan="2" style="text-align: left;">Twin test<br><span class="math inline">\(\hat{Y}\bot Z|X\)</span></td>
<td style="text-align: left;">Causal mean difference</td>
<td style="text-align: left;"><span class="math inline">\(\displaystyle \mathbb{E}(\hat{Y} | Z=1, X=x) - \mathbb{E}(\hat{Y} | Z=0, X=x)\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Observed mean difference</td>
<td style="text-align: left;"><span class="math inline">\(\displaystyle \mathbb{E}(Y | Z=1, X=x) - \mathbb{E}(Y | Z=0, X=x)\)</span></td>
</tr>
</tbody>
</table>
</div>
<div class="tablenotes">
<p><sup>a</sup>Also called, demographic parity and parity impact.</p>
<p><sup>b</sup>Also called, discrimination score and statistical parity difference.</p>
<p><sup>c</sup>Also called, impact ratio and disparate impact ratio.</p>
</div>
<section id="independence-hatybot-z" class="level4 unnumbered">
<h4 class="unnumbered">Independence (<span class="math inline">\(\hat{Y}\bot Z\)</span>)</h4>
<ul>
<li><p>Independence metrics can be evaluated on both data and model output. Comparing them is important in understanding if our model is inadvertently introducing or exaggerating biases in the training data.</p></li>
<li><p>If the target variable <span class="math inline">\(Y\)</span> and sensitive feature <span class="math inline">\(Z\)</span> are not independent then imposing it on a model does not permit the theoretically perfect solution <span class="math inline">\(Y = \hat{Y}\)</span>. The stronger the relationship between <span class="math inline">\(Z\)</span> and <span class="math inline">\(Y\)</span>, the greater the trade-off between fairness and utility in satisfying independence criterion.</p></li>
<li><p>Independence does not consider the existence of confounding variables.</p></li>
<li><p>In the case where independence is not satisfied by the data, imposing it on a model implies a level of distrust in the data or modelling of the problem.</p></li>
</ul>
</section>
<section id="the-twin-test-hatybot-zx" class="level4 unnumbered">
<h4 class="unnumbered">The Twin Test (<span class="math inline">\(\hat{Y}\bot Z|X\)</span>)</h4>
<ul>
<li><p>The twin test tries to establish cause (of differing treatment across protected groups), by comparing results for counterfactual twins that differ only by group membership.</p></li>
<li><p>Given access to the model in the form of a black box, the twin test consists of a randomised experiment, sampling individuals and comparing the output for the corresponding twins.</p></li>
<li><p>For a stochastic model, the twin test is computationally more expensive, since we must evaluate our model for each pair of twins a sufficiently large number of times to obtain the predicted target distribution.</p></li>
</ul>
</section>
</section>
<section id="comparing-errors" class="level3 unnumbered">
<h3 class="unnumbered">Comparing errors</h3>
<ul>
<li><p>Criteria comparing errors assume that the data is fair.</p></li>
<li><p>Unlike criteria comparing outcomes, criteria comparing errors do not preclude the theoretically perfect solution, <span class="math inline">\(\hat{Y}=Y\)</span>.</p></li>
</ul>
<div id="tbl:GFErrSumm">
<table>
<caption>Table 3.7: Group fairness metrics comparing errors.</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Category</th>
<th style="text-align: left;">Criterion</th>
<th style="text-align: left;">Definition</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Error<br><span class="math inline">\((\hat{Y}-Y)\bot Z\)</span></td>
<td style="text-align: left;">Balanced residuals</td>
<td style="text-align: left;"><span class="math inline">\(d_{\text{err}} = \mathbb{E}(\hat{Y} - Y | Z=1) - \mathbb{E}(\hat{Y} - Y | Z=0)\)</span></td>
</tr>
<tr class="even">
<td rowspan="2" style="text-align: left;">Error rate</td>
<td style="text-align: left;">Error rate difference</td>
<td style="text-align: left;"><span class="math inline">\(\mathbb{P}(\hat{Y}\neq Y | Z=1) - \mathbb{P}(\hat{Y}\neq Y | Z=0)\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Error rate ratio</td>
<td style="text-align: left;"><span class="math inline">\(\displaystyle \frac{\mathbb{P}(\hat{Y}\neq Y | Z=0)}{\mathbb{P}(\hat{Y}\neq Y | Z=1)}\)</span></td>
</tr>
<tr class="even">
<td rowspan="5" style="text-align: left;">Separation<br><span class="math inline">\(\hat{Y}\bot Z|Y\)</span></td>
<td style="text-align: left;">Equalised odds</td>
<td style="text-align: left;"><span class="math inline">\(TPR_{Z=0} = TPR_{Z=1}\)</span> and <span class="math inline">\(TNR_{Z=0} = TNR_{Z=1}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Average odds difference</td>
<td style="text-align: left;"><span class="math inline">\(\frac{1}{2}
    [ TPR_{Z=0} - TPR_{Z=1} + FPR_{Z=0} - FPR_{Z=1} ]\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Average odds error</td>
<td style="text-align: left;"><span class="math inline">\(\frac{1}{2}
    [ |TPR_{Z=0} - TPR_{Z=1}| + |FPR_{Z=0} - FPR_{Z=1}| ]\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Equal opportunity</td>
<td style="text-align: left;"><span class="math inline">\(TPR_{Z=0} = TPR_{Z=1}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Equal opportunity difference</td>
<td style="text-align: left;"><span class="math inline">\(TPR_{Z=0} - TPR_{Z=1}\)</span></td>
</tr>
<tr class="odd">
<td rowspan="2" style="text-align: left;">Sufficiency<br><span class="math inline">\(Y\bot Z|\hat{Y}\)</span></td>
<td style="text-align: left;">Equally sufficient</td>
<td style="text-align: left;"><span class="math inline">\(PPV_{Z=0} = PPV_{Z=1}\)</span> and <span class="math inline">\(NPV_{Z=0} = NPV_{Z=1}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Calibration by group</td>
<td style="text-align: left;"><span class="math inline">\(\mathbb{P}(Y=1 | P=p, Z=z) = p \quad \forall \, p, z\)</span></td>
</tr>
</tbody>
</table>
</div>
<section id="separation-hatybot-zy" class="level4 unnumbered">
<h4 class="unnumbered">Separation (<span class="math inline">\(\hat{Y}\bot Z|Y\)</span>)</h4>
<ul>
<li><p>Separation, allows for dependence between the predicted target variable and the sensitive feature but only to the extent that it exists between the actual target variable and the sensitive feature.</p></li>
</ul>
</section>
<section id="sufficiency-ybot-zhaty" class="level4 unnumbered">
<h4 class="unnumbered">Sufficiency (<span class="math inline">\(Y\bot Z|\hat{Y}\)</span>)</h4>
<ul>
<li><p>For a binary classification model, sufficiency requires the probability of of an error for a given prediction to be equal across protected groups.</p></li>
<li><p>A model that is calibrated by group satisfies sufficiency.</p></li>
<li><p>Sufficiency is is a weaker model constraint compared to separation as it is satisfied implicitly through the training process.</p></li>
</ul>
</section>
</section>
<section id="incompatibility-between-fairness-criteria" class="level3 unnumbered">
<h3 class="unnumbered">Incompatibility between fairness criteria</h3>
<ul>
<li><p>Independence (<span class="math inline">\(Z \bot \hat{Y}\)</span>) and sufficiency (<span class="math inline">\(Z \bot Y | \hat{Y}\)</span>) can only be simultaneously be satisfied if the sensitive feature <span class="math inline">\(Z\)</span>, and the target variable <span class="math inline">\(\hat{Y}\)</span>, are independent (<span class="math inline">\(Z \bot Y\)</span>).</p></li>
<li><p>In the case that <span class="math inline">\(Y\)</span> is binary, independence (<span class="math inline">\(Z \bot \hat{Y}\)</span>) and separation (<span class="math inline">\(Z \bot \hat{Y} | Y\)</span>) criteria can only be simultaneously satisfied if either <span class="math inline">\(\hat{Y} \bot Y\)</span> or <span class="math inline">\(Y \bot Z\)</span>.</p></li>
<li><p>Separation (<span class="math inline">\(Z \bot \hat{Y} | Y\)</span>) and sufficiency (<span class="math inline">\(Z \bot Y | \hat{Y}\)</span>) can only be simultaneously be satisfied if the sensitive feature, <span class="math inline">\(Z\)</span> is independent of both the target variable <span class="math inline">\(Y\)</span> and the predicted target <span class="math inline">\(\hat{Y}\)</span>, that is if <span class="math inline">\(Z \bot Y\)</span> and <span class="math inline">\(Z \bot \hat{Y}\)</span>.</p></li>
<li><p>In the case where <span class="math inline">\(Y\)</span> is binary, separation and sufficiency can only be satisfied simultaneously if the sensitive feature is independent of the target variable, or the model has an accuracy of 100% (<span class="math inline">\(\hat{Y}=Y\)</span>) or the model has an accuracy of 0% (<span class="math inline">\(\hat{Y}=1-Y\)</span>).</p></li>
</ul>
</section>
<section id="concluding-remarks-1" class="level3 unnumbered">
<h3 class="unnumbered">Concluding remarks</h3>
<ul>
<li><p>We need more degrees of freedom in the target variable. Would ensure we are always able to satisfy independent errors.</p></li>
<li><p>Equalising statistical properties at the <em>group level</em>, don’t guarantee fair treatment at an <em>individual level</em>.</p></li>
</ul>
</section>
</section>
</section>
<section id="ch_IndividualFairness" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Individual Fairness</h1>
<div class="chapsumm">
<p><strong>This chapter at a glance</strong></p>
<ul>
<li><p>Fairness at an individual level</p></li>
<li><p>Individual fairness as continuity</p></li>
<li><p>Individual fairness as uncertainty</p></li>
<li><p>Individual fairness as consistency</p></li>
</ul>
</div>
<p>Broadly speaking, <em>individual fairness</em> is the idea that a given decision process is fair, if similar people (with respect to the task), receive similar decisions. Compared to group fairness, individual fairness is arguably a much more expansive concept of fairness. Group fairness criteria are rather specific. They tackle the question of fairness by comparing pairs of groups but this approach has limitations. In particular, equalising statistical properties at the group level, don’t guarantee fair treatment for any given individual. What do we mean by fairness at an individual level and how does it relate to group fairness? Let’s go back to our applicant filter. We wanted to understand if our algorithm is biased against female applicants. What if there are more than two genders? Then we need to calculate our metric on all the subgroups. But what we really want to do is make sure we’re being fair to all intersections of protected features too - disadvantages (and indeed advantages) on multiple dimensions can compound. As we create finer grained partitions of the population, we increase the number of groups. Eventually every group contains a single individual. In order to measure fairness at an individual level then, we need a way of comparing individuals rather than groups - a similarity metric.</p>
<p>As a measure, individual fairness cares not about the decision itself, but rather about the consistency with which decisions are made. Individual fairness is a property of a mapping from features to output (<span class="math inline">\(Y\)</span> or <span class="math inline">\(\hat{Y}\)</span>), not a measure of how one mapping differs from another (<span class="math inline">\(\hat{Y}-Y\)</span>). In this sense, <em>utility</em> and <em>individual fairness</em> are orthogonal. It’s not immediately obvious but this is an important conceptual leap from group fairness but it is. Individual fairness does not assume the existence of a fair ground truth dataset in its definition of fairness; it cares only how similar people are, (not how to rank them, that is the job of utility function). The similarity metric represents the ground truth for what is fair; that is, how similar people are (with respect to the task) in feature space.</p>
<p>In this chapter we’ll provide the formal definition of individual fairness (as originally proposed by Dwork et. al.<span class="citation" data-cites="DworkIndFair"><a href="#ref-DworkIndFair" role="doc-biblioref">[59]</a></span><span class="marginnote"><span id="ref-DworkIndFair" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[59] </span><span class="csl-right-inline">C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel, <span>“Fairness through awareness.”</span> 2011.Available: <a href="https://arxiv.org/abs/1104.3913">https://arxiv.org/abs/1104.3913</a></span>
</span>
</span>). We will see that by this notion of fairness, deterministic classification models are inherently unfair. We resolve this issue by returning a distribution over outcomes and sampling predictions randomly from our distribution. Finally, we look at popular measures of individual fairness and analyse them. Let’s get started!</p>
<section id="individual-fairness-as-continuity" class="level2" data-number="4.1">
<h2 data-number="4.1"><span class="header-section-number">4.1</span> Individual Fairness as Continuity</h2>
<p>What does individual fairness mean for a model? Let’s start with a deterministic regression model and think of it as a function that maps individuals to predictions. Individual fairness can then be interpreted as a requirement that, two points that are close in input (feature) space are also close in output (target/prediction) space. To satisfy this constraint our model mapping must be continuous. At a discontinuity, two individuals falling either side of it can be arbitrarily similar (identical) in feature space and yet receive entirely different outcomes. Below we define <em>Lipschitz Continuity</em><span class="sidenote-wrapper"><label for="sn-8" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-8" class="margin-toggle"/><span class="sidenote">Named after the German mathematician Rudolf Lipschitz, perhaps most well known for his contributions to mathematical analysis.<br />
<br />
</span></span> in the context of a deterministic regression model.</p>
<div class="lookbox">
<p><strong>Lipschitz Continuity (Regression)</strong></p>
<p>Consider <span class="math inline">\(\hat{y}\)</span>, to be determined by our model function <span class="math inline">\(f\)</span> which maps individuals <span class="math inline">\(\boldsymbol{x}\in\mathcal{X}\)</span> to predictions <span class="math inline">\(\hat{y}\in\mathcal{Y}\)</span>, that is to say <span class="math inline">\(\hat{y}=f(\boldsymbol{x})\)</span> and <span class="math inline">\(f:\mathcal{X}\mapsto\mathcal{Y}\)</span>. The function <span class="math inline">\(f\)</span> is <em>Lipschitz continuous</em> if there exists a real valued, non-negative constant <span class="math inline">\(K\in\mathbb{R}_{\geq 0}\)</span> such that, for every pair of individuals <span class="math inline">\(\boldsymbol{x}_i, \boldsymbol{x}_j \in \mathcal{X}\)</span>,</p>
<div id="eq:LipContReg">
<table>
<colgroup>
<col style="width: 90%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
d_{\mathcal{Y}}(f(\boldsymbol{x}_i), f(\boldsymbol{x}_j)) \leq K d_{\mathcal{X}}(\boldsymbol{x}_i, \boldsymbol{x}_j).\]</span></td>
<td style="text-align: right;">(4.1)</td>
</tr>
</tbody>
</table>
</div>
<p>Where <span class="math inline">\(d_{\mathcal{X}}:\mathcal{X}\times\mathcal{X}\mapsto\mathbb{R}\)</span> and <span class="math inline">\(d_{\mathcal{Y}}:\mathcal{Y}\times\mathcal{Y}\mapsto\mathbb{R}\)</span> are <em>distance metrics</em> (the properties of which we recap below) that allow us to determine how close (similar) any two points are in the feature and target spaces respectively. <span class="math inline">\(K\)</span> is called the Lipshitz constant.</p>
</div>
<p>For the simplest case where all our features and the target are real values, that is <span class="math inline">\(\mathcal{X}=\mathbb{R}^m\)</span> and <span class="math inline">\(\mathcal{Y}=\mathbb{R}\)</span>, our model <span class="math inline">\(\hat{y}=f(\boldsymbol{x})\)</span>, can be visualised as an <span class="math inline">\(m+1\)</span> dimensional surface. In this case, we can interpret continuity as the requirement that the slope of our model (with respect to our similarity metric) is finite and bounded between <span class="math inline">\(\pm K\)</span> on the domain <span class="math inline">\(\mathcal{X}\)</span>. The smaller the slope, the more similarly neighbouring individuals are treated. We can apply this idea to a finite set of data points, <span class="math inline">\(\mathcal{X}=\{\boldsymbol{x}_1,\boldsymbol{x}_2,...,\boldsymbol{x}_n,\}\)</span> and <span class="math inline">\(\mathcal{Y}=\{y_1, y_2,...,y_n\}\)</span>, (again where <span class="math inline">\(\boldsymbol{x}_i\in\mathbb{R}^m\,\forall\, i\)</span> and <span class="math inline">\(y_i\in\mathbb{R}\)</span>). If the gradient of the line between any two data points in the dataset is bounded between <span class="math inline">\(\pm K\)</span> then there no evidence that the mapping violates the criterion.</p>
<div class="lookbox">
<p><strong>Distance metric properties</strong></p>
<p>A distance metric <span class="math inline">\(d\)</span> on the set <span class="math inline">\(\mathcal{X}\)</span> is a function <span class="math inline">\(d:\mathcal{X}\times\mathcal{X}\mapsto\mathbb{R}_{\geq 0}\)</span> that has the following properties <span class="math inline">\(\forall\,\,x, y, z\in\mathcal{X}\)</span></p>
<ul>
<li><p>Identity: <span class="math inline">\(d(x,y)=0 \Leftrightarrow x=y\)</span></p></li>
<li><p>Symmetry: <span class="math inline">\(d(x,y)=d(y,x)\)</span></p></li>
<li><p>Triangle inequality: <span class="math inline">\(d(x,y)\leq d(x,z)+d(z,y)\)</span></p></li>
</ul>
<p>Combining Symmetry with the triangle inequality shows that the metric must return a non-negative value.</p>
<span class="marginnote"><figure>
<img src="04_IndividualFairness/figures/Fig_TriangleInequality.png" id="fig:TriIneq" alt="Figure 4.1: Triangle inequality." />
<figcaption aria-hidden="true">Figure 4.1: Triangle inequality.</figcaption>
</figure>
</div>
</section>
<section id="individual-fairness-as-randomness" class="level2" data-number="4.2">
<h2 data-number="4.2"><span class="header-section-number">4.2</span> Individual Fairness as Randomness</h2>
<p>For classification problems our target variable is discrete, the example falls into one class or another and we treat individuals differently based on their classification. Our job applicant filter either accepts or rejects an applicant, there isn’t anything in between. Then how can a classification model satisfy continuity and thus individual fairness? It can’t. A deterministic classifier indeed cannot satisfy individual fairness by construction because it has a discontinuity at the decision boundary. For example, let’s suppose our job applicant filter outputs a score. We use a threshold <span class="math inline">\(t=0.5\)</span> on the score, so we accept the applicant if their score is greater than or equal to 0.5 and reject them if they score lower. At a score of 0.5 the probability of acceptance ‘jumps’ from zero to one. The threshold <span class="math inline">\(t=0.5\)</span> defines the decision boundary. We will reject an applicant that scores 0.4999 but accept an applicant that scores 0.5 despite them being the same (within error say) according to our model. See Figure <a href="#fig:DiscThreshold" data-reference-type="ref" data-reference="fig:DiscThreshold">4.2</a>.</p>
<figure>
<img src="04_IndividualFairness/figures/Fig_Score2ProbDisc.png" id="fig:DiscThreshold" style="width:60.0%" alt="Figure 4.2: Discontinuity in the probability of acceptance as a function of model score (at the threshold t) under a deterministic binary classifier." />
<figcaption aria-hidden="true">Figure 4.2: Discontinuity in the probability of acceptance as a function of model score (at the threshold <span class="math inline">\(t\)</span>) under a deterministic binary classifier.</figcaption>
</figure>
<p>If we want our model to be fair at the individual level, we need to remove the discontinuity (close the gap) at our decision boundary. How might we do this? Let’s return to our simple example of the job applicant filter. Let’s assume our binary classifier outputs a score and that score is a continuous function of our features. In this case, the discontinuity in our model mapping is a result of the threshold alone because continuity holds under composition. That is to say, a continuous function of a continuous function is also continuous<span class="sidenote-wrapper"><label for="sn-9" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-9" class="margin-toggle"/><span class="sidenote">More precisely, for <span class="math inline">\(f(x)=g(h(x))\)</span>, if <span class="math inline">\(h(x)\)</span> is continuous at <span class="math inline">\(x=a\)</span> and <span class="math inline">\(g(x)\)</span> is continuous at <span class="math inline">\(x=h(a)\)</span> then <span class="math inline">\(f\)</span> is continuous at <span class="math inline">\(x=a\)</span>.<br />
<br />
</span></span>. Then if we can remove the discontinuity at the threshold our model mapping will be continuous. Rather than imposing a threshold on the model score and rejecting or accepting individuals based on which side of the threshold they fall, we can use the score to determine the probability of acceptance. We then randomly draw a value according to that probability distribution, to determine if the individual is accepted or not. This approach allows the probability of acceptance to be a continuous function of model score. See for example Figure <a href="#fig:ContThreshold" data-reference-type="ref" data-reference="fig:ContThreshold">4.3</a>.</p>
<figure>
<img src="04_IndividualFairness/figures/Fig_Score2ProbCont.png" id="fig:ContThreshold" style="width:60.0%" alt="Figure 4.3: Piecewise linear probability of acceptance as a function of model score under a stochastic binary classifier. Predictions are random draws between the thresholds t_1 and t_2." />
<figcaption aria-hidden="true">Figure 4.3: Piecewise linear probability of acceptance as a function of model score under a stochastic binary classifier. Predictions are random draws between the thresholds <span class="math inline">\(t_1\)</span> and <span class="math inline">\(t_2\)</span>.</figcaption>
</figure>
<p>At first glance, this approach might sound bizarre. We are saying that in order to remedy the problem that similar individuals receive different predictions, we must instead turn to a model which can make different predictions for the same individual?! Indeed the definition of <em>consistency</em> in judgement is a debated topic among legal scholars. For some randomness is explicitly forbidden<span class="citation" data-cites="ConsistencyDet"><a href="#ref-ConsistencyDet" role="doc-biblioref">[60]</a></span><span class="marginnote"><span id="ref-ConsistencyDet" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[60] </span><span class="csl-right-inline">A. V. Dicey, <span>“The law of the constitution.”</span> 1978.</span>
</span>
</span>, others allow flexibility in the interpretation of the rules<span class="citation" data-cites="ConsistencyFlex"><a href="#ref-ConsistencyFlex" role="doc-biblioref">[61]</a></span><span class="marginnote"><span id="ref-ConsistencyFlex" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[61] </span><span class="csl-right-inline">R. Dworkin, <span>“No right answer.”</span> 1978.</span>
</span>
</span> but not randomness in the decision. Clearly there is value in being able to make a single and predictable judgement <em>most</em> of the time. That might mean favouring one decision over another in the face of uncertainty. But the value of certainty is in itself is contextual. In legal decisions, the stakes are high, we need a process for making the decision that provides some confidence that we are correct and so we might favour letting a guilty person go free than an innocent person be incarcerated (beyond reasonable doubt so to speak) but this need not always be the case.</p>
<p>With a deterministic model we allow arbitrarily similar individuals to be <em>guaranteed</em> to receive different predictions. By randomising our predictions we accept that in any decision, we may have incomplete or erroneous knowledge (and thus uncertainty in our predictions). At the very least there is uncertainty around the decision boundary where individuals (according to our own model) fall into the <em>maybe</em> category. For those individuals, the decision is more a matter of luck (or risk depending on your perspective) than others. By moving to a stochastic model we are able to always gives similar individuals a similar <em>chance</em> of being accepted (or rejected). Randomness in predictions in machine translation for example makes complete sense. If the translation of a word in a sentence has 55% probability of being the masculine variation and 40% chance of being the feminine variation (according to your own model) then does it always make sense to consistently predict the masculine? In this example we see more clearly how randomness in predictions when faced with uncertainty can be desirable trait when it comes to being fair. In Figure <a href="#fig:ContThreshold" data-reference-type="ref" data-reference="fig:ContThreshold">4.3</a> we illustrate the simplest way to achieve continuity at the decision boundary. We create a region (between two thresholds <span class="math inline">\(t_1\)</span> and <span class="math inline">\(t_2\)</span>) in which model scores result in randomised predictions. <a href="#IF_RandPreds">Implementation</a> in appendix <a href="#sec_app_IFSolutions" data-reference-type="ref" data-reference="sec_app_IFSolutions">D.2</a>.</p>
<p>For classification then, our model must be probabilistic, that is, it maps each individual in feature space to a distribution over the possible outcomes, which we can then randomly draw from to make predictions. Our predictions are then randomised rather than deterministic and to satisfy individual fairness we require our probabilistic model mapping to be continuous. Let’s write our continuity condition for our classifier more formally.</p>
<div class="lookbox">
<p><strong>Lipschitz Continuity (Classification)</strong></p>
<p>Consider our classification model to be a function <span class="math inline">\(f\)</span>, which maps individuals <span class="math inline">\(\boldsymbol{x}\in\mathcal{X}\)</span> and outcomes <span class="math inline">\(y\in\mathcal{Y}\)</span> to probabilities <span class="math inline">\(p_{\boldsymbol{x}}(y)\)</span>, that is to say <span class="math inline">\(p_{\boldsymbol{x}}(y)=f(\boldsymbol{x}, y)\)</span> and <span class="math inline">\(f:\mathcal{X}\times\mathcal{Y}\mapsto[0,1]\)</span>. For a fixed value of <span class="math inline">\(\boldsymbol{x}\)</span>, <span class="math inline">\(p_{\boldsymbol{x}}(y)=f(\boldsymbol{x}, y) \in \mathcal{P}(\mathcal{Y})\)</span> is a distribution over all possible outcomes <span class="math inline">\(y\in\mathcal{Y}\)</span>. Then the mapping <span class="math inline">\(f\)</span> is <em>Lipschitz continuous</em> if there exists a real valued, non-negative constant <span class="math inline">\(K \in \mathbb{R}_{\geq 0}\)</span> such that,</p>
<div id="eq:LipContClf">
<table>
<colgroup>
<col style="width: 90%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
d_{\mathcal{P}(\mathcal{Y})}(f(\boldsymbol{x}_i,y), f(\boldsymbol{x}_j,y)) \leq K d_{\mathcal{X}}(\boldsymbol{x}_i, \boldsymbol{x}_j) \quad
\forall\,\, \boldsymbol{x}_i, \boldsymbol{x}_j \in \mathcal{X}\]</span></td>
<td style="text-align: right;">(4.2)</td>
</tr>
</tbody>
</table>
</div>
<p>where <span class="math inline">\(d_{\mathcal{X}}:\mathcal{X}\times\mathcal{X}\mapsto\mathbb{R}\)</span> and <span class="math inline">\(d_{\mathcal{P}(\mathcal{Y})}:\mathcal{P}(\mathcal{Y})\times\mathcal{P}(\mathcal{Y})\mapsto\mathbb{R}\)</span> denote distance metrics. <span class="math inline">\(d_{\mathcal{X}}\)</span> determines how similar two individuals are in feature space and <span class="math inline">\(d_{\mathcal{P}(\mathcal{Y})}\)</span> measures how similar two probability distributions over <span class="math inline">\(\mathcal{Y}\)</span> are.</p>
</div>
<p>We now have a theoretical understanding of how individual fairness translates to a model behaviour, ideally our model mapping is continuous and the smaller the slope of the surface (with respect to our similarity metric), the more similarly neighbouring individuals are treated. In fact, if the slope is zero everywhere then everyone is treated the same. All individuals get mapped to the same distribution over outcomes and we have satisfied our individual fairness constraint. Of course such a model would not make a very good predictor as it would not take into account the features of the individuals in its predictions. We can then think of the problem of satisfying individual fairness as an additional constraint in our model optimisation task. We want to maximise utility (minimise some loss function <span class="math inline">\(\mathcal{L}\)</span>, on the training data) and to satisfy individual fairness we want to ensure the slope of our model, with respect to our similarity metric is bounded between <span class="math inline">\(\pm K\)</span>. In practice we can absorb the value <span class="math inline">\(K\)</span> into our similarity metric <span class="math inline">\(d_{\mathcal{X}}(\boldsymbol{x}_i, \boldsymbol{x}_j)\)</span>. Notice that we are indifferent to the direction of the slope, we care only about its size. Getting the direction of the slope <em>right</em> is achieved by maximising utility. Thus we have reduced our problem of training a fair model to one of constrained optimisation. <span class="math display">\[\begin{aligned}
&amp; \min\left\{\mathbb{E}_{\boldsymbol{x}\in\boldsymbol{X}}\,
\mathbb{E}_{\hat{Y}\sim f(\boldsymbol{x},Y)}\,\left[\mathcal{L}(\boldsymbol{X}, \boldsymbol{Y}, \boldsymbol{\hat{Y}})\right]\right\}, \\
\textrm{such that}\quad &amp;
d_{\mathcal{P}(\mathcal{Y})}(f(\boldsymbol{x}_i,y), f(\boldsymbol{x}_j,y)) \leq d_{\mathcal{X}}(\boldsymbol{x}_i, \boldsymbol{x}_j) \\
\textrm{and}\quad &amp; \quad f(\boldsymbol{x_i},y) \in \mathcal{P}(\mathcal{Y})\qquad\forall\,\boldsymbol{x}_i, \boldsymbol{x}_j \in \boldsymbol{X}.
\end{aligned}\]</span></p>
</section>
<section id="similarity-metrics" class="level2" data-number="4.3">
<h2 data-number="4.3"><span class="header-section-number">4.3</span> Similarity Metrics</h2>
<section id="similarity-between-individuals" class="level3" data-number="4.3.1">
<h3 data-number="4.3.1"><span class="header-section-number">4.3.1</span> Similarity Between Individuals</h3>
<p>A question we have glossed over so far is on the similarity metrics <span class="math inline">\(d_{\mathcal{X}}(\boldsymbol{x}_i, \boldsymbol{x}_j)\)</span>. It might not seem like we have gained much in reframing fairness as treating similar people similarly. After all, we still have to specify a similarity metric. Determining how similar individuals (or more generally examples in feature space) are, is a question that we answer either explicitly or implicitly by machine learning solutions when maximising utility. In practice the requirement of defining a similarity metric exposes our definition of fairness and decouples it from utility (or predictive performance). Recall in the last chapter, when considering different notions group fairness we saw different trade-offs with utility. Individual fairness unifies these different definitions of fairness by exposing our belief about what is fair (be it anti classification, anti-subordination or something in between) in the form of a similarity metric. In some sense it provides a better model for fairness. A particular advantage of this framework is that it allows separation of the classification task between two distinct parties, a <em>data owner</em> and a <em>model user</em>. The data owner is a trusted party while the model user is the party who wishes to classify individuals. Under the proposed constrained optimisation framework, the model user is free to define the loss function, but the classification task (map from individuals to distributions over outcomes) could be the responsibility of the trusted data owner.</p>
</section>
<section id="similarity-between-probability-distributions" class="level3" data-number="4.3.2">
<h3 data-number="4.3.2"><span class="header-section-number">4.3.2</span> Similarity Between Probability Distributions</h3>
<p>Let’s look at two possible choices for <span class="math inline">\(d_{\mathcal{P}(\mathcal{Y})}\)</span>.</p>
<section id="total-variation-l_1-norm-d_1" class="level4 unnumbered">
<h4 class="unnumbered">Total Variation (<span class="math inline">\(L_1\)</span>) Norm: <span class="math inline">\(D_{1}\)</span></h4>
<p>One possible distance metric on distributions <span class="math inline">\(d_{\mathcal{P}(\mathcal{Y})}\)</span> is the <strong>total variation</strong>, <span class="math display">\[d_{\mathcal{P}(\mathcal{Y})} = d_{tv}(p,q)
= \frac{1}{2} \sum_{y\in\mathcal{Y}} |p(y)-q(y)|.\]</span> Note that <span class="math inline">\(d_{tv}\)</span> is bounded between zero (when the distributions are the same) and one (when the distributions are entirely non-overlapping), therefore the Lipschitz condition would require us to the choose the distance metric <span class="math inline">\(d_{\mathcal{X}}\)</span> between individuals to be scaled similarly. This can be problematic depending on the feature space.</p>
</section>
<section id="relative-l_infty-norm-d_infty" class="level4 unnumbered">
<h4 class="unnumbered">Relative (<span class="math inline">\(L_{\infty}\)</span>) Norm: <span class="math inline">\(D_{\infty}\)</span></h4>
<p>An alternative choice for <span class="math inline">\(d_{\mathcal{P}(\mathcal{Y})}\)</span> which resolves this issue is the <strong>relative <span class="math inline">\(l_{\infty}\)</span> metric</strong>: <span class="math display">\[d_{\mathcal{P}(\mathcal{Y})} = d_{\infty}(p,q) = \sup_{y\in\mathcal{Y}}
\log \left[\max\left(\frac{p(y)}{q(y)}, \frac{q(y)}{p(y)}\right)\right].\]</span></p>
</section>
</section>
</section>
<section id="measuring-individual-fairness-in-practice" class="level2" data-number="4.4">
<h2 data-number="4.4"><span class="header-section-number">4.4</span> Measuring Individual Fairness in Practice</h2>
<p>The metric consistency, measures individual fairness by looking at the changes in our model output for neighbouring points on a finite set of data points. <span class="math display">\[yNN = 1 - \frac{1}{n} \sum_{i=1}^n \left| \hat{y}_i -
\frac{1}{k}\sum_{j|x_j\in kNN(\boldsymbol{x}_i)} \hat{y}_j \right|\]</span> It is described as measuring “the consistency of the model classifications locally in input space”<span class="citation" data-cites="ZemelLearnFairReps"><a href="#ref-ZemelLearnFairReps" role="doc-biblioref">[62]</a></span><span class="marginnote"><span id="ref-ZemelLearnFairReps" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[62] </span><span class="csl-right-inline">R. Zemel, Y. Wu, K. Swersky, T. Pitassi, and C. Dwork, <span>“Learning fair representations,”</span> in <em>Proceedings of the 30th international conference on machine learning</em>, 2013, vol. 28, pp. 325–333.</span>
</span>
</span>. Values close to one indicate that similar inputs are treated similarly. Note that if all individuals receive the same prediction, consistency will be exactly one. The consistency metric described above, rather conveniently, avoids the need to choose a metric that compares probability distributions over outcomes but we still need a distance metric in feature space to compare how similar two individuals are and thus find the <span class="math inline">\(k\)</span> nearest neighbours.</p>
</section>
<section id="summary-3" class="level2 unnumbered">
<h2 class="unnumbered">Summary</h2>
<ul>
<li><p>Individual fairness is the idea that a given decision process is fair, if similar people (with respect to the task) receive similar decisions. As a measure, individual fairness cares not about the actual decision, but rather about the consistency with which they are made.</p></li>
<li><p>Individual fairness is orthogonal to utility, it does not factor a ground truth <span class="math inline">\(\hat{Y}\)</span> into it’s calculation, it is only interested in the change in prediction relative to the similarity. That is a property of a mapping.</p></li>
<li><p>Individual fairness can be interpreted as a continuity requirement on our data or model. In practice it can be implemented by imposing a bound on the slope of our model mapping, with respect to our similarity metric.</p></li>
<li><p>A deterministic classifier (one that typically outputs a score and then imposes a threshold on it to determine the predicted class) cannot satisfy individual fairness by construction, because the threshold results in a discontinuity in the model mapping where the gradient becomes infinite.</p></li>
<li><p>For a classification model to satisfy individual fairness (continuity) we must turn to a probabilistic model which maps individuals to distributions over outcomes. The continuity requirement then applies to the change in distribution of outcomes relative to the similarity of individuals. Predictions must be randomised, based on the model output distributions.</p></li>
<li><p>The metric consistency is given by <span class="math display">\[yNN = 1 - \frac{1}{n} \sum_{i=1}^n \left| \hat{y}_i -
\frac{1}{k}\sum_{j|x_j\in kNN(\boldsymbol{x}_i)} \hat{y}_j \right|.\]</span> It uses <span class="math inline">\(k\)</span>-Nearest Neighbours to measure the consistency of model classifications locally in input space in an effort to quantify individual fairness in a dataset. Values close to one indicate that similar inputs are treated similarly.</p></li>
</ul>
</section>
</section>
<section id="ch_UtilityFairness" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Fairness as Utility</h1>
<div class="chapsumm">
<p><strong>This chapter at a glance</strong></p>
<ul>
<li><p>Inequality indices for ranking distributions.</p></li>
<li><p>Subgroup decomposability of generalised entropy indices</p></li>
<li><p>A unified approach to measuring fairness across individuals and groups</p></li>
<li><p>Minimising inequality as maximising utility</p></li>
<li><p>Analysing the behaviour of the index as a function of model performance metrics</p></li>
</ul>
</div>
<p>In this chapter we review inequality indices and their application in measuring algorithmic fairness. More specifically, “measuring how unequally the outcomes of an algorithm, benefit different individuals or groups in a population”<span class="citation" data-cites="IneqIndPaper"><a href="#ref-IneqIndPaper" role="doc-biblioref">[63]</a></span><span class="marginnote"><span id="ref-IneqIndPaper" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[63] </span><span class="csl-right-inline">T. Speicher <em>et al.</em>, <span>“A unified approach to quantifying algorithmic unfairness,”</span> <em>Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em>, 2018, doi: <a href="https://doi.org/10.1145/3219819.3220046">10.1145/3219819.3220046</a>.</span>
</span>
</span>. Inequality indices measure divergence from the uniform distribution and as such are an important tool for measuring fairness. They are used extensively in economics and social development to measure inequality in metrics across individuals and groups in a population. Indices such as the coefficient of variation, Gini and Theil are well known tools for measuring income inequality. Their application extends beyond fairness to any problem where there is value in understanding how far from uniformly distributed some attribute is, for example measuring racial segregation and the efficiency of distributed systems.</p>
<p>Let’s dive in with an overview of the proposed application to predictive algorithms. Using inequality indices to measure algorithmic unfairness is a two step process. First, we must define a <em>benefit function</em> which maps the outcome of an algorithm to the corresponding benefit. Next, given the resulting benefits for a group of individuals, we can calculate the value of the <em>index</em> for that group, by simply plugging the values into the formula for the index. The value of the index provides a measure of how unfair the algorithm is, in its distribution of benefits over that group. The larger the value of the index or inequality measure, the more unequally, the benefits are distributed. There are then two fundamental questions we must answer in measuring algorithmic unfairness in this way.</p>
<ol>
<li><p><strong>Index calculation:</strong> There are lots of indices available that each rank inequality in different ways, which one should we use?</p></li>
<li><p><strong>Benefit function:</strong> How do we map our predictions to benefits?</p></li>
</ol>
<p>Inspired by the work of Speicher, Heidari et al.<span class="citation" data-cites="IneqIndPaper"><a href="#ref-IneqIndPaper" role="doc-biblioref">[63]</a></span> (here on in referred to as the <em>original paper</em>), we discuss these questions, specifically for algorithmic classifiers. Following their work, we focus our attention on <em>generalised entropy indices</em>, a special family of inequality indices that are <em>subgroup decomposable</em> into a <em>between-group</em> component and a <em>within-group</em> component. We show how generalised entropy indices can be viewed as the class of <em>subgroup decomposable loss functions</em>. We analyse the effect of the generalisation parameter <span class="math inline">\(\alpha\)</span> and show that for <span class="math inline">\(\alpha=0\)</span>, the index is a linear function of the cross entropy loss. We show that in the special case <span class="math inline">\(\alpha=1/2\)</span>, the contribution to the total loss from the between-group component is maximised. For the benefit function proposed in the original paper, which we describe as <em>equal luck</em>, we provide an analytical account of the index’s behaviour as a function of the generalisation parameter <span class="math inline">\(\alpha\)</span>, model accuracy <span class="math inline">\(\lambda\)</span>, and mean error <span class="math inline">\(\mu\)</span>.</p>
<section id="properties-of-inequality-indices" class="level4 unnumbered">
<h4 class="unnumbered">Properties of Inequality Indices</h4>
<p>Before getting into the particular family of indices we’ll focus on in this chapter, we mention some more general properties of inequality indices that describe their behaviour. To do this it will help to introduce some notation. We denote the benefits received by each individual, in our population size <span class="math inline">\(n\)</span>, as the vector <span class="math inline">\(b=(b_1, b_2,...,b_n)\)</span>. We use <span class="math inline">\((b_{(1)}, b_{(2)},...,b_{(n)})\)</span> to denote the benefit vector, <span class="math inline">\(\boldsymbol{b}\)</span> sorted in ascending order. We denote our inequality measure with <span class="math inline">\(I\)</span> where <span class="math inline">\(I:\mathbb{R}^n_{\geq 0}\mapsto\mathbb{R}_{\geq 0}\)</span>; that is, the inequality measure maps a vector of <span class="math inline">\(n\)</span> non-negative, real valued benefits <span class="math inline">\(\boldsymbol{b}\)</span> to a positive real number <span class="math inline">\(I(\boldsymbol{b})\geq0\)</span>.</p>
<section id="anonymity" class="level5 unnumbered">
<h5 class="unnumbered">Anonymity</h5>
<p>The index value depends only on the benefits. No other characteristics of the individuals are relevant. The inequality measure is a function of <span class="math inline">\(\boldsymbol{b}\)</span> alone (not the individuals features <span class="math inline">\(\boldsymbol{X}\)</span> or <span class="math inline">\(\boldsymbol{Z}\)</span>). It does not matter who earned which benefit and neither does the order of the benefits <span class="math inline">\(b_i\)</span> in the benefit vector <span class="math inline">\(\boldsymbol{b}\)</span>.</p>
</section>
<section id="scale-invariance" class="level5 unnumbered">
<h5 class="unnumbered">Scale invariance</h5>
<p>The value of the index does not change under a constant scaling of the benefits. That is, for any constant <span class="math inline">\(c&gt;0\)</span>, <span class="math inline">\(I(c\boldsymbol{b})=I(\boldsymbol{b})\)</span>.</p>
</section>
<section id="transfer-principle" class="level5 unnumbered">
<h5 class="unnumbered">Transfer principle</h5>
<p>Transferring benefit, from a higher benefit individual to a lower benefit individual, must decrease the value of the measure, provided the amount of benefit transferred, does not exceed the amount required for the individuals to switch places in their benefit ranking. That is, for any <span class="math inline">\(1\leq i&lt;j\leq n\)</span> and <span class="math inline">\(0&lt;\delta&lt;(b_{(j)}-b_{(i)})/2\)</span>, we must have <span class="math display">\[I(b_{(1)},...,b_{(i)}+\delta,...,b_{(j)}-\delta,....,b_{(n)}) &lt; I(\boldsymbol{b}).\]</span></p>
</section>
<section id="zero-normalisation" class="level5 unnumbered">
<h5 class="unnumbered">Zero-normalisation</h5>
<p>The measure achieves the minimal value of zero, when all individuals receive the same benefit. That is, for any <span class="math inline">\(b&gt;0\)</span>, <span class="math inline">\(I(b,b,...,b)=0\)</span></p>
</section>
</section>
<section id="generalised-entropy-indices" class="level2" data-number="5.1">
<h2 data-number="5.1"><span class="header-section-number">5.1</span> Generalised Entropy Indices</h2>
<p>In our analysis, we consider the one parameter family of inequality metrics known as <em>generalised entropy indices</em>. These represent the (entire) class of inequality measures that are <em>additively decomposable</em><span class="citation" data-cites="Shorrocks"><a href="#ref-Shorrocks" role="doc-biblioref">[64]</a></span><span class="marginnote"><span id="ref-Shorrocks" class="csl-entry" role="doc-biblioentry">
<span class="csl-left-margin">[64] </span><span class="csl-right-inline">A. F. Shorrocks, <span>“The class of additively decomposable inequality measures,”</span> <em>Econometrica: Journal of the Econometric Society</em>, vol. 48, no. 613–625, 1980.</span>
</span>
</span>. This means that for any given partition of a population into distinct subgroups, generalised entropy indices can be decomposed as, the sum of a <em>between-group</em> (or <em>intergroup</em>) component, and a <em>within-group</em> (or <em>intragroup</em>) component.</p>
<section id="between-group-component" class="level5 unnumbered">
<h5 class="unnumbered">Between-group component</h5>
<p>The between-group component is computed as the value of the index, assuming all individuals receive the mean benefit, of the partition to which they belong. Essentially, it measures the contribution to the inequality index, from differences in the average benefit between the subgroups (akin to the notion of group fairness we discussed in chapter <a href="#ch_GroupFairness" data-reference-type="ref" data-reference="ch_GroupFairness">3</a>, except here, the relative sizes of the subgroups matter). If all the groups have the same mean benefit the between-group component is zero.</p>
</section>
<section id="within-group-component" class="level5 unnumbered">
<h5 class="unnumbered">Within-group component</h5>
<p>The within-group component is computed as a weighted sum of the index value for each subgroup, and can be thought of as measuring the contribution to overall (individual) unfairness, arising from variation in benefits between individuals in the subgroups. For a within-group component to be zero, we require every individual in the subgroup to have exactly the same benefit.<br />
The ability to additively decompose these inequality measures into intergroup and intragroup components, is arguably where their value lies. The group fairness measures in chapter <a href="#ch_GroupFairness" data-reference-type="ref" data-reference="ch_GroupFairness">3</a>, make pairwise comparisons of groups. Thanks to their property of additive decomposability, generalised entropy indices have the advantage of providing a principled way of aggregating the fairness measures over any number of subgroups of the population. Historically, much of the research and development of techniques for reducing algorithmic bias, has focussed on improving group fairness metrics. Generalised entropy indices then, provide a simple way to see when trade-offs between the different notions of fairness (between-group and within-group) might occur.</p>
</section>
<section id="properties-of-generalised-entropy-indices" class="level4 unnumbered">
<h4 class="unnumbered">Properties of Generalised Entropy Indices</h4>
<p>Let’s summarise the more specific properties of generalised entropy indices which make them of particular interest for measuring unfairness.</p>
<section id="subgroup-decomposability" class="level5 unnumbered">
<h5 class="unnumbered">Subgroup decomposability</h5>
<p>For any partition <span class="math inline">\(G\)</span> of the population into (mutually exclusive) subgroups, the measure <span class="math inline">\(I(\boldsymbol{b})\)</span> can be written as the sum of a between-group component <span class="math inline">\(I_{\beta}^G(\boldsymbol{b})\)</span> (calculated as the value of the index where all individuals are assigned the mean benefit of their subgroup) and a within-group component <span class="math inline">\(I_{\omega}^G(\boldsymbol{b})\)</span> (calculated as a weighted sum of the index values for the subgroups).</p>
</section>
<section id="population-invariance" class="level5 unnumbered">
<h5 class="unnumbered">Population invariance</h5>
<p>The measure does not depend on the size of the population. More specifically, the value of the inequality measure does not change if we increase the population under consideration by replicating it <span class="math inline">\(k\)</span> times. That is, if <span class="math inline">\(\boldsymbol{b}&#39; = \langle\boldsymbol{b}, \boldsymbol{b},...,\boldsymbol{b}\rangle \in\mathbb{R}^{kn}_{\geq 0}\)</span> is a <span class="math inline">\(k\)</span>-replication of <span class="math inline">\(\boldsymbol{b}\)</span>, then <span class="math inline">\(I(\boldsymbol{b}&#39;)=I(\boldsymbol{b})\)</span>. Note that generalised entropy indices are the only differentiable family of inequality indices, which satisfy both population and scale invariance.</p>
</section>
</section>
<section id="index-calculation" class="level3" data-number="5.1.1">
<h3 data-number="5.1.1"><span class="header-section-number">5.1.1</span> Index Calculation</h3>
<div class="lookbox">
<p><strong>Generalised Entropy Indices</strong></p>
<p>The generalised entropy index for benefits <span class="math inline">\(b_1, b_2,...,b_n\)</span> with mean benefit <span class="math inline">\(\mu\)</span> can be written as</p>
<div id="eq:GEI0">
<table>
<colgroup>
<col style="width: 90%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
I_{\alpha}(\boldsymbol{b}) = \frac{1}{n}\sum_{i=1}^n f_{\alpha}(x_i) \quad\textrm{where}\quad x_i = \frac{b_i}{\mu}.\]</span></td>
<td style="text-align: right;">(5.1)</td>
</tr>
</tbody>
</table>
</div>
<p><span class="math inline">\(x_i\)</span> denotes what proportion of the mean benefit, individual <span class="math inline">\(i\)</span> received. <span class="math inline">\(\alpha\)</span> is a free parameter that determines the strength of the contribution to the index, from different parts of the benefit distribution.</p>
<div id="eq:GEI">
<table>
<colgroup>
<col style="width: 90%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
f_{\alpha}(x) = \left\{
\begin{array}{cl}
-\ln x &amp; \textrm{if}\quad \alpha=0 \\
x\ln x &amp; \textrm{if}\quad \alpha=1 \\
\rule{0em}{3.5ex}
\dfrac{x^{\alpha}-1}{\alpha(\alpha-1)}
       &amp; \textrm{if}\quad \alpha\in\mathbb{R}.
\end{array}\right.\]</span></td>
<td style="text-align: right;">(5.2)</td>
</tr>
</tbody>
</table>
</div>
</div>
<section id="observation-1." class="level5 unnumbered">
<h5 class="unnumbered">Observation 1.</h5>
<p>For <span class="math inline">\(\alpha\leq0\)</span> the index is undefined for zero benefits (since <span class="math inline">\(f_{\alpha}(x)\rightarrow\infty\)</span> as <span class="math inline">\(x\rightarrow0\)</span>), making it unsuitable for measuring inequality where zero benefits are possible.<br />
Given an array of benefits we can calculate what proportion of the total benefit each individual received by dividing their benefit by the sum of the benefits in the array. If the total benefit is equally divided among the population, each individual receives the mean benefit <span class="math inline">\(\mu\)</span>. <span class="math display">\[S_b = \sum_{i=1}^n b_i \qquad\textrm{and}\qquad \mu = \frac{S_b}{n}.\]</span> If we divide the benefits by the total, <span class="math inline">\(p_i=b_i/S_b\)</span>, all elements of our array are bounded between zero and one. If we divide the benefits by the mean benefit (instead of the sum), we calculate <span class="math inline">\(x_i=b_i/\mu\)</span> which tells us how many times the <em>fair</em> (mean) amount each individual received. Notice that,</p>
<div id="eq:benefit2prob">
<table>
<colgroup>
<col style="width: 90%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
\boldsymbol{x} = \frac{\boldsymbol{b}}{\mu} = n\boldsymbol{p} \qquad\Rightarrow\qquad \boldsymbol{b} = n\mu \boldsymbol{p}\]</span></td>
<td style="text-align: right;">(5.3)</td>
</tr>
</tbody>
</table>
</div>
<p>where <span class="math inline">\(p_i\)</span> is the proportion of the total benefit ascribed to individual <span class="math inline">\(i\)</span>. Since the index is scale invariant we know that <span class="math inline">\(I_{\alpha}(\boldsymbol{b})=I_{\alpha}(\boldsymbol{p})\)</span>. Since <span class="math inline">\(p_i\in[0,1]\;\forall\; i\)</span>, we know that <span class="math inline">\(x\in[0,n]\)</span>.</p>
</section>
<section id="observation-2." class="level5 unnumbered">
<h5 class="unnumbered">Observation 2.</h5>
<p>Let <span class="math inline">\(B\)</span> and <span class="math inline">\(P\)</span> denote the random variables that generate <span class="math inline">\(b_i\)</span> and <span class="math inline">\(p_i\)</span> respectively. We know that <span class="math inline">\(\mathbb{E}(B)=\mu\)</span> and <span class="math inline">\(\mathbb{E}(P)=1/n\)</span>. The generalised entropy index can be written as,</p>
<div id="eq:Efnp">
<table>
<colgroup>
<col style="width: 90%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
I_{\alpha}(\boldsymbol{b}) = \mathbb{E}\left[f_{\alpha}(B/\mu)\right]
\qquad\textrm{or}\qquad
I_{\alpha}(\boldsymbol{p}) = \mathbb{E}\left[f_{\alpha}(nP)\right]\]</span></td>
<td style="text-align: right;">(5.4)</td>
</tr>
</tbody>
</table>
</div>
<p>We know that inequality indices measure divergence from the uniform distribution and we can think of them as a system for ranking distributions from most to least fair. The most fair (and least uncertain) distribution, where everyone receives the mean benefit, has an index value of zero. In this case the benefit distribution has all of its weight at the mean <span class="math inline">\(\mu\)</span>. We can write the distribution of benefits in this case as <span class="math inline">\(\delta(b-\mu)\)</span> where <span class="math inline">\(\delta\)</span> is the <em>delta function</em> (see appendix <a href="#app_Notation" data-reference-type="ref" data-reference="app_Notation">A</a>). To understand the role of the generalisation parameter in ranking, consider two closely related distributions with the same mean, illustrated in Figure <a href="#fig:MirrorDist" data-reference-type="ref" data-reference="fig:MirrorDist">5.1</a>. The first distribution <span class="math inline">\(f(b)\)</span> is skewed, and the second is its reflection in the mean <span class="math inline">\(f(\mu-b)\)</span>.</p>
<figure>
<img src="05_UtilityFairness/figures/Fig_MirrorDistributions.png" id="fig:MirrorDist" style="width:65.0%" alt="Figure 5.1: Comparing benefit distributions" />
<figcaption aria-hidden="true">Figure 5.1: Comparing benefit distributions</figcaption>
</figure>
<p>Which distribution of benefits is preferred? The generalisation parameter <span class="math inline">\(\alpha\)</span> determines the weight <span class="math inline">\(f_{\alpha}(b/\mu)\)</span> applied to different parts of the distribution in calculating it’s ranking.</p>
</section>
<section id="observation-3." class="level5 unnumbered">
<h5 class="unnumbered">Observation 3.</h5>
<p>From equation (5.2) we can show that, <span class="math display">\[f_{\alpha}&#39;(x) = \left\{
\begin{array}{cl}
-1/x                    &amp; \textrm{if}\quad \alpha=0 \\
1+\ln x                 &amp; \textrm{if}\quad \alpha=1 \\
x^{\alpha-1}/(\alpha-1) &amp; \textrm{if}\quad \alpha\in\mathbb{R}
\end{array}\right\}
\qquad\textrm{and}\qquad
f_{\alpha}&#39;&#39;(x) = x^{\alpha-2}.\]</span> Note that since <span class="math inline">\(x&gt;0, \;f&#39;&#39;_{\alpha}(x)&gt;0\;\forall\;\alpha\)</span>, thus <span class="math inline">\(f_{\alpha}(x)\)</span> is convex.</p>
</section>
</section>
<section id="special-cases" class="level3" data-number="5.1.2">
<h3 data-number="5.1.2"><span class="header-section-number">5.1.2</span> Special Cases</h3>
<p>Let’s review some familiar special cases of the parameter <span class="math inline">\(\alpha\)</span> starting with zero. Suppose we have the results of a classification model on a sample of <span class="math inline">\(n\)</span> individuals <span class="math inline">\(\boldsymbol{P}\)</span> together with the true outcome, a <span class="math inline">\(\boldsymbol{y}\)</span>. Let <span class="math inline">\(p_i=\mathbb{P}(y_i|\boldsymbol{P}_i)\)</span> denote the probability of observing the true outcome, <span class="math inline">\(y_i\)</span> for individual <span class="math inline">\(i\)</span> according to our model. Recall that the cross entropy loss is given by <span class="math display">\[\mathcal{L}(\boldsymbol{P},\boldsymbol{y}) =
\mathcal{L}(\boldsymbol{p}) = -\sum_{i=1}^n \ln(p_i).\]</span> The cross-entropy loss is minimised at a value of zero, when all probabilities are unity. The loss is unbounded above, <span class="math inline">\(\mathcal{L}(\boldsymbol{p})\rightarrow\infty\)</span> as <span class="math inline">\(p_i\rightarrow0\)</span> . Notice all probabilities must be greater than zero for the integrand to be defined. We see that generalised entropy indices can be thought of as a subgroup decomposable family of loss functions. The commonly used case for training classifiers is <span class="math inline">\(\alpha=0\)</span>.</p>
<div class="lookbox">
<p><strong><span class="math inline">\(I_0\)</span> and Cross Entropy Loss</strong></p>
<div id="eq:CrossEntropyLoss">
<table>
<colgroup>
<col style="width: 90%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
\mathcal{L}(\boldsymbol{p}) = n I_0(n\mu\boldsymbol{p}) + n\ln(n)\]</span></td>
<td style="text-align: right;">(5.5)</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Recall that entropy is calculated as follows, <span class="math display">\[\mathrm{entropy}(\boldsymbol{p}) = -\sum_{i=1}^n p_i\ln(p_i),\]</span> where <span class="math inline">\(p_i\)</span> is the probability of the <span class="math inline">\(i\)</span>th possible event. One interpretation of entropy is as a measure of uncertainty which is inversely related to equality. The most uncertain distribution is the most equal, the <em>uniform distribution</em>. It assigns every possible outcome, the same probability. The least uncertain distribution is most unequal, the <em>delta distribution</em>, where one outcome occurs with probability one, and all others have zero probability. For a uniform distribution with <span class="math inline">\(n\)</span> possible events, each event occurs with probability of <span class="math inline">\(\frac{1}{n}\)</span>, in which case entropy has a maximal value of <span class="math display">\[\max_{\boldsymbol{p}}\{\mathrm{entropy}\} = \ln(n).\]</span> For the delta distribution, the entropy is minimal with a value of zero.</p>
<div class="lookbox">
<p><strong><span class="math inline">\(I_1\)</span> and Entropy</strong></p>
<div id="eq:Entropy">
<table>
<colgroup>
<col style="width: 90%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
I_1(\boldsymbol{b}) = I_1(n\mu\boldsymbol{p}) = \max_{\boldsymbol{p}}\{\mathrm{entropy}\} - \mathrm{entropy}(\boldsymbol{p}).\]</span></td>
<td style="text-align: right;">(5.6)</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>For <span class="math inline">\(\alpha=1\)</span>, the generalised entropy index, is also known as the Theil index.</p>
<p>In the special case <span class="math inline">\(\alpha=2\)</span>, the generalised entropy index is a monotonic increasing function of the <em>relative standard deviation</em> (the standard deviation divided by the mean, also known as the coefficient of variation).</p>
<div class="lookbox">
<p><strong><span class="math inline">\(I_2\)</span> and Relative Standard Deviation</strong></p>
<div id="eq:RelStdDev">
<table>
<colgroup>
<col style="width: 90%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
\frac{\sigma}{\mu} = \sqrt{2I_2(\boldsymbol{b})}.\]</span></td>
<td style="text-align: right;">(5.7)</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>The standard deviation <span class="math inline">\(\sigma\)</span> tells us how spread out (around the mean) the distribution of benefits is. So for <span class="math inline">\(\alpha=2\)</span>, the index is a monotonic increasing function of the spread and a monotonic decreasing function of the mean benefit.</p>
<p>Recall that the Gini index is calculated as <span class="math display">\[\mathrm{Gini}(\boldsymbol{p}) = 1 - \sum_{i=1}^n p_i^2\]</span> <span class="math inline">\(I_2\)</span> is also related to the Gini index.</p>
<div class="lookbox">
<p><strong><span class="math inline">\(I_2\)</span> and Gini Index</strong></p>
<div id="eq:Gini">
<table>
<colgroup>
<col style="width: 90%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
\frac{2}{n} \left[ I_2(\boldsymbol{b}) + n^2\right]
= 1 - \mathrm{Gini}(\boldsymbol{p})\]</span></td>
<td style="text-align: right;">(5.8)</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>The Atkinson index, which can be written as <span class="math display">\[A_{\epsilon} = 1 - \frac{1}{\mu} \left(\frac{1}{n}\sum_{i=1}^n b_i^{1-\epsilon}\right)^{1/(1-\epsilon)},\]</span> is related to the generalised entropy index as follows.</p>
<div class="lookbox">
<p><strong><span class="math inline">\(I_{\alpha}\)</span> and the Atkinson Index <span class="math inline">\(A_{\epsilon}\)</span></strong></p>
<div id="eq:Atkinson">
<table>
<colgroup>
<col style="width: 90%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
1 + \alpha(\alpha-1)I_{\alpha}(\boldsymbol{b})
= \left[ 1 - A_{\epsilon}(\boldsymbol{b})\right]^{\alpha}\]</span></td>
<td style="text-align: right;">(5.9)</td>
</tr>
</tbody>
</table>
</div>
<p>where <span class="math inline">\(\epsilon=1-\alpha\geq0\)</span>.</p>
</div>
</section>
<section id="behaviour-with-respect-to-generalisation-parameter-alpha" class="level3" data-number="5.1.3">
<h3 data-number="5.1.3"><span class="header-section-number">5.1.3</span> Behaviour with Respect to Generalisation Parameter <span class="math inline">\(\alpha\)</span></h3>
<p>In Figure <a href="#fig:fxvaralpha" data-reference-type="ref" data-reference="fig:fxvaralpha">5.2</a>, we plot the function <span class="math inline">\(f_{\alpha}(x)\)</span>, for different choices of <span class="math inline">\(\alpha\)</span>.</p>
<figure>
<img src="05_UtilityFairness/figures/Fig_fxvaralpha.png" id="fig:fxvaralpha" style="width:65.0%" alt="Figure 5.2: f_{\alpha}(x) for varying \alpha." />
<figcaption aria-hidden="true">Figure 5.2: <span class="math inline">\(f_{\alpha}(x)\)</span> for varying <span class="math inline">\(\alpha\)</span>.</figcaption>
</figure>
<p>We note that the contribution to the index, from individuals that receive the mean benefit, is always zero. As we increase <span class="math inline">\(\alpha\)</span>, the contribution to the index from the upper end of the benefit distribution grows, while the contribution from the lower end decays.</p>
<section id="for-alpha1" class="level5 unnumbered">
<h5 class="unnumbered">For <span class="math inline">\(\alpha&lt;1\)</span>:</h5>
<p>A fixed transfer in benefit (from rich to poor) at the low end of the distribution (where <span class="math inline">\(f_{\alpha}(x)\)</span> is steeply declining), decreases the the value of the index more than at the top end (where <span class="math inline">\(f_{\alpha}(x)\)</span> is flatter).</p>
</section>
<section id="for-alpha1-1" class="level5 unnumbered">
<h5 class="unnumbered">For <span class="math inline">\(\alpha&gt;1\)</span>:</h5>
<p>The reverse is true. A fixed transfer in benefit (from rich to poor) at the upper end of the distribution (where the <span class="math inline">\(f_{\alpha}(x)\)</span> is steeply increasing), decreases the the value of the index more than at the lower end (where the <span class="math inline">\(f_{\alpha}(x)\)</span> is flatter).<br />
One interpretation of this is that For <span class="math inline">\(\alpha&lt;1\)</span>, the index prioritises equality for the poor, while for <span class="math inline">\(\alpha&gt;1\)</span> equality is prioritised for the rich. Recall that Rawls’ maximin principle as the requirement that, <em>social and economic inequalities must be of the greatest benefit to the least-advantaged members of society</em>. As <span class="math inline">\(\alpha\rightarrow-\infty\)</span>, the associated rankings of distributions correspond to those generated by maximin principle<span class="citation" data-cites="Shorrocks"><a href="#ref-Shorrocks" role="doc-biblioref">[64]</a></span>.</p>
<div class="lookbox">
<p><strong>Behaviour of <span class="math inline">\(f_{\alpha}(x)\)</span></strong></p>
<ul>
<li><p>For <span class="math inline">\(\alpha&lt;1\)</span>, <span class="math inline">\(f_{\alpha}(x)\)</span> is a strictly decreasing.</p></li>
<li><p>For <span class="math inline">\(\alpha=1\)</span>, <span class="math inline">\(f_{\alpha}(x)\)</span> is minimal at <span class="math inline">\(x=e^{-1}\)</span>.</p></li>
<li><p>For <span class="math inline">\(\alpha&gt;1\)</span>, <span class="math inline">\(f_{\alpha}(x)\)</span> is a strictly increasing.</p></li>
</ul>
<p><a href="#II_falphax">Proof</a> in appendix <a href="#sec_app_IISolutions" data-reference-type="ref" data-reference="sec_app_IISolutions">D.3</a>.</p>
</div>
</section>
</section>
<section id="index-decomposition-over-partitions" class="level3" data-number="5.1.4">
<h3 data-number="5.1.4"><span class="header-section-number">5.1.4</span> Index Decomposition over Partitions</h3>
<div class="lookbox">
<p><strong>Generalised Entropy Index Decomposition</strong></p>
<p>For any partition <span class="math inline">\(G\)</span> of the population into subgroups, the generalised entropy index <span class="math inline">\(I\)</span>, is additively decomposable, into a within-group component <span class="math inline">\(I_{\omega}^G\)</span>, and between-group component <span class="math inline">\(I_{\beta}^G\)</span>, <span class="math display">\[\begin{aligned}
I(\boldsymbol{b};\alpha)
= \frac{1}{n}\sum_{i=1}^n f_{\alpha}\left(\frac{b_i}{\mu}\right)
= I_{\omega}^G(\boldsymbol{b};\alpha) + I_{\beta }^G(\boldsymbol{b};\alpha).
\end{aligned}\]</span> The within-group component is the weighted sum of the index measure for each subgroup</p>
<div id="eq:GEIomega">
<table>
<colgroup>
<col style="width: 90%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
I_{\omega}^G(\boldsymbol{b};\alpha) = \sum_{g=1}^{|G|} \frac{n_g}{n} \left(\frac{\mu_g}{\mu}\right)^{\alpha} I(\boldsymbol{b}_g;\alpha)
\qquad \forall \, \alpha.\]</span></td>
<td style="text-align: right;">(5.10)</td>
</tr>
</tbody>
</table>
</div>
<p>The between-group component is computed as the value of the index in the case where, each individual is assigned the mean benefit of their subgroup,</p>
<div id="eq:GEIbeta">
<table>
<colgroup>
<col style="width: 90%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
I_{\beta}^G(\boldsymbol{b};\alpha) = \sum_{g=1}^{|G|}
\frac{n_g}{n} f_{\alpha}\left(\frac{\mu_g}{\mu}\right).\]</span></td>
<td style="text-align: right;">(5.11)</td>
</tr>
</tbody>
</table>
</div>
<p><a href="#II_GEIdecomp">Proof</a> in appendix <a href="#sec_app_IISolutions" data-reference-type="ref" data-reference="sec_app_IISolutions">D.3</a>.</p>
</div>
<p>We describe the value of the index on the population as <em>overall unfairness</em>. There are several noteworthy observations to be made from the functional forms of the indices in equations (5.10) and 5.11.</p>
<section id="observation-4." class="level5 unnumbered">
<h5 class="unnumbered">Observation 4.</h5>
<p>The contribution to the between-group component, from each subgroup, is weighted by the size of the subgroup. This serves to favour more prevalent groups in the data. It could be argued then, that the between-group component of the index (as an approach to measuring group fairness) is more aligned with utilitarian principles than those described in chapter <a href="#ch_GroupFairness" data-reference-type="ref" data-reference="ch_GroupFairness">3</a> which do not account for group sizes. Arguably, this is intentional since aggregating as we do to calculate utility, can hide adverse impacts on underrepresented groups. Ignoring group sizes makes them less reliant on the assumption of representativeness of the data (with respect to those groups).</p>
</section>
<section id="observation-5." class="level5 unnumbered">
<h5 class="unnumbered">Observation 5.</h5>
<p>The number of subgroups, greatly influences the size of the relative contributions from the between-group and within-group components of the inequality index. Notice that to calculate the between-group component, we first average the benefits over each group to get their means. We then calculate the value of the index on the means. The fewer subgroups, the fewer elements there are to sum in the between-group component. For large groups, <span class="math inline">\(\mu_g/\mu\)</span> is close to unity and <span class="math inline">\(f_{\alpha}(\mu_g/\mu)\)</span> is close to zero. Consider partitioning our population into subgroups of equal sizes. At one extreme, we have only a single group. In this case, the contribution from the between-group component is zero, and the index is equal to the within-group component. As the number of subgroups in the partition increases, the subgroups get smaller and the relative contribution to the index from the between-group component increases. Eventually, we have <span class="math inline">\(n\)</span> groups, each composed of a single individual. In this case, the within-group component is zero, and the index is equal to the between-group component.</p>
</section>
<section id="observation-6." class="level5 unnumbered">
<h5 class="unnumbered">Observation 6.</h5>
<p>For the values <span class="math inline">\(\alpha=0\)</span> and <span class="math inline">\(\alpha=1\)</span>, the within-group component is a true weighted average of the index values for the subgroups, since the coefficients sum to one. For <span class="math inline">\(\alpha\in(0,1)\)</span> the coefficients sum to less than unity, For <span class="math inline">\(\alpha&gt;1\)</span>, the coefficients sum to more than unity. The sum of the coefficients is minimised for <span class="math inline">\(\alpha=1/2\)</span>.</p>
<div class="lookbox">
<p><strong>Relative contribution from the Between and Within-group Components</strong></p>
<p>By substituting for <span class="math inline">\(f_{\alpha}\)</span> in the between group component, equation (5.11), it’s straightforward to prove that for <span class="math inline">\(\alpha\in\mathbb{R}\)</span>, <span class="math inline">\(\alpha\notin\{0,1\}\)</span>, the sum of coefficients is linearly dependent on between-group component. In particular, <span class="math display">\[\sum_{g=1}^{|G|} \frac{n_g}{n} \left(\frac{\mu_g}{\mu}\right)^{\alpha}
= 1 + \alpha(\alpha-1) I_{\beta}^{G}(\boldsymbol{b}; \alpha).\]</span> The relative contribution to the index from the between-group component is maximised when <span class="math inline">\(\alpha=1/2\)</span>, in which case the sum of the coefficients of the within-group component are given by, <span class="math display">\[\sum_{g=1}^{|G|} \frac{n_g}{n} \sqrt{\frac{\mu_g}{\mu}}
= 1 - \frac{1}{2} I_{\beta}^{G}(\boldsymbol{b}; \alpha).\]</span></p>
</div>
</section>
</section>
<section id="sec_GEImax" class="level3" data-number="5.1.5">
<h3 data-number="5.1.5"><span class="header-section-number">5.1.5</span> Generalised Entropy Index Maximums</h3>
<p>For <span class="math inline">\(\alpha&gt;0\)</span> and fixed <span class="math inline">\(n\)</span>, the value of the index is capped. The maximum benefit any individual can receive is the total benefit which is <span class="math inline">\(n\)</span> times the mean, <span class="math inline">\(b_{\max}=n\mu\)</span>, in which case <span class="math inline">\(x_{\max}=n\)</span> and <span class="math inline">\(p_{\max}=1\)</span>. The maximal value of the index is attained when only a single individual benefits.</p>
<div class="lookbox">
<p><strong>Generalised Entropy Index Maximum</strong></p>
<p><span class="math display">\[\max_{\boldsymbol{b}}[I_{\alpha}(\boldsymbol{b})] = \left\{
\begin{array}{cl}
\ln n &amp; \textrm{if}\quad\alpha=1 \\
\dfrac{n^{\alpha-1}-1}{\alpha(\alpha-1)}
      &amp; \textrm{if}\quad\alpha&gt;0
\end{array}\right.\]</span> <a href="#II_GEImax">Proof</a> in appendix <a href="#sec_app_IISolutions" data-reference-type="ref" data-reference="sec_app_IISolutions">D.3</a>.</p>
</div>
<p>In Figure <a href="#fig:GEI_Imax" data-reference-type="ref" data-reference="fig:GEI_Imax">5.3</a>, we plot the maximal value of the index as a function of <span class="math inline">\(n\)</span> for different values of <span class="math inline">\(\alpha&gt;0\)</span>.</p>
<figure>
<img src="05_UtilityFairness/figures/Fig_GEI_Imaxvaralpha.png" id="fig:GEI_Imax" style="width:65.0%" alt="Figure 5.3: \max[I_{\alpha}(n)] for varying values of \alpha. The maximal value is attained when only a single individual benefits. For \alpha\leq0 the index is unbounded above." />
<figcaption aria-hidden="true">Figure 5.3: <span class="math inline">\(\max[I_{\alpha}(n)]\)</span> for varying values of <span class="math inline">\(\alpha\)</span>. The maximal value is attained when only a single individual benefits. For <span class="math inline">\(\alpha\leq0\)</span> the index is unbounded above.</figcaption>
</figure>
<p>The maximal value of the index is always an increasing function of <span class="math inline">\(n\)</span>. For <span class="math inline">\(\alpha=2\)</span>, the maximal value of the index is a linear function of <span class="math inline">\(n\)</span>, <span class="math display">\[\max_{\boldsymbol{b}}[I_{2}(\boldsymbol{b})] = \frac{n-1}{2}.\]</span> For <span class="math inline">\(0&lt;\alpha&lt;1\)</span>, <span class="math display">\[\max_{\boldsymbol{b}}[I_{\alpha}(n)] = \frac{1-n^{-(1-\alpha)}}{\alpha(1-\alpha)}\rightarrow\frac{1}{\alpha(1-\alpha)}
\quad\textrm{as}\quad n\rightarrow\infty.\]</span> For <span class="math inline">\(0&lt;\alpha&lt;1\)</span>, the index maximum has a fixed upper bound.</p>
<p>Interestingly, looking at the maximal value of the generalised entropy index (as a function of <span class="math inline">\(n\)</span>), also gives us some insight into the relative size of the between and within-group components, as we change the number of subgroups. Suppose we partition our population, into <span class="math inline">\(|G|\)</span> equally sized subgroups. Recall from equation (5.11), we can write our between-group component as, <span class="math display">\[I_{\beta}^G(\boldsymbol{b};\alpha)
= \sum_{g=1}^{|G|}\frac{n_g}{n} f_{\alpha}\left(\frac{\mu_g}{\mu}\right)
= \frac{1}{|G|}\sum_{g=1}^{|G|} f_{\alpha}\left(\frac{\mu_g}{\mu}\right);\]</span> which looks exactly like the formula for the index, given in equation (5.1). Therefore, just as the index has a maximal value, so does the between-group component, <span class="math display">\[\max_{\boldsymbol{b}}\left[I^G_{\beta}(\boldsymbol{b};\alpha)\right] = \left\{
\begin{array}{cl}
\ln(|G|) &amp; \textrm{if}\quad\alpha=1\\
\rule{0em}{4.2ex}
\dfrac{|G|^{\alpha-1}-1}{\alpha(\alpha-1)}
&amp; \textrm{if}\quad\alpha&gt;0.
\end{array}\right.\]</span> This further confirms our earlier observation, that the number of subgroups in a partition <span class="math inline">\(|G|\)</span>, greatly influences the size of the between-group component as a proportion of the index, assuming the groups to be equal in size.</p>
</section>
</section>
<section id="defining-a-benefit-function" class="level2" data-number="5.2">
<h2 data-number="5.2"><span class="header-section-number">5.2</span> Defining a Benefit Function</h2>
<p>A key component of this inequality measure is the definition of the mapping from algorithmic prediction to benefit. For the index to be meaningful, all benefits must be greater than or equal to zero, there must be at least one non-zero benefit, and benefits must be defined on a ratio scale (as oppose to an interval scale), so that relative comparisons of benefits are meaningful.</p>
<section id="ratio-scale" class="level5 unnumbered">
<h5 class="unnumbered">Ratio scale</h5>
<p>A ratio scale is defined on the basis of a unique and non-arbitrary zero value which allows meaningful interpretation of ratios. Examples are, mass, length, duration and temperature (measured in Kelvin). For example, four metres is twice as long as two metres.</p>
</section>
<section id="interval-scale" class="level5 unnumbered">
<h5 class="unnumbered">Interval scale</h5>
<p>An interval scale allows meaningful comparison of the degree of differences between values, but not ratios of the values themselves. They are characterised by the definition of an arbitrary zero or reference point. Examples include temperature (measured in Celsius or Fahrenheit) and location in a cartesian co-ordinate system. While ratios are not meaningful on an interval scale (<span class="math inline">\(100^{\circ}\)</span>C is not twice as hot as <span class="math inline">\(50^{\circ}\)</span>C), ratios of differences are. For example, one temperature difference can be twice that of another.<br />
<br />
For a binary classifier, all algorithmic predictions (where the ground truth is known) can be categorised in a confusion matrix, as either a true positive (TP), false positive (FP), false negative (FN) or true negative (TN). A benefit function can then be defined by simply assigning a non-negative benefit value, to each of the four cases, that is, <span class="math inline">\(b_{ij}=\mathrm{benefit}(\hat{y}=i, y=j)\)</span>.</p>
</section>
<section id="observation-7." class="level5 unnumbered">
<h5 class="unnumbered">Observation 7.</h5>
<p>In doing this, we make a coarse comparison of individuals. For a binary target, we bucket everyone into one of four groups and consider individuals in each group to have benefited the same amount from the algorithm regardless of their individual features or circumstances.</p>
</section>
<section id="between-group-fairness" class="level3" data-number="5.2.1">
<h3 data-number="5.2.1"><span class="header-section-number">5.2.1</span> Between-Group Fairness</h3>
<p>We noted earlier that the value of using generalised entropy indices, as a measure of fairness, lies in the property of subgroup decomposability. This property allows us to identify when trade-offs between the different notions of fairness (overall and between-group) might occur. But the ability to identify these trade-offs is only useful if the benefits are defined in such a way, that both measures of fairness (within-group and between-group) are similarly meaningful. More specifically, both uniformity of mean benefit across groups, and uniformity of benefits across individuals in the the population, must be similarly meaningful goals which achieve a <em>reasonable</em> notion of fairness. If all we care about is fairness across groups, using generalised entropy indices is arguably a rather convoluted and unnecessarily restrictive way to measure it. In this vain, let’s review some benefit functions described in the original paper. Table <a href="#tbl:clfBenefitFuncs" data-reference-type="ref" data-reference="tbl:clfBenefitFuncs">5.1</a> (adapted from the original paper <span class="citation" data-cites="IneqIndPaper"><a href="#ref-IneqIndPaper" role="doc-biblioref">[63]</a></span> for correctness and completeness) shows some examples of benefit functions for a classification model.</p>
<div id="tbl:clfBenefitFuncs">
<table>
<caption>Table 5.1: Summary of benefit function definitions corresponding to a variety of notions of fairness<span class="citation" data-cites="IneqIndPaper"><a href="#ref-IneqIndPaper" role="doc-biblioref">[63]</a></span>.</caption>
<thead>
<tr class="header">
<th colspan="2" style="text-align: left;">Between-Group Fairness</th>
<th colspan="4" style="text-align: center;">Benefit Function<sup>b</sup></th>
<th style="text-align: left;">Overall Fairness</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Comparing</td>
<td style="text-align: left;">Criterion<sup>a</sup></td>
<td style="text-align: center;"><span class="math inline">\(b_{11}\)</span> (TP)</td>
<td style="text-align: center;"><span class="math inline">\(b_{00}\)</span> (TN)</td>
<td style="text-align: center;"><span class="math inline">\(b_{10}\)</span> (FP)</td>
<td style="text-align: center;"><span class="math inline">\(b_{01}\)</span> (FN)</td>
<td style="text-align: left;">Criterion<sup>c</sup></td>
</tr>
<tr class="even">
<td rowspan="2" style="text-align: left;">Outcomes</td>
<td style="text-align: left;"><span class="math inline">\(= ACR\)</span> (data)</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: left;"><span class="math inline">\(Y=1\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(= ACR\)</span> (model)</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: left;"><span class="math inline">\(\hat{Y}=1\)</span></td>
</tr>
<tr class="even">
<td rowspan="6" style="text-align: left;">Errors</td>
<td style="text-align: left;"><span class="math inline">\(= ACC\)</span></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: left;"><span class="math inline">\(\hat{Y}=Y\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(= FPR\)</span></td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: left;"><span class="math inline">\(FPR=0\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(= FNR\)</span></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">0</td>
<td style="text-align: left;"><span class="math inline">\(FNR=0\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(= FDR\)</span></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: left;"><span class="math inline">\(FPR=0\)</span>, <span class="math inline">\(TPR&gt;0\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(= FOR\)</span></td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">n/a</td>
<td style="text-align: center;">0</td>
<td style="text-align: left;"><span class="math inline">\(FNR=0\)</span>, <span class="math inline">\(TNR&gt;0\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">= Luck<sup>d</sup></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: left;"><span class="math inline">\(\hat{Y}=Y\)</span></td>
</tr>
</tbody>
</table>
</div>
<div class="tablenotes">
<p><sup>a</sup>The criteria tells us how we achieve equality across groups, i.e. a between-group index component of zero. We abbreviate acceptance rate (ACR), accuracy (ACC), false positive rate (FPR), false negative rate (FNR), false discovery rate (FDR) and false omission rate (FOR).</p>
<p><sup>b</sup>The benefit function maps true positives (TP), true negatives (TN), false positives (FP) and false negatives (FN) to a benefit value. n/a indicates that such points are not considered under that fairness notion and thus do not contribute to the benefit array.</p>
<p><sup>c</sup>These criteria tell us the conditions under which overall fairness is achieved, i.e. an index value of zero.</p>
<p><sup>d</sup>Here we assume the positive prediction to be the advantageous outcome. In this case, false positives are lucky errors, false negatives are unlucky and accurate predictions are neither. The benefit is here is the error plus one (to ensure non-negative values).</p>
</div>
</section>
<section id="overall-fairness" class="level3" data-number="5.2.2">
<h3 data-number="5.2.2"><span class="header-section-number">5.2.2</span> Overall Fairness</h3>
<p>For the benefit functions in Table <a href="#tbl:clfBenefitFuncs" data-reference-type="ref" data-reference="tbl:clfBenefitFuncs">5.1</a>, the corresponding group fairness criteria are listed in the left two columns. It’s straightforward to see that minimising the between-group component of the index, would be desirable. The between-group component is zero, when the mean benefit for all groups are equal. What about the index? Under what conditions does the index consider the algorithm to be fair overall?</p>
<section id="binary-benefits" class="level4 unnumbered">
<h4 class="unnumbered">Binary Benefits</h4>
<p>All but the last benefit function in Table <a href="#tbl:clfBenefitFuncs" data-reference-type="ref" data-reference="tbl:clfBenefitFuncs">5.1</a> result in binary arrays of benefits. Individuals either benefit from the system or they do not. For binary benefits, the distribution of benefits can be characterised with a single parameter, the mean benefit <span class="math inline">\(\mu\)</span>.</p>
<div class="lookbox">
<p><strong>Index value for Binary Benefits</strong></p>
<p>For binary benefits, the value of the index is given by <span class="math display">\[I_{\alpha}(\boldsymbol{b}) = I_{\alpha}(\mu)
= \left\{
\begin{array}{cl}
-\ln\mu &amp; \textrm{if}\quad\alpha=1 \\
\rule{0em}{4ex}
\dfrac{1}{\alpha(\alpha-1)}\left(\dfrac{1}{\mu^{\alpha-1}}-1\right)
&amp; \textrm{if}\quad\alpha&gt;0.
\end{array}\right.\]</span> <a href="#II_Binary">Proof</a> in in appendix <a href="#sec_app_IISolutions" data-reference-type="ref" data-reference="sec_app_IISolutions">D.3</a>.</p>
</div>
<p>For binary benefits, the index is a monotonic increasing function of the mean benefit. The index is maximal where only one individual benefits. As we increase the proportion of people who benefit from from <span class="math inline">\(1/n\)</span> to <span class="math inline">\(n\)</span>, the distribution of benefits approaches uniform. As the number of individuals grows, so does the maximal value of the index (as shown in section <a href="#sec_GEImax" data-reference-type="ref" data-reference="sec_GEImax">5.1.5</a>). In Figure <a href="#fig:GEI_bmin0" data-reference-type="ref" data-reference="fig:GEI_bmin0">5.4</a>, we plot the value of the index as a function of the mean benefit <span class="math inline">\(\mu\)</span> for a variety of values of <span class="math inline">\(\alpha&gt;0\)</span>.</p>
<figure>
<img src="05_UtilityFairness/figures/Fig_GEI_bmin0_n10.png" id="fig:GEI_bmin0" style="width:65.0%" alt="Figure 5.4: Generalised entropy index as a function of the mean benefit \mu for binary benefits." />
<figcaption aria-hidden="true">Figure 5.4: Generalised entropy index as a function of the mean benefit <span class="math inline">\(\mu\)</span> for binary benefits.</figcaption>
</figure>
<p>For the benefit functions listed in Table <a href="#tbl:clfBenefitFuncs" data-reference-type="ref" data-reference="tbl:clfBenefitFuncs">5.1</a> the only way to achieve overall fairness (index value zero) is if every individual under consideration receives a benefit of exactly one. We showed this to be the case for binary benefits above. For the fairness criterion in the last row of Table <a href="#tbl:clfBenefitFuncs" data-reference-type="ref" data-reference="tbl:clfBenefitFuncs">5.1</a>, comparing luck, benefits are no longer binary. We can also achieve a zero index value if all individuals receive a benefit of two; but this is only possible in the degenerate case where all predictions are false positives, that is, <span class="math inline">\(\hat{Y}=1\)</span> and <span class="math inline">\(Y=0\)</span>. For each benefit function, the scenarios for which overall fairness is minimal (all individuals under consideration receive exactly one benefit) differs.</p>
</section>
<section id="comparing-outcomes-1" class="level4 unnumbered">
<h4 class="unnumbered">Comparing Outcomes</h4>
<p>For benefit functions comparing outcomes, the higher the acceptance rate, the lower the value of the index. We achieve a perfectly fair model (zero index value) only by accepting everyone. Recall in the previous chapter where our model was stochastic, we saw that treating every individual the same, corresponded to mapping all individuals to the same the distribution over outcomes. Here, the only way to treat all individuals the same, is to accept them all. Notice that our metric is undefined in the case where we reject all individuals.</p>
</section>
<section id="comparing-errors-1" class="level4 unnumbered">
<h4 class="unnumbered">Comparing Errors</h4>
<p>In the case where we translate group fairness criterion comparing errors, the accurate prediction is always defined as the beneficial one. In all cases we can achieve a zero index value with a 100% accurate model, i.e. <span class="math inline">\(\hat{Y}=Y\)</span>. For the benefit function corresponding to equal accuracy, our index is a monotonic function of accuracy and we can only achieve a zero index value, with a 100% accurate model. Neglecting to consider all the points means that in some cases, achieving 100% accuracy is no longer the only way to minimise the index. For example, consider the benefit function corresponding to equal false positive rates. For this we achieve a zero index value so long as the false positive rate is zero. Similar arguments apply to the benefit function corresponding to equal false negative rate, we need the false negative rate to be zero. We leave it to the reader to consider the remaining benefit functions in the Table <a href="#tbl:clfBenefitFuncs" data-reference-type="ref" data-reference="tbl:clfBenefitFuncs">5.1</a>.</p>
</section>
<section id="equal-luck" class="level4 unnumbered">
<h4 class="unnumbered">Equal Luck</h4>
<p>The final row of Table <a href="#tbl:clfBenefitFuncs" data-reference-type="ref" data-reference="tbl:clfBenefitFuncs">5.1</a> shows the benefit function proposed by Speicher, Heidari et al. which distinguishes between false negative and false positive errors. We describe the criterion as requiring equal <em>luck</em>. It assumes a positive outcome to be the most advantageous to the individual. It assigns false negative predictions a benefit of zero (the least lucky), while a false positive prediction (the most lucky) is deemed twice as beneficial as a correct prediction. The benefits in this case are a measure of the discrepancy between the individuals assigned label (prediction) and the label <em>deserved</em> according to the ground truth. In fact the benefit is exactly one plus the error, i.e. <span class="math inline">\(b_i=\hat{y}_i-y_i+1\)</span>. It provides a measure of the relative prevalence of false positive to false negative errors; that is, if the model over or underestimates on average.</p>
<p>In the original paper the index value corresponding to equal luck is described as a measure of <em>individual fairness</em> in that "individuals <em>deserving</em> similar outcomes, receive similar outcomes". Recall that <em>individual fairness</em> (as described by Dwork et. al.<span class="citation" data-cites="DworkIndFair"><a href="#ref-DworkIndFair" role="doc-biblioref">[59]</a></span> and discussed in the previous chapter), is the notion that in a fair system, similar people are treated similarly. For the benefit function associated with equal luck, the similarity of individuals is based solely on their associated error, <span class="math inline">\(\hat{y}_i-y_i\)</span> (and not on their features, as described by Dwork et. al.<span class="citation" data-cites="DworkIndFair"><a href="#ref-DworkIndFair" role="doc-biblioref">[59]</a></span>). The metric looks at the difference between the prediction and ground truth and thus clearly some measure of model performance. As demonstrated earlier, generalised entropy indices are subgroup decomposable loss functions and thus some measure of utility. That said, generalised entropy indices are a measure of <em>individual fairness</em> on some level, albeit one that makes a coarse comparison of individuals and places absolute faith in the data.</p>
</section>
</section>
<section id="overall-fairness-as-utility" class="level3" data-number="5.2.3">
<h3 data-number="5.2.3"><span class="header-section-number">5.2.3</span> Overall Fairness as Utility</h3>
<p>We know that for the benefit function corresponding to equal accuracy the index is a monotonic decreasing function of <span class="math inline">\(\mu\)</span> accuracy. Here the assumption is that false positives and false negatives are equally undesirable. For example consider the binary gender recognition systems reviewed in the project gender shades. In this case the beneficial outcome is a correct prediction. Erroneous predictions (regardless of one’s gender) are never more beneficial than correct predictions. We also know that for both <em>equal accuracy</em> and <em>equal luck</em> benefit functions, the only way to achieve an index value of zero is to have a perfectly accurate solution. It seems like (at least in these two cases), equalising benefits (minimising the index) corresponds to maximising utility (minimising the expected cost). In this case, different choices of benefit function correspond to different costs associated with different predictions, and different choices of <span class="math inline">\(\alpha\)</span> correspond to different loss functions.</p>
<p>The desirability of a given classification will, in general, depend on one’s perspective. For example, take an algorithm that predicts credit risk, and thus which interest rate (of two - high or low), a given loan applicant is eligible for. Low risk individuals are offered a low interest rate loan, while high risk customers are offered a high interest rate loan. From the perspective of the applicant, being labelled low risk will always be more desirable than being labelled high risk. From the perspective of the bank however, it would be undesirable to label high risk individuals as low risk. If we are interested in the perspective of the individual and we assume <span class="math inline">\(\hat{Y}=1\)</span> to be the advantageous outcome, then our benefit function <span class="math inline">\(\mathrm{benefit}(\hat{y}=i, y=j)=b_{ij}\)</span> must satisfy the following constraints <span class="math inline">\(b_{10}&gt;b_{00}\)</span> and <span class="math inline">\(b_{11}&gt;b_{01}\)</span>. This is because, from the perspective of the individual, a low interest rate loan (<span class="math inline">\(\hat{y}=1\)</span>) will always be better than a high interest rate loan (<span class="math inline">\(\hat{y}=0\)</span>), regardless of the actual risk level the individual presents.</p>
<p>In the case, where <span class="math inline">\(\hat{Y}=1\)</span> is the more advantageous outcome, the least beneficial prediction should be a false negative prediction, where despite presenting low risk, the individual is assigned to the high risk pool. From the perspective of the individual, nothing could be worse. False negative predictions then, should be assigned the minimum possible benefit, that is, <span class="math inline">\(b_{01}=0\)</span>. Thanks to the property of scale invariance (multiplying all the benefits in our matrix by a constant does not change the value of the index), we can choose any non-zero positive value for <span class="math inline">\(b_{11}&gt;b_{01}=0\)</span>. In fact, all other benefits in the matrix should be greater than zero (to choose <span class="math inline">\(b_{00}=b_{01}\)</span> would be to ignore the information provided by <span class="math inline">\(Y\)</span>). We choose <span class="math inline">\(b_{11}=1\)</span>. So, for our 2x2 benefit matrix representing individual fairness, we have two degrees of freedom: <span class="math display">\[\mathrm{benefit}(\hat{y}=i,y=j) = b_{ij} = \left(
\begin{array}{cc}
b_{00} &amp; 0 \\
b_{10} &amp; 1
\end{array}
\right)\]</span> where <span class="math inline">\(b_{10}&gt;b_{00}&gt;0\)</span>. The remaining benefits in the matrix <span class="math inline">\(b_{00}\)</span> and <span class="math inline">\(b_{10}\)</span> establish how beneficial they are relative a true positive prediction.</p>
<p>Suppose we restrict ourselves to the case where accurate predictions are equally beneficial (neither lucky nor unlucky), that is <span class="math inline">\(b_{00}=b_{11}=1\)</span>. Then in general, the benefit function that maps predictions to luck is characterised with a single parameter (the false positive benefit). <span class="math display">\[\mathrm{benefit}(\hat{y}=i,y=j) = b_{ij} = \left(
\begin{array}{cc}
1   &amp; 0 \\
b_+ &amp; 1
\end{array}
\right)\]</span> Note that the benefit function equal accuracy, corresponds to the special case <span class="math inline">\(b_+=0\)</span>, where all types of errors are equally unlucky. To make the analysis easier in the case where <span class="math inline">\(\hat{Y}=1\)</span> is the advantageous outcome, we examine the behaviour for the specific case <span class="math inline">\(b_+=2\)</span>, (as suggested in the original paper and specified in Table <a href="#tbl:clfBenefitFuncs" data-reference-type="ref" data-reference="tbl:clfBenefitFuncs">5.1</a>). Here the false positive benefit is twice as lucky as an accurate prediction.</p>
<p>The value of our inequality index is computed much like an expected cost. The associated cost matrix is given by, <span class="math display">\[c_{ij} = \mathrm{cost}(\hat{y}=i, y=j) = b_{ij}/\mu.\]</span> The difference here is that the associated cost matrix is not constant, but rather depends on the distribution of benefits. Our choice of parameter <span class="math inline">\(\alpha\)</span>, corresponds to different loss functions. As our model performance changes, so does the mean benefit and thus the associated costs. The mean benefit <span class="math inline">\(\mu\)</span> is always positive and so does not affect the relative size or ordering of the associated costs in the matrix, but can still impact the relative preference of different predictions (as is the case when we define a cost sensitive utility). Crucially, cost sensitive utilities mean that making a more accurate prediction might not always reduce the expected cost.</p>
</section>
</section>
<section id="fairness-as-utility" class="level2" data-number="5.3">
<h2 data-number="5.3"><span class="header-section-number">5.3</span> Fairness as Utility</h2>
<p>In section <a href="#sec_GEImax" data-reference-type="ref" data-reference="sec_GEImax">5.1.5</a> we saw how the value of the generalised entropy index is maximal when only one individual benefits. In this section we will show that the distribution of benefits and thus the index are more tightly constrained for any <em>reasonable</em> model <span class="math inline">\(\hat{Y}\)</span>, and that those constraints become tighter still on fixing the dataset. We derive an analytical account of the behaviour of the index given by the criterion of equal luck for different values of <span class="math inline">\(\alpha\)</span>, and show how it relates to other well known model performance metrics.</p>
<p>Under the criterion of equal luck, our benefit distribution can be characterised with two parameters, the mean benefit <span class="math inline">\(\mu\)</span> and the model accuracy <span class="math inline">\(\lambda\)</span>.</p>
<div class="lookbox">
<p><strong>Index value for Equal Luck</strong></p>
<div id="eq:GEI_acc-mu">
<table>
<colgroup>
<col style="width: 90%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
I_{\alpha}\left(\mu,\lambda\right) = \left\{
\begin{array}{cl}
\left(1-\dfrac{\lambda}{\mu}\right)\ln2-\ln\mu
&amp; \textrm{if}\quad\alpha = 1 \\
\rule{0em}{4.5ex}
\dfrac{1}{\alpha(\alpha-1)}
\left[ \left(\dfrac{2}{\mu}\right)^{\alpha-1}
- \dfrac{(2^{\alpha-1}-1)}{\mu^{\alpha}}\lambda - 1 \right]
&amp; \textrm{if}\quad \alpha&gt;0.
\end{array}\right.\]</span></td>
<td style="text-align: right;">(5.12)</td>
</tr>
</tbody>
</table>
</div>
<p><a href="#II_Luck">Proof</a> in in appendix <a href="#sec_app_IISolutions" data-reference-type="ref" data-reference="sec_app_IISolutions">D.3</a>.</p>
</div>
<p>The mean benefit <span class="math inline">\(\mu\)</span> here, gives us an indication of the relative number of false positive to false negative errors, made by the model; it tells us if the model is over or underestimating the target on average. From another point of view, it quantifies the amount of skew in the distribution of luck. Skew describes the extent of asymmetry in a distribution. For negatively skewed distributions, the tail is longer (and thinner) on the left (and vice versa for positively skewed distributions). Therefore, <span class="math inline">\(\mu&lt;1\)</span> indicates more weight on the left (and thus the tail on the right) hence positive skew (and vice versa for <span class="math inline">\(\mu&gt;1\)</span>). Figure <a href="#fig:BenefitMeanSkew" data-reference-type="ref" data-reference="fig:BenefitMeanSkew">5.5</a> provides visual illustrations of benefit distributions with different mean benefits <span class="math inline">\(\mu\)</span>. When the mean benefit is one (as in the centre figure), the distribution has no skew; it is symmetric.</p>
<figure>
<img src="05_UtilityFairness/figures/Fig_BenefitMeanSkew.png" id="fig:BenefitMeanSkew" alt="Figure 5.5: Characterisation of benefit distributions with different mean benefits." />
<figcaption aria-hidden="true">Figure 5.5: Characterisation of benefit distributions with different mean benefits.</figcaption>
</figure>
<p>We can see from these equations that for fixed <span class="math inline">\(\mu\)</span>, <span class="math inline">\(I_{\alpha}(\mu,\lambda)\)</span> is a linearly decreasing function of accuracy. We know that for most problems, the accuracy of our model is bounded below by our dataset, <span class="math display">\[0.5 \leq \max[\mathbb{P}(Y=0), \mathbb{P}(Y=1)] &lt; \lambda \leq 1.\]</span> If we can find the maximal value of the index for a given accuracy, this allows us to find an upper bound for the index, based on a dataset with known <span class="math inline">\(Y\)</span>. Before analysing the behaviour of the index as a function of <span class="math inline">\(\mu\)</span>, we note that <span class="math inline">\(\mu\)</span> is also constrained for our classifier. For a model with accuracy <span class="math inline">\(\lambda=n_c/n\)</span>, the total number of benefits <span class="math inline">\(B\)</span>, must satisfy the following bounds, <span class="math display">\[n_c \leq B \leq n_c+2(n-n_c) = 2n-n_c.\]</span> We also know, that the total number of benefits must equate to <span class="math inline">\(n\)</span> times the mean, that is, <span class="math inline">\(B=n\mu\)</span>. Given this, it is straightforward to show that we must have</p>
<div id="eq:mu_bounds">
<table>
<colgroup>
<col style="width: 90%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
\lambda \leq \mu \leq 2 - \lambda.\]</span></td>
<td style="text-align: right;">(5.13)</td>
</tr>
</tbody>
</table>
</div>
<p>As the accuracy of the model <span class="math inline">\(\lambda\)</span> increases, the range of possible values the mean benefit <span class="math inline">\(\mu\)</span> can take, decreases. Our domain is then an isosceles triangle. In Figure <a href="#fig:domain" data-reference-type="ref" data-reference="fig:domain">5.6</a> we provide a visualisation of the domain space. We choose to plot the mean benefit <span class="math inline">\(\mu\)</span> on the horizontal axis, enabling us to visualise the benefit distributions in the natural orientation.</p>
<figure>
<img src="05_UtilityFairness/figures/Fig_domain_1.png" id="fig:domain" style="width:60.0%" alt="Figure 5.6: Visualisation of index domain." />
<figcaption aria-hidden="true">Figure 5.6: Visualisation of index domain.</figcaption>
</figure>
<p>In Figures <a href="#fig:Imu_varlambda" data-reference-type="ref" data-reference="fig:Imu_varlambda">5.7</a> we plot <span class="math inline">\(I_{\alpha}(\mu,\lambda)\)</span> as a function of <span class="math inline">\(\mu\)</span> for a range of values of <span class="math inline">\(\lambda\)</span>. Each plot corresponds to a different value of <span class="math inline">\(\alpha\)</span>.</p>
<figure>
<img src="05_UtilityFairness/figures/Fig_I_alpha_1-4_varlambda.png" id="fig:Imu_varlambda" alt="Figure 5.7: Generalised entropy index I_{\alpha}\left(\mu,\lambda\right) as a function of \mu for varying \lambda and fixed \alpha." />
<figcaption aria-hidden="true">Figure 5.7: Generalised entropy index <span class="math inline">\(I_{\alpha}\left(\mu,\lambda\right)\)</span> as a function of <span class="math inline">\(\mu\)</span> for varying <span class="math inline">\(\lambda\)</span> and fixed <span class="math inline">\(\alpha\)</span>.</figcaption>
</figure>
<section id="index-maximum" class="level3" data-number="5.3.1">
<h3 data-number="5.3.1"><span class="header-section-number">5.3.1</span> Index maximum</h3>
<div class="lookbox">
<p><strong>Index turning point</strong></p>
<p>The index has exactly one turning point (a maxima) for <span class="math inline">\(\alpha&gt;0\)</span>, at <span class="math inline">\(\mu=\tilde{\mu}\)</span> where, <span class="math inline">\(\tilde{\mu} = g(\alpha)\lambda\)</span> and,</p>
<div id="eq:g_alpha">
<table>
<colgroup>
<col style="width: 90%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
\quad g(\alpha) = \left\{
\begin{array}{cl}
\ln2 &amp; \textrm{if}\quad\alpha = 1 \\
\rule{0em}{3.8ex}
\dfrac{\alpha(2^{\alpha-1}-1)}{(\alpha-1)2^{\alpha-1}}
&amp; \textrm{if}\quad \alpha&gt;0
\end{array}\right.\]</span></td>
<td style="text-align: right;">(5.14)</td>
</tr>
</tbody>
</table>
</div>
<p><a href="#II_IndexTpt">Proof</a> in in appendix <a href="#sec_app_IISolutions" data-reference-type="ref" data-reference="sec_app_IISolutions">D.3</a>.</p>
</div>
<p>Let’s summarise what we know about the behaviour of the index as a function of <span class="math inline">\(\mu\)</span>. <span class="math display">\[\begin{aligned}
\tilde{\mu}\leq\lambda &amp; \quad\Rightarrow\quad I_{\alpha}(\mu,\lambda)
\textrm{ is a strictly decreasing function of }\mu.\\
\lambda&lt;\tilde{\mu}&lt;2-\lambda &amp; \quad\Rightarrow\quad I_{\alpha}(\mu,\lambda)
\textrm{ is maximal at }\mu=\tilde{\mu}=g(\alpha)\lambda.\\
\tilde{\mu}\geq2-\lambda &amp; \quad\Rightarrow\quad I_{\alpha}(\mu,\lambda)
\textrm{ is a strictly increasing function of }\mu.
\end{aligned}\]</span></p>
<p>From equations (5.13) and (5.14), we see that for <span class="math inline">\(\tilde{\mu}\)</span> to fall within the domain of <span class="math inline">\(\mu\)</span> we require,</p>
<div id="eq:g_bounds">
<table>
<colgroup>
<col style="width: 90%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
\lambda \leq g(\alpha)\lambda \leq 2 - \lambda \quad\Leftrightarrow\quad
1 &lt; g(\alpha) &lt; \frac{2}{\lambda}-1.\]</span></td>
<td style="text-align: right;">(5.15)</td>
</tr>
</tbody>
</table>
</div>
<p>For convenience, we reformulate the upper bound on <span class="math inline">\(g(\alpha)\)</span> as a bound on <span class="math inline">\(\lambda\)</span>.</p>
<div id="eq:lambda_tilde">
<table>
<colgroup>
<col style="width: 90%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
g(\alpha) &lt; \frac{2}{\lambda}-1 \quad\Leftrightarrow\quad
\lambda &lt; \tilde{\lambda}(\alpha) = \frac{2}{1+g(\alpha)},\]</span></td>
<td style="text-align: right;">(5.16)</td>
</tr>
</tbody>
</table>
</div>
<p>where <span class="math inline">\(g(\alpha)\)</span> is given in equation 5.14. To understand the behaviour of the index better, we need to understand the behaviour of <span class="math inline">\(g(\alpha)\)</span> and <span class="math inline">\(\tilde{\lambda}(\alpha)\)</span>. We plot <span class="math inline">\(g(\alpha)\)</span> and <span class="math inline">\(\tilde{\lambda}(\alpha)\)</span> in Figures <a href="#fig:g_alpha" data-reference-type="ref" data-reference="fig:g_alpha">5.8</a>.</p>
<figure>
<img src="05_UtilityFairness/figures/Fig_g_lambda_tilde_alpha.png" id="fig:g_alpha" alt="Figure 5.8: g(\alpha) and \tilde{\lambda}(\alpha), see equation (5.16)." />
<figcaption aria-hidden="true">Figure 5.8: <span class="math inline">\(g(\alpha)\)</span> and <span class="math inline">\(\tilde{\lambda}(\alpha)\)</span>, see equation (5.16).</figcaption>
</figure>
<p>Note that, <span class="math display">\[g(2)=1 \quad\textrm{and}\quad\left\{
\begin{array}{lcr}
g(\alpha)&lt;1 &amp; \textrm{if} &amp; 0&lt;\alpha&lt;2,\\
g(\alpha)&gt;1 &amp; \textrm{if} &amp;   \alpha&gt;2.
\end{array}\right.\]</span> This allows us to reformulate the lower bound on <span class="math inline">\(g(\alpha)\)</span> (given in equation (5.15)) as bound on <span class="math inline">\(\alpha\)</span>, <span class="math display">\[g(\alpha) &gt; 1 \quad\Leftrightarrow\quad \alpha &gt; 2.\]</span> Since <span class="math inline">\(2^{\alpha-1}\)</span> dominates <span class="math inline">\(\alpha\)</span> for large <span class="math inline">\(\alpha\)</span>, we know that <span class="math display">\[g(\alpha) \rightarrow 1^+ \quad\textrm{as}\quad \alpha\rightarrow\infty
\quad\Rightarrow\quad\tilde{\lambda}(\alpha) \rightarrow 1^-
\quad\textrm{as}\quad \alpha\rightarrow\infty.\]</span> Differentiating <span class="math inline">\(g(\alpha)\)</span> in equation (5.14) gives, <span class="math display">\[g&#39;(\alpha) = \frac{\alpha(\alpha-1)\ln2 - (2^{\alpha-1}-1)}
                  {[(\alpha-1)2^{\alpha-1}]^2}=0
\quad\Leftrightarrow\quad \alpha = \alpha_*\]</span> where <span class="math inline">\(\alpha_*\)</span> satisfies <span class="math display">\[\alpha_*(\alpha_*-1)\ln2 = 2^{\alpha_*-1}-1.\]</span> <span class="math inline">\(g&#39;(\alpha)\)</span> has exactly one root <span class="math inline">\(\alpha=\alpha_*\)</span> (somewhere between 4 and 5) which can be found numerically. <span class="math display">\[g&#39;&#39;(\alpha_*) = \frac{2-\alpha_*\ln2}{(\alpha_*-1)2^{\alpha_*-1}} &lt; 0.\]</span> Thus <span class="math inline">\(g(\alpha)\)</span> is maximal at <span class="math inline">\(\alpha=\alpha_*\)</span>. For reference, <span class="math display">\[\begin{aligned}
\alpha_* \approx 4.72 \quad\Rightarrow\quad
&amp; \max_{\alpha&gt;0}[g(\alpha)] = g(\alpha_*) \approx 1.17 \\
\Rightarrow\quad
&amp; \min_{\alpha&gt;0}[\tilde{\lambda}(\alpha)]
= \tilde{\lambda}(\alpha_*) \approx 92.1\%.
\end{aligned}\]</span> We can now summarise the behaviour of the index for a given model accuracy, in terms of our metric inputs (<span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\alpha\)</span>). <span class="math display">\[\begin{aligned}
\alpha\leq2 \phantom{\textrm{ and }\lambda&lt;\tilde{\lambda}}
&amp; \quad\Rightarrow\quad I_{\alpha}(\mu,\lambda)
\textrm{ is a strictly decreasing function of }\mu,
\textrm{ maximal at }\mu=\lambda.\\
\alpha&gt;2\textrm{ and }\lambda&lt;\tilde{\lambda}
&amp; \quad\Rightarrow\quad I_{\alpha}(\mu,\lambda)
\textrm{ is maximal at }\mu=g(\alpha)\lambda.\\
\alpha&gt;2\textrm{ and }\lambda\geq\tilde{\lambda}
&amp; \quad\Rightarrow\quad I_{\alpha}(\mu,\lambda)
\textrm{ is a strictly increasing function of }\mu,
\textrm{ maximal at }\mu=2-\lambda.
\end{aligned}\]</span> where <span class="math inline">\(g(\alpha)\)</span> and <span class="math inline">\(\tilde{\lambda}\)</span> are given in equations (5.14) and (5.16) respectively. These characteristically different behaviours are indeed observed in Figures <a href="#fig:Imu_varlambda" data-reference-type="ref" data-reference="fig:Imu_varlambda">5.7</a> for different values of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\lambda\)</span>. We note that in these plots, the accuracy <span class="math inline">\(\lambda\)</span> does exceed the level required, for the index to become a strictly increasing function of <span class="math inline">\(\mu\)</span>. For reference, <span class="math inline">\(\tilde{\lambda}(3)=16/17\approx94.1\%\)</span> and <span class="math inline">\(\tilde{\lambda}(4)=12/13\approx92.3\%\)</span>.</p>
<p>We are almost there. We now know that, <span class="math display">\[\max_{\mu}\left[I_{\alpha}\left(\mu,\lambda\right)\right]
= I_{\alpha}\left(\mu_*,\lambda\right) \nonumber\\
\]</span> where, <span class="math display">\[\mu_* = \left\{
\begin{array}{cl}
\lambda          &amp; \quad\textrm{if}\quad 0&lt;\alpha\leq2 \\
g(\alpha)\lambda &amp; \quad\textrm{if}\quad \alpha&gt;2
\quad\textrm{and}\quad \lambda&lt;\tilde{\lambda}(\alpha) \\
2-\lambda        &amp; \quad\textrm{if}\quad \alpha&gt;2
\quad\textrm{and}\quad \lambda\geq\tilde{\lambda}(\alpha).
\end{array}\right.\]</span> Substituting <span class="math inline">\(\mu=\mu_*\)</span> into equation (5.12) yields the index maximum (for fixed <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\lambda\)</span>),</p>
<div class="lookbox">
<p><strong>Equal Luck Generalised Entropy Index Maximum Value</strong></p>
<p>We can write the maximal value of the generalised entropy index as a function of <span class="math inline">\(\lambda\)</span>, <span class="math display">\[\max_{\mu}\left[I_{\alpha}\left(\mu,\lambda\right)\right] = \left\{
\begin{array}{cl}
-\ln\lambda &amp; \textrm{if}\quad \alpha=1 \\
\rule{0em}{4.1ex}
\dfrac{1}{\alpha(\alpha-1)}\left(\dfrac{1}{\lambda^{\alpha-1}} - 1 \right)
&amp; \textrm{if}\quad 0&lt;\alpha\leq2 \\
\rule{0em}{4.1ex}
\dfrac{1}{\alpha(\alpha-1)}\left[\dfrac{2^{\alpha-1}}
{\alpha g^{\alpha-1}(\alpha)\lambda^{\alpha-1}} - 1 \right]
&amp; \textrm{if}\quad \alpha&gt;2,\,\lambda&lt;\tilde{\lambda}(\alpha) \\
\rule{0em}{4.1ex}
\dfrac{1}{\alpha(\alpha-1)}
\left(\dfrac{1}{(2-\lambda)^{\alpha-1}} - 1 \right)
&amp; \textrm{if}\quad \alpha&gt;2,\,\lambda\geq\tilde{\lambda}(\alpha)
\end{array}\right.\]</span> where, <span class="math display">\[\tilde{\lambda}(\alpha) = \frac{2}{1+g(\alpha)}
\quad\textrm{and}\quad g(\alpha) = \left\{
\begin{array}{cl}
\ln2 &amp; \textrm{if}\quad\alpha = 1 \\
\rule{0em}{3.8ex}
\dfrac{\alpha(2^{\alpha-1}-1)}{(\alpha-1)2^{\alpha-1}}
&amp; \textrm{if}\quad \alpha&gt;0
\end{array}\right.\]</span></p>
</div>
<p>In Figures <a href="#fig:Imu_varalpha" data-reference-type="ref" data-reference="fig:Imu_varalpha">5.9</a> we plot <span class="math inline">\(I_{\alpha}(\mu,\lambda)\)</span> as a function of <span class="math inline">\(\mu\)</span> for a range of values of <span class="math inline">\(\alpha\)</span>. Each plot corresponds to a different value of <span class="math inline">\(\lambda\)</span>. In the final plot, <span class="math inline">\(\lambda=95\%\)</span>; we see that <span class="math inline">\(I_{\alpha}(\mu,\lambda)\)</span> is a strictly increasing function of <span class="math inline">\(\mu\)</span>, for both <span class="math inline">\(\alpha=3\)</span> and <span class="math inline">\(\alpha=4\)</span>.</p>
<figure>
<img src="05_UtilityFairness/figures/Fig_I_lambda_50-95_varalpha.png" id="fig:Imu_varalpha" alt="Figure 5.9: Generalised entropy index I_{\alpha}\left(\mu,\lambda\right) as a function of \mu for varying \alpha and fixed \lambda." />
<figcaption aria-hidden="true">Figure 5.9: Generalised entropy index <span class="math inline">\(I_{\alpha}\left(\mu,\lambda\right)\)</span> as a function of <span class="math inline">\(\mu\)</span> for varying <span class="math inline">\(\alpha\)</span> and fixed <span class="math inline">\(\lambda\)</span>.</figcaption>
</figure>
<p>From Figure <a href="#fig:Imu_varalpha" data-reference-type="ref" data-reference="fig:Imu_varalpha">5.9</a>, we can see that both <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\alpha\)</span>, have an impact on the relative preference between false positive and false negative errors. As the accuracy of our model increases, the change in behaviour of the index, for different choices of <span class="math inline">\(\alpha\)</span>, becomes more extreme.</p>
</section>
<section id="when-is-making-an-error-preferable" class="level3" data-number="5.3.2">
<h3 data-number="5.3.2"><span class="header-section-number">5.3.2</span> When is Making an Error Preferable?</h3>
<p>The final task in our analysis if the inequality index implied by <em>equal luck</em>, is to calculate the <em>cost of an error</em>. In particular, we want to know when increasing the accuracy of a model does not correspond to reducing the value of the index. For a binary classifier, the space of possible benefit distributions is constrained. We cannot arbitrarily transfer benefits from rich to poor. The range of possible benefits an individual <span class="math inline">\(i\)</span> can receive is limited by their value <span class="math inline">\(y_i\)</span>. If <span class="math inline">\(y_i=0\)</span> then <span class="math inline">\(b\in\{1,2\}\)</span>, but if <span class="math inline">\(y_i=1\)</span> then <span class="math inline">\(b\in\{0,1\}\)</span>.</p>
<section id="the-cost-of-an-error" class="level4 unnumbered">
<h4 class="unnumbered">The Cost of an Error</h4>
<p>Let us denote the cost of an error as, <span class="math display">\[\Delta I^{\pm}_{\alpha}(\boldsymbol{b})
= I_{\alpha}(\boldsymbol{b^{\pm}}) - I_{\alpha}(\boldsymbol{b}).\]</span> Here <span class="math inline">\(\boldsymbol{b^{\pm}}\)</span> differs from <span class="math inline">\(\boldsymbol{b}\)</span> by one prediction only, containing one less correct prediction, and one more erroneous one. For <span class="math inline">\(\boldsymbol{b^{+}}\)</span>, the additional error is a false positive. For <span class="math inline">\(\boldsymbol{b^{-}}\)</span>, the additional error is a false negative. An additional false negative error, reduces the total benefits by one; both the accuracy <span class="math inline">\(\lambda\)</span> and the mean benefit <span class="math inline">\(\mu\)</span> are reduced by <span class="math inline">\(1/n\)</span>. An additional false positive error, increases the total benefits by one; the accuracy <span class="math inline">\(\lambda\)</span> is, once again, reduced by <span class="math inline">\(1/n\)</span>, and the mean benefit <span class="math inline">\(\mu\)</span> increases by <span class="math inline">\(1/n\)</span>. Therefore, we can write,</p>
<div id="eq:DeltaIpm">
<table>
<colgroup>
<col style="width: 90%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
\Delta I^{\pm}_{\alpha}(\mu,\lambda;n)
= I_{\alpha}\left(\lambda-\frac{1}{n}, \mu\pm\frac{1}{n}\right)
- I_{\alpha}(\mu,\lambda).\]</span></td>
<td style="text-align: right;">(5.17)</td>
</tr>
</tbody>
</table>
</div>
<p>The discrete grid of adjacent models we can reach through a small change in the model (given <span class="math inline">\(\mu\)</span>, <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(n\)</span>), is shown in Figure <a href="#fig:domain_2" data-reference-type="ref" data-reference="fig:domain_2">5.10</a>.</p>
<figure>
<img src="05_UtilityFairness/figures/Fig_domain_2.png" id="fig:domain_2" style="width:40.0%" alt="Figure 5.10: Visualisation of the local domain space for given \mu, \lambda and n (assuming the point (\mu,\lambda) is not on an edge)." />
<figcaption aria-hidden="true">Figure 5.10: Visualisation of the local domain space for given <span class="math inline">\(\mu\)</span>, <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(n\)</span> (assuming the point <span class="math inline">\((\mu,\lambda)\)</span> is not on an edge).</figcaption>
</figure>
<p>Equation 5.12 provides an expression for <span class="math inline">\(I_{\alpha}(\mu,\lambda)\)</span>. Substituting for <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\mu\)</span> in the case <span class="math inline">\(\alpha=1\)</span> gives, <span class="math display">\[I_{\alpha}\left(\lambda-\frac{1}{n},\mu\pm\frac{1}{n}\right) =
\left[1-\left(\frac{\lambda}{\mu}-\frac{1}{n\mu}\right)
\left(1\pm\frac{1}{n\mu}\right)^{-1}\right]\ln2-\ln\mu
- \ln\left(1\pm\frac{1}{n\mu}\right).\]</span> For <span class="math inline">\(\alpha&gt;0\)</span>, we get, <span class="math display">\[I_{\alpha}\left(\lambda-\frac{1}{n},\mu\pm\frac{1}{n}\right) =
\frac{1}{\alpha(\alpha-1)} \left[ \left(\frac{2}{\mu}\right)^{\alpha-1}
\left(1\pm\frac{1}{n\mu}\right)^{1-\alpha}
- \frac{(2^{\alpha-1}-1)}{\mu^{\alpha-1}}
\left(\frac{\lambda}{\mu}-\frac{1}{n\mu}\right)
\left(1\pm\frac{1}{n\mu}\right)^{-\alpha} - 1 \right].\]</span> We showed earlier that we must have, <span class="math inline">\(\lambda\leq\mu\leq2-\lambda\)</span>, in addition, for most problems, any reasonable model should have <span class="math inline">\(0.5\leq\lambda\leq1\)</span>. We deduce that we must have <span class="math inline">\(0.5\leq\mu\leq1.5\)</span> and so <span class="math inline">\(\mu=\mathrm{O}(1)\)</span>. Then for large <span class="math inline">\(n\)</span>, we can be sure that <span class="math inline">\(n\mu\)</span> is large and its reciprocal is small. For large <span class="math inline">\(n\)</span>, we can write the cost of an error as <span class="math display">\[\Delta I^{\pm}_{\alpha}(\mu,\lambda;n) = \xi_{\alpha}(\mu,\lambda)
\left(\frac{1}{n\mu}\right) + \mathrm{O}\left(\frac{1}{n\mu}\right)^2\]</span> where, <span class="math display">\[\xi_{\alpha}(\mu,\lambda) = \left\{
\begin{array}{cl}
\left(1\pm\dfrac{\lambda}{\mu}\right)\ln2\mp1
&amp; \textrm{if}\quad\alpha = 1 \\
\rule{0em}{4.1ex}
\dfrac{1}{\alpha(\alpha-1)\mu^{\alpha}}\bigg[
\Big((1\pm1\mp\alpha)2^{\alpha-1}-1\Big)\mu \pm
\alpha(2^{\alpha-1}-1)\lambda\bigg]
&amp; \textrm{if}\quad \alpha&gt;0.
\end{array}\right.\]</span></p>
<p>From these expressions, we can get a clearer understanding of when the index deviates from simply being a measure of error. In particular, we want to know when an error, is preferable to an accurate prediction; that is, when the index change (resulting from an error) is negative. With a little effort we can show that,</p>
<div id="eq:DeltaI_neg">
<table>
<colgroup>
<col style="width: 90%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
\left.
\begin{array}{cl}
&amp; \Delta I^-_{\alpha}(\mu,\lambda;n) &lt; 0 \quad\Rightarrow\quad
\mu &lt; h^-(\alpha) \lambda \\
&amp; \Delta I^+_{\alpha}(\mu,\lambda;n) &lt; 0 \quad\Rightarrow\quad
\mu &gt; h^+(\alpha) \lambda
\end{array}\qquad\right\}\]</span></td>
<td style="text-align: right;">(5.18)</td>
</tr>
</tbody>
</table>
</div>
<p>where,</p>
<div id="eq:h_alpha">
<table>
<colgroup>
<col style="width: 90%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
h^{\pm}(\alpha) =  \left\{
\begin{array}{cl}
\dfrac{\ln2}{1\mp\ln2} &amp; \textrm{if}\quad\alpha = 1 \\
\rule{0em}{4.1ex}
\dfrac{\alpha(2^{\alpha-1}-1)}{(\alpha-1\mp1)2^{\alpha-1}\pm1}
&amp; \textrm{if}\quad \alpha&gt;0
\end{array}\right.\]</span></td>
<td style="text-align: right;">(5.19)</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="false-negative-errors" class="level4 unnumbered">
<h4 class="unnumbered">False Negative Errors</h4>
<p>Let’s start by looking at <span class="math inline">\(h^-(\alpha)\)</span>, which we re-write as,</p>
<div id="eq:h_minus">
<table>
<colgroup>
<col style="width: 90%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
h^-(\alpha) =  \left\{
\begin{array}{cl}
\dfrac{\ln2}{1+\ln2} \approx 0.41 &amp; \textrm{if}\quad\alpha = 1 \\
\rule{0em}{4.1ex}
1-\dfrac{\alpha-1}{\alpha2^{\alpha-1}-1}
&amp; \textrm{if}\quad \alpha&gt;0
\end{array}\right.\]</span></td>
<td style="text-align: right;">(5.20)</td>
</tr>
</tbody>
</table>
</div>
<p>Equation (5.20) reveals that <span class="math inline">\(h^-(\alpha)\)</span> is a strictly increasing function of <span class="math inline">\(\alpha\)</span>, for <span class="math inline">\(\alpha&gt;0\)</span> (since <span class="math inline">\(\alpha2^{\alpha-1}\)</span> dominates <span class="math inline">\(\alpha\)</span>). In addition, we can see that <span class="math inline">\(h^-(\alpha)\rightarrow1^-\)</span> as <span class="math inline">\(\alpha\rightarrow\infty\)</span>. In Figure <a href="#fig:DeltaIminus" data-reference-type="ref" data-reference="fig:DeltaIminus">5.11</a> we plot <span class="math inline">\(h^-(\alpha)\)</span>.</p>
<figure>
<img src="05_UtilityFairness/figures/Fig_h_minus_alpha.png" id="fig:DeltaIminus" style="width:65.0%" alt="Figure 5.11: h^-(\alpha) = 1 - (\alpha-1)/(\alpha2^{\alpha-1}-1)." />
<figcaption aria-hidden="true">Figure 5.11: <span class="math inline">\(h^-(\alpha) = 1 - (\alpha-1)/(\alpha2^{\alpha-1}-1)\)</span>.</figcaption>
</figure>
<p>Earlier we showed that we must have <span class="math inline">\(\mu\geq\lambda\)</span>. Then from equation (5.18), for <span class="math inline">\(\Delta I^-_{\alpha}(\mu,\lambda;n)&lt;0\)</span> we need <span class="math inline">\(h^-(\alpha)&gt;1\)</span>. Since <span class="math inline">\(h^-(\alpha)&lt;1\)</span> for all <span class="math inline">\(\alpha&gt;0\)</span>, we know that making an additional false negative error, never decreases the value of the index. What about false positive errors?</p>
</section>
<section id="false-positive-errors" class="level4 unnumbered">
<h4 class="unnumbered">False Positive Errors</h4>
<p>We rewrite <span class="math inline">\(h^+(\alpha)\)</span> as,</p>
<div id="eq:h_plus">
<table>
<colgroup>
<col style="width: 90%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[
h^+(\alpha) =  \left\{
\begin{array}{cl}
\dfrac{\ln2}{1-\ln2} \approx 2.26 &amp; \textrm{if}\quad\alpha = 1 \\
\rule{0em}{4.2ex}
\dfrac{\alpha(1-2^{1-\alpha})}{(\alpha-2)+2^{1-\alpha}}
&amp; \textrm{if}\quad \alpha&gt;0.
\end{array}\right.\]</span></td>
<td style="text-align: right;">(5.21)</td>
</tr>
</tbody>
</table>
</div>
<p>Equation (5.21) reveals that <span class="math inline">\(h^+(\alpha)\)</span> is a decreasing function of <span class="math inline">\(\alpha\)</span>, since <span class="math inline">\(2^{1-\alpha}\)</span> is a strictly decreasing function of <span class="math inline">\(\alpha\)</span>. In addition, we can see that <span class="math inline">\(h^+(\alpha)\rightarrow1^+\)</span> as <span class="math inline">\(\alpha\rightarrow\infty.\)</span> Earlier we showed that we must have <span class="math inline">\(\mu\leq2-\lambda\)</span>. Then from equation (5.18), for <span class="math inline">\(\Delta I^+_{\alpha}(\mu,\lambda;n)&lt;0\)</span> we need, <span class="math display">\[h^+(\alpha)\lambda &lt; 2-\lambda \quad\Leftrightarrow\quad
\lambda &lt; \bar{\lambda}(\alpha) = \frac{2}{1+h^+(\alpha)}.\]</span> From what ew know about <span class="math inline">\(h^+(\alpha)\)</span>, we can deduce that <span class="math inline">\(\bar{\lambda}(\alpha)\)</span> is an increasing function of <span class="math inline">\(\alpha\)</span>, and <span class="math inline">\(\bar{\lambda}(\alpha)\rightarrow1^-\)</span> as <span class="math inline">\(\alpha\rightarrow\infty\)</span>. Since <span class="math inline">\(\bar{\lambda}(\alpha)&lt;1\)</span> for all <span class="math inline">\(\alpha&gt;0\)</span>, we know there are indeed some circumstances, under which a false positive error, decreases the value of the index. In Figures <a href="#fig:DeltaIplus" data-reference-type="ref" data-reference="fig:DeltaIplus">5.12</a>, we plot <span class="math inline">\(h^+(\alpha)\)</span> and <span class="math inline">\(\bar{\lambda}(\alpha)\)</span>.</p>
<figure>
<img src="05_UtilityFairness/figures/Fig_h_plus_lambda_bar_alpha.png" id="fig:DeltaIplus" alt="Figure 5.12: h^+(\alpha)=[\alpha(2^{\alpha-1}-1)]/[(\alpha-2)2^{\alpha-1}+1]\quad and \quad\bar{\lambda}(\alpha)=2/[1+h^+(\alpha)]." />
<figcaption aria-hidden="true">Figure 5.12: <span class="math inline">\(h^+(\alpha)=[\alpha(2^{\alpha-1}-1)]/[(\alpha-2)2^{\alpha-1}+1]\quad\)</span> and <span class="math inline">\(\quad\bar{\lambda}(\alpha)=2/[1+h^+(\alpha)]\)</span>.</figcaption>
</figure>
</section>
<section id="the-deviation-region" class="level4 unnumbered">
<h4 class="unnumbered">The Deviation Region</h4>
<p>We call the <em>deviation region</em> the part of the domain for which the index is not reduced by reducing the error rate, but instead reduced by increasing the error rate. For our benefit function <em>equal luck</em>, <span class="math display">\[I_{\alpha}(\mu,\lambda): ([\lambda, 2-\lambda], [0.5,1]) \mapsto \mathbb{R}_{\geq0}.\]</span> The only kind of error which is ever preferable to a correct prediction under this benefit function is a false positive error. This happens only when the mean benefit exceeds <span class="math inline">\(h^+(\alpha)\lambda\)</span>, that is when the ratio of lucky to unlucky people is sufficiently high. We note that for a model whose accuracy is greater than <span class="math inline">\(\bar{\lambda}(\alpha)\)</span>, it is not possible for the mean benefit (skew) to exceed the required level. That is, <span class="math display">\[\begin{aligned}
&amp; \Delta I^-_{\alpha}(\mu,\lambda;n) &gt; 0 \quad
\forall\,\mu, \,\lambda, \, n \\
&amp; \Delta I^+_{\alpha}(\mu,\lambda;n) &lt; 0 \quad\Rightarrow\quad
\mu &gt; h^+(\alpha) \lambda
\end{aligned}\]</span> where, <span class="math display">\[h^+(\alpha) =  \left\{
\begin{array}{cl}
\dfrac{\ln2}{1-\ln2} \approx 2.26 &amp; \textrm{if}\quad\alpha = 1 \\
\rule{0em}{4.2ex}
\dfrac{\alpha(2^{\alpha-1}-1)}{(\alpha-2)2^{\alpha-1}+1}
&amp; \textrm{if}\quad \alpha&gt;0
\end{array}\right.\]</span> This is only possible if the accuracy is sufficiently low, <span class="math display">\[\lambda &lt; \bar{\lambda}(\alpha)=\frac{2}{1+h^+(\alpha)}.\]</span> The deviation region is then described as, <span class="math inline">\(\mu&gt;h^+(\alpha)\lambda\)</span>, where <span class="math inline">\(h^+(\alpha)\)</span> is given in equation (5.21). We mark the deviation region on the contour plot for <span class="math inline">\(I_{\alpha}(\mu, \lambda)\)</span> in Figure <a href="#fig:cplot_varalpha" data-reference-type="ref" data-reference="fig:cplot_varalpha">5.13</a>.</p>
<figure>
<img src="05_UtilityFairness/figures/Fig_ccplot_varalpha.png" id="fig:cplot_varalpha" alt="Figure 5.13: Contour plots showing I_{\alpha}(\mu, \lambda) for different values of \alpha." />
<figcaption aria-hidden="true">Figure 5.13: Contour plots showing <span class="math inline">\(I_{\alpha}(\mu, \lambda)\)</span> for different values of <span class="math inline">\(\alpha\)</span>.</figcaption>
</figure>
<p>For reference, in Table <a href="#tbl:BeneficialErrorThresholds" data-reference-type="ref" data-reference="tbl:BeneficialErrorThresholds">5.2</a>, we provide some values of <span class="math inline">\(\bar{\lambda}(\alpha)\)</span> and <span class="math inline">\(h^+(\alpha)\)</span>.</p>
<div id="tbl:BeneficialErrorThresholds">
<table>
<caption>Table 5.2: Reference thresholds that tell us when increasing the error rate reduces the value of the index.</caption>
<thead>
<tr class="header">
<th style="text-align: left;"><span class="math inline">\(\alpha\)</span></th>
<th style="text-align: left;"><span class="math inline">\(\bar{\lambda}(\alpha)\)</span><sup>a</sup></th>
<th style="text-align: left;"><span class="math inline">\(h^+(\alpha)\)</span><sup>b</sup></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">1</td>
<td style="text-align: left;">61.4%</td>
<td style="text-align: left;">2.26</td>
</tr>
<tr class="even">
<td style="text-align: left;">2</td>
<td style="text-align: left;">66.7%</td>
<td style="text-align: left;">2</td>
</tr>
<tr class="odd">
<td style="text-align: left;">3</td>
<td style="text-align: left;">71.4%</td>
<td style="text-align: left;">1.8</td>
</tr>
<tr class="even">
<td style="text-align: left;">4</td>
<td style="text-align: left;">75.6%</td>
<td style="text-align: left;">1.65</td>
</tr>
</tbody>
</table>
</div>
<div class="tablenotes">
<p><sup>a</sup>We require <span class="math inline">\(\lambda&lt;\bar{\lambda}(\alpha)\)</span> for the possibility that reducing the index value may not correspond to reducing the error rate. At <span class="math inline">\(\lambda=\bar{\lambda}(\alpha)\)</span>, all the errors must be false positives to achieve the value of <span class="math inline">\(\mu\)</span> required for <span class="math inline">\(\Delta I^+(\mu,\lambda;\alpha,n)&lt;0\)</span>.</p>
<p><sup>b</sup>We require <span class="math inline">\(\mu&gt;h^+(\alpha)\lambda\)</span> for a false positive error to result in a reduction of the index value.</p>
</div>
</section>
</section>
</section>
<section id="summary-4" class="level2 unnumbered">
<h2 class="unnumbered">Summary</h2>
<ul>
<li><p>Inequality indices measure divergence from the uniform distribution. We can think of them as a system for ranking distributions from most fair to least fair, the most fair having an index value of zero and becoming more unfair as the value of the index increases.</p></li>
<li><p>Note that two distributions that diverge equally from the uniform distribution, need not be the same distribution. Different inequality indices break ties in different ways.</p></li>
</ul>
<section id="generalised-entropy-indices-1" class="level3 unnumbered">
<h3 class="unnumbered">Generalised Entropy Indices</h3>
<ul>
<li><p><em>Generalised entropy indices</em>, are a special family of inequality indices that are <em>subgroup decomposable</em>. That is, they can be disaggregated across subgroups of a population, into the sum of a <em>between-group</em> component and a <em>within-group</em> component.</p></li>
<li><p>The between-group component is computed as the value of the index, assuming all individuals receive the mean benefit of the partition to which they belong. Essentially, it measures the contribution to the inequality index, from variation in the average benefit between the subgroups (akin to the notion of group fairness we discussed in chapter <a href="#ch_GroupFairness" data-reference-type="ref" data-reference="ch_GroupFairness">3</a>, except here, the relative sizes of the subgroups matter). If all the groups have the same mean benefit the between-group component is zero.</p></li>
<li><p>The within-group component is computed as a weighted sum of the index value for each subgroup, and can be thought of as measuring the contribution to overall (individual) unfairness, arising from variation in benefits between individuals in the subgroups. For a within-group component to be zero, we require every individual in the subgroup to have exactly the same benefit.</p></li>
<li><p>The ability to additively decompose this inequality measure into intergroup and intragroup components, allows us to identify when trade-offs between the different notions of fairness (between-group and within-group) might occur.</p></li>
<li><p>We posit <em>generalised entropy indices</em> as a special family of <em>subgroup decomposable loss functions</em> with generalisation parameter <span class="math inline">\(\alpha\)</span>.</p></li>
<li><p><span class="math inline">\(\alpha\)</span> controls the weight applied to different parts of the distribution when calculating the loss.</p></li>
<li><p>For <span class="math inline">\(\alpha=0\)</span>, the index <span class="math inline">\(I_0\)</span> is a linear function of the cross entropy loss.</p></li>
<li><p>In the special case <span class="math inline">\(\alpha=1/2\)</span>, the contribution to the total loss from the between-group component is maximal.</p></li>
</ul>
</section>
<section id="defining-a-benefit-function-1" class="level3 unnumbered">
<h3 class="unnumbered">Defining a Benefit Function</h3>
<ul>
<li><p>Benefit functions map predictions to benefits <span class="math inline">\(\mathrm{benefit}(\hat{y}=i,y=j)=b_{ij}\)</span>.</p></li>
<li><p>In the special case where a accurate predictions are equally beneficial and a false negative yields no benefit, the benefit function might be thought of as a measure of <em>luck</em> (where a false negative error is as unlucky as it gets). In this case <em>luck</em> is characterised with a single parameter (the false positive benefit) <span class="math inline">\(b_+\geq0\)</span> which tells us how lucky it is relative to an accurate prediction. <span class="math display">\[\mathrm{benefit}(\hat{y}=i,y=j) = b_{ij} = \left(
\begin{array}{cc}
1   &amp; 0 \\
b_+ &amp; 1
\end{array}
\right)\]</span></p></li>
<li><p>The index is computed much like an expected loss on the cost where the cost matrix is given by, <span class="math display">\[c_{ij} = \mathrm{cost}(\hat{y}=i, y=j) = b_{ij}/\mu.\]</span></p></li>
<li><p>The cost matrix is not constant, but rather depends on the mean benefit.</p></li>
</ul>
</section>
<section id="fairness-as-utility-1" class="level3 unnumbered">
<h3 class="unnumbered">Fairness as Utility</h3>
<p>For the choice <span class="math inline">\(b_+=0\)</span>,</p>
<ul>
<li><p>Only an accurate prediction is lucky; errors correspond to zero luck.</p></li>
<li><p>The index is a monotonic decreasing function of the mean benefit (or equivalently model accuracy) <span class="math inline">\(\mu\)</span>, essentially a cost insensitive measure of utility.</p></li>
</ul>
<p>For the choice <span class="math inline">\(b_+=2\)</span>,</p>
<ul>
<li><p>a false positive is twice as lucky as an accurate prediction. The benefit can be computed as one plus the error, <span class="math inline">\(b_i=\hat{y}_i-y_i\)</span>.</p></li>
<li><p>The index can be written as a function of the mean benefit <span class="math inline">\(\mu\)</span> and model accuracy <span class="math inline">\(\lambda\)</span>. The mean benefit gives an indication of the skew in the distribution of errors. For fixed mean benefit <span class="math inline">\(\mu\)</span>, the index is a linearly decreasing function of the accuracy <span class="math inline">\(\lambda\)</span>.</p></li>
<li><p>For any <em>reasonable</em> model and <span class="math inline">\(\alpha&gt;0\)</span>, the value of the index is bounded.</p></li>
<li><p>We find that false negative predictions are never more fair than an accurate prediction.</p></li>
<li><p>False positive predictions are fairer than accurate predictions when the distribution is sufficiently skewed.</p></li>
<li><p>Once the accuracy of a model is sufficiently high, it becomes impossible for the distribution of errors to be sufficiently skewed and the index always decreases with increasing accuracy.</p></li>
<li><p>The threshold on skew <span class="math inline">\(\mu\)</span> for which a false positive is deemed fairer than an accurate prediction, is a decreasing function of <span class="math inline">\(\alpha\)</span>.</p></li>
</ul>
</section>
</section>
</section>
<section id="app_Notation" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">A</span> Notation and Conventions</h1>
<table>
<caption>Typographical conventions.</caption>
<thead>
<tr class="header">
<th style="text-align: center;">Letter case &amp;/ Typeface</th>
<th style="text-align: left;">Denotes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Lowercase</td>
<td style="text-align: left;">Scalar variables, e.g. <span class="math inline">\(a\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">Uppercase</td>
<td style="text-align: left;">Random variables, e.g. <span class="math inline">\(X\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Lowercase bold</td>
<td style="text-align: left;">Vectors, e.g. <span class="math inline">\(\boldsymbol{y}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">Uppercase bold</td>
<td style="text-align: left;">Matrices and vectors of random variables e.g. <span class="math inline">\(\boldsymbol{X}\)</span></td>
</tr>
</tbody>
</table>
<table>
<caption>Common expressions.</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Type</th>
<th style="text-align: center;">Expression</th>
<th style="text-align: left;">Denotes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td rowspan="6" style="text-align: left;">Symbols</td>
<td style="text-align: center;"><span class="math inline">\(\forall\)</span></td>
<td style="text-align: left;">For all</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(|\)</span></td>
<td style="text-align: left;">Such that</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(\in\)</span></td>
<td style="text-align: left;">Is a member of</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(\Rightarrow\)</span></td>
<td style="text-align: left;">Implies</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(\Leftrightarrow\)</span></td>
<td style="text-align: left;">If and only if</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(\rightarrow\)</span></td>
<td style="text-align: left;">Tends to</td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: center;"><span class="math inline">\(x\rightarrow a^{\pm}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(x\)</span> tends to <span class="math inline">\(a\)</span> from above (+) or below (-)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Brackets</td>
<td style="text-align: center;"><span class="math inline">\(x \in [a,b)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(a\leq x&lt;b\)</span> (inclusive and exclusive parenthesis)</td>
</tr>
<tr class="odd">
<td rowspan="2" style="text-align: left;">Sets</td>
<td style="text-align: center;"><span class="math inline">\(\cup\)</span></td>
<td style="text-align: left;">Union, logical OR</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(\cap\)</span></td>
<td style="text-align: left;">Intersection, logical AND</td>
</tr>
</tbody>
</table>
<table>
<caption>Special functions.</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Function</th>
<th style="text-align: left;">Definition</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Heaviside step function</td>
<td style="text-align: left;"><span class="math inline">\(\displaystyle \phantom{\delta(x)&#39;=} H(x) = \left\{
\begin{array}{rl}
1 &amp; \textrm{if} \quad x &gt; 0 \\
0 &amp; \textrm{otherwise}
\end{array}
\right.\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Delta function</td>
<td style="text-align: left;"><span class="math inline">\(\displaystyle \delta(x) = H&#39;(x) = \left\{
\begin{array}{cl}
\infty &amp; \textrm{if} \quad x=0 \\
0      &amp; \textrm{otherwise}
\end{array}
\right.\)</span></td>
</tr>
</tbody>
</table>
<table>
<caption>Data, model and metrics: notation and conventions.</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Type</th>
<th style="text-align: center;">Expression</th>
<th style="text-align: left;">Denotes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td rowspan="2" style="text-align: left;">Data size</td>
<td style="text-align: center;"><span class="math inline">\(n\)</span></td>
<td style="text-align: left;">Number of data points / individuals</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(m\)</span></td>
<td style="text-align: left;">Number of features (predictive model input size)</td>
</tr>
<tr class="odd">
<td rowspan="4" style="text-align: left;">Random variables</td>
<td style="text-align: center;"><span class="math inline">\(\boldsymbol{X}\)</span> <span class="math inline">\(\in\mathcal{X}\)</span></td>
<td style="text-align: left;">Features: <span class="math inline">\(\boldsymbol{X}\)</span> <span class="math inline">\(=(X_1,...,X_m)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(\boldsymbol{Z}\)</span> <span class="math inline">\(\in\mathcal{Z}\)</span></td>
<td style="text-align: left;">Sensitive features: gender, race, etc.</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(Y\in\mathcal{Y}\)</span></td>
<td style="text-align: left;">Target</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(\hat{Y}=f(\boldsymbol{X})\)</span></td>
<td style="text-align: left;">Model predictions <span class="math inline">\(\hat{y}\)</span> are a function <span class="math inline">\(f\)</span> of the features <span class="math inline">\(\boldsymbol{x}\)</span></td>
</tr>
<tr class="odd">
<td rowspan="5" style="text-align: left;">Data</td>
<td style="text-align: center;"><span class="math inline">\(\boldsymbol{X}\)</span>, <span class="math inline">\(\boldsymbol{Z}\)</span>, <span class="math inline">\(\boldsymbol{y}\)</span></td>
<td style="text-align: left;">Data for all <span class="math inline">\(n\)</span> individuals</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(\boldsymbol{x}_i\)</span>, <span class="math inline">\(\boldsymbol{z}_i\)</span>, <span class="math inline">\(y_i\)</span></td>
<td style="text-align: left;">Data for individual <span class="math inline">\(i\)</span>.</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(x_{ij}\)</span>, <span class="math inline">\(z_{ij}\)</span></td>
<td style="text-align: left;">The element of matrix <span class="math inline">\(\boldsymbol{X}\)</span> at row and column indices <span class="math inline">\(i, j\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(p(\boldsymbol{x})\)</span></td>
<td style="text-align: left;">Regression</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(H(p(\boldsymbol{x})-\tau)\)</span></td>
<td style="text-align: left;">Deterministic binary classification</td>
</tr>
<tr class="even">
<td rowspan="2" style="text-align: left;">Special values</td>
<td style="text-align: center;"><span class="math inline">\(Y = y_{\pm}\)</span></td>
<td style="text-align: left;">Advantageous (+) or disadvantageous (-) outcome</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(Z = z_{\pm}\)</span></td>
<td style="text-align: left;">Privileged / advantaged (+) or disadvantaged (-) class</td>
</tr>
<tr class="even">
<td rowspan="2" style="text-align: left;">Metrics</td>
<td style="text-align: center;"><span class="math inline">\(d\)</span></td>
<td style="text-align: left;">Difference</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(r\)</span></td>
<td style="text-align: left;">Rate / ratio</td>
</tr>
<tr class="even">
<td rowspan="4" style="text-align: left;">Probability</td>
<td style="text-align: center;"><span class="math inline">\(\mathbb{P}(A)\)</span></td>
<td style="text-align: left;">Probability of event <span class="math inline">\(A\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(f_X(x)\)</span></td>
<td style="text-align: left;">Probability density function for the random variable <span class="math inline">\(X\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">Discrete <span class="math inline">\(X\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\displaystyle \mathbb{P}(x)=\mathbb{P}(X=x)=f_{X}(x)\)</span><sup>a</sup></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Continuous <span class="math inline">\(X\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\displaystyle \mathbb{P}(a\leq X&lt;b)=\int_a^b f_X(x) \, \mathrm{d}x\)</span></td>
</tr>
<tr class="even">
<td rowspan="3" style="text-align: left;">Expectation</td>
<td style="text-align: center;"><span class="math inline">\(\mathbb{E}[X)\)</span></td>
<td style="text-align: left;">Expected value of random variable <span class="math inline">\(X\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(\mathbb{E}[g(X)]\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\displaystyle \sum_{x\in\mathcal{X}} g(x)f_X(x)
= \int_{x\in\mathcal{X}} g(x)f_X(x) \, \mathrm{d}x\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(\mathbb{E}_X[g(X,Y)]\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\displaystyle \sum_{x\in\mathcal{X}} g(x,y)f_X(x)
= \int_{x\in\mathcal{X}} g(x,y)f_X(x) \, \mathrm{d}x\)</span></td>
</tr>
</tbody>
</table>
<div class="tablenotes">
<p><sup>a</sup>For readability, when it is clear from the context, we shall omit the random variable in the event descriptor, for example, <span class="math inline">\(\mathbb{P}(X=x)=\mathbb{P}(x)\)</span>.</p>
</div>
</section>
<section id="app_Metrics" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">B</span> Performance Metrics</h1>
<section id="confusion-matrix-metrics" class="level3 unnumbered">
<h3 class="unnumbered">Confusion Matrix Metrics</h3>
<section id="performance-metrics" class="level4 unnumbered">
<h4 class="unnumbered">Performance Metrics</h4>
<div id="tbl:app_CMPerfMetrics">
<table>
<caption>Table B.1: Summary of performance metrics for a binary classifier</caption>
<thead>
<tr class="header">
<th colspan="2" style="text-align: center;"></th>
<th colspan="2" style="text-align: center;">Ground Truth</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td colspan="2" style="text-align: center;"></td>
<td style="text-align: center;"><span class="math inline">\(y=1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(y=0\)</span></td>
<td style="text-align: center;">Metric</td>
</tr>
<tr class="even">
<td rowspan="4" style="text-align: center;">Prediction</td>
<td rowspan="2" style="text-align: center;"><span class="math inline">\(\hat{y}=1\)</span></td>
<td rowspan="2" style="text-align: center;">True Positive</td>
<td rowspan="2" style="text-align: center;">False Positive<br>Type I Error</td>
<td rowspan="2" style="text-align: center;">Positive Predictive Value<sup>a</sup><br><span class="math inline">\(\mathbb{P}(\hat{y}=y|\hat{y}=1)\)</span></td>
</tr>
<tr class="odd">
</tr>
<tr class="even">
<td rowspan="2" style="text-align: center;"><span class="math inline">\(\hat{y}=0\)</span></td>
<td rowspan="2" style="text-align: center;">False Negative<br>Type II Error</td>
<td rowspan="2" style="text-align: center;">True Negative</td>
<td rowspan="2" style="text-align: center;">Negative Predictive Value<br><span class="math inline">\(\mathbb{P}(\hat{y}=y|\hat{y}=0)\)</span></td>
</tr>
<tr class="odd">
</tr>
<tr class="even">
<td colspan="2" rowspan="2" style="text-align: center;">Metric</td>
<td rowspan="2" style="text-align: center;">True Positive Rate<sup>b</sup><br><span class="math inline">\(\mathbb{P}(\hat{y}=y|y=1)\)</span></td>
<td rowspan="2" style="text-align: center;">True Negative Rate<br><span class="math inline">\(\mathbb{P}(\hat{y}=y|y=0)\)</span></td>
<td rowspan="2" style="text-align: center;">Accuracy<br><span class="math inline">\(\mathbb{P}(\hat{y}=y)\)</span></td>
</tr>
<tr class="odd">
</tr>
</tbody>
</table>
</div>
<div class="tablenotes">
<p><sup>a</sup> Positive Predictive Value = Precision</p>
<p><sup>b</sup> True Positive Rate = Recall</p>
</div>
</section>
<section id="error-metrics" class="level4 unnumbered">
<h4 class="unnumbered">Error Metrics</h4>
<div id="tbl:app_CMErrMetrics">
<table>
<caption>Table B.2: Summary of error rate types for a binary classifier</caption>
<thead>
<tr class="header">
<th colspan="2" style="text-align: center;"></th>
<th colspan="2" style="text-align: center;">Ground Truth</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td colspan="2" style="text-align: center;"></td>
<td style="text-align: center;"><span class="math inline">\(y=1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(y=0\)</span></td>
<td style="text-align: center;">Error Rate Type</td>
</tr>
<tr class="even">
<td rowspan="4" style="text-align: center;">Prediction</td>
<td rowspan="2" style="text-align: center;"><span class="math inline">\(\hat{y}=1\)</span></td>
<td rowspan="2" style="text-align: center;">True Positive</td>
<td rowspan="2" style="text-align: center;">False Positive<br>Type I Error</td>
<td rowspan="2" style="text-align: center;">False Discovery Rate<br><span class="math inline">\(\mathbb{P}(\hat{y}\neq y|\hat{y}=1)\)</span></td>
</tr>
<tr class="odd">
</tr>
<tr class="even">
<td rowspan="2" style="text-align: center;"><span class="math inline">\(\hat{y}=0\)</span></td>
<td rowspan="2" style="text-align: center;">False Negative<br>Type II Error</td>
<td rowspan="2" style="text-align: center;">True Negative</td>
<td rowspan="2" style="text-align: center;">False Omission Rate<br><span class="math inline">\(\mathbb{P}(\hat{y}\neq y|\hat{y}=0)\)</span></td>
</tr>
<tr class="odd">
</tr>
<tr class="even">
<td colspan="2" rowspan="2" style="text-align: center;">Error Rate Type</td>
<td rowspan="2" style="text-align: center;">False Negative Rate<br><span class="math inline">\(\mathbb{P}(\hat{y}\neq y|y=1)\)</span></td>
<td rowspan="2" style="text-align: center;">False Positive Rate<br><span class="math inline">\(\mathbb{P}(\hat{y}\neq y|y=0)\)</span></td>
<td rowspan="2" style="text-align: center;">Error Rate<br><span class="math inline">\(\mathbb{P}(\hat{y}\neq y)\)</span></td>
</tr>
<tr class="odd">
</tr>
</tbody>
</table>
</div>
</section>
<section id="combined-table" class="level4 unnumbered">
<h4 class="unnumbered">Combined table</h4>
<div id="tbl:app_CMPerfMetrics2">
<table>
<caption>Table B.3: Summary of performance metrics for a binary classifier</caption>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th colspan="2" style="text-align: center;">Ground Truth</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Prediction</td>
<td style="text-align: center;"><span class="math inline">\(y=1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(y=0\)</span></td>
<td style="text-align: center;">Performance</td>
<td style="text-align: center;">Error rate</td>
</tr>
<tr class="even">
<td rowspan="2" style="text-align: center;"><span class="math inline">\(\hat{y}=1\)</span></td>
<td rowspan="2" style="text-align: center;">True Positive</td>
<td rowspan="2" style="text-align: center;">False Positive<br>Type I Error</td>
<td rowspan="2" style="text-align: center;">Positive Predictive Value<sup>a</sup><br><span class="math inline">\(\mathbb{P}(\hat{y}=y|\hat{y}=1)\)</span></td>
<td rowspan="2" style="text-align: center;">False Discovery Rate<br><span class="math inline">\(\mathbb{P}(\hat{y}\neq y|\hat{y}=1)\)</span></td>
</tr>
<tr class="odd">
</tr>
<tr class="even">
<td rowspan="2" style="text-align: center;"><span class="math inline">\(\hat{y}=0\)</span></td>
<td rowspan="2" style="text-align: center;">False Negative<br>Type II Error</td>
<td rowspan="2" style="text-align: center;">True Negative</td>
<td rowspan="2" style="text-align: center;">Negative Predictive Value<br><span class="math inline">\(\mathbb{P}(\hat{y}=y|\hat{y}=0)\)</span></td>
<td rowspan="2" style="text-align: center;">False Omission Rate<br><span class="math inline">\(\mathbb{P}(\hat{y}\neq y|\hat{y}=0)\)</span></td>
</tr>
<tr class="odd">
</tr>
<tr class="even">
<td rowspan="2" style="text-align: center;">Performance</td>
<td rowspan="2" style="text-align: center;">True Positive Rate<sup>b</sup><br><span class="math inline">\(\mathbb{P}(\hat{y}=y|y=1)\)</span></td>
<td rowspan="2" style="text-align: center;">True Negative Rate<br><span class="math inline">\(\mathbb{P}(\hat{y}=y|y=0)\)</span></td>
<td rowspan="2" style="text-align: center;">Accuracy<br><span class="math inline">\(\mathbb{P}(\hat{y}=y)\)</span></td>
<td rowspan="2" style="text-align: center;"></td>
</tr>
<tr class="odd">
</tr>
<tr class="even">
<td rowspan="2" style="text-align: center;">Error Rate</td>
<td rowspan="2" style="text-align: center;">False Negative Rate<br><span class="math inline">\(\mathbb{P}(\hat{y}\neq y|y=1)\)</span></td>
<td rowspan="2" style="text-align: center;">False Positive Rate<br><span class="math inline">\(\mathbb{P}(\hat{y}\neq y|y=0)\)</span></td>
<td rowspan="2" style="text-align: center;"></td>
<td rowspan="2" style="text-align: center;">Error rate <span class="math inline">\(\mathbb{P}(\hat{y}\neq y)\)</span></td>
</tr>
<tr class="odd">
</tr>
</tbody>
</table>
</div>
<div class="tablenotes">
<p><sup>a</sup> Positive Predictive Value = Precision</p>
<p><sup>b</sup> True Positive Rate = Recall</p>
</div>
</section>
</section>
</section>
<section id="app_ProbRules" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">C</span> Rules of Probability</h1>
<div id="tbl:app_ProbRules">
<table>
<caption>Table C.1: Rules of probability</caption>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Rule</strong></th>
<th style="text-align: left;"><strong>Continuous Variables</strong></th>
<th style="text-align: left;"><strong>Discrete Variables</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Sum rule</strong></td>
<td style="text-align: left;"><span class="math inline">\(\displaystyle f_{X}(x) = \int_{y\in\mathcal{Y}} f_{X,Y}(x,y) \, \mathrm{d}y\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\displaystyle \mathbb{P}(x) = \sum_{y\in\mathcal{Y}} \mathbb{P}(x,y)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Product rule</strong></td>
<td style="text-align: left;"><span class="math inline">\(f_{X,Y}(x,y) = f_{Y|X}(x,y) f_X(x)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\mathbb{P}(x,y) = \mathbb{P}(y|x) \mathbb{P}(x)\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Bayes’ rule</strong></td>
<td style="text-align: left;"><span class="math inline">\(\displaystyle f_{Y|X}(x,y) = \frac{f_{X|Y}(x,y) f_Y(y)}{f_X(x)}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\displaystyle \mathbb{P}(y|x) = \frac{\mathbb{P}(x|y)\mathbb{P}(y)}{\mathbb{P}(x)}\)</span></td>
</tr>
<tr class="even">
<td colspan="3" style="text-align: left;"><strong>Independence</strong></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(X\bot Y\)</span></td>
<td style="text-align: left;"><span class="math inline">\(f_{Y|X}(x,y) = f_Y(y)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\mathbb{P}(y|x) = \mathbb{P}(y)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">From the product rule</td>
<td style="text-align: left;"><span class="math inline">\(f_{X,Y}(x,y) = f_X(x)f_Y(y)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\mathbb{P}(x,y) = \mathbb{P}(x) \mathbb{P}(y)\)</span></td>
</tr>
<tr class="odd">
<td colspan="3" style="text-align: left;"><strong>Conditional Independence</strong></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(X \bot Y | Z\)</span></td>
<td style="text-align: left;"><span class="math inline">\(f_{Y|X,Z}(x,y,z) = f_{Y|Z}(y,z)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\mathbb{P}(y|x,z) = \mathbb{P}(y|z)\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Using the product rule</td>
<td style="text-align: left;"><span class="math inline">\(f_{X,Y|Z}(x,y,z) = f_{Y|X,Z}(x,y,z)f_{X|Z}(x,z)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\mathbb{P}(x,y|z) = \mathbb{P}(y|x,z)\mathbb{P}(x|z)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Substituting for <span class="math inline">\(Y|X,Z\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\phantom{f_{X,Y|Z}(x,y,z)} = f_{Y|Z}(y,z)f_{X|Z}(x,y)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\phantom{\mathbb{P}(x,y|z)} = \mathbb{P}(y|z)\mathbb{P}(x|z)\)</span></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="app_Solutions" class="level1" data-number="9">
<h1 data-number="9"><span class="header-section-number">D</span> Proofs and Code</h1>
<section id="sec_app_GFSolutions" class="level2" data-number="9.1">
<h2 data-number="9.1"><span class="header-section-number">D.1</span> Group Fairness</h2>
<section id="comparing-outcomes-2" class="level3" data-number="9.1.1">
<h3 data-number="9.1.1"><span class="header-section-number">D.1.1</span> Comparing Outcomes</h3>
<div class="lookbox">
<div id="GF_NPI">
<p><strong>Code: Normalised Prejudice Index</strong></p>
</div>
<p>Write a function that takes two arrays <span class="math inline">\(y\)</span> and <span class="math inline">\(z\)</span> of categorical features and returns the normalised prejudice index. Hint:</p>
<ol>
<li><p>Compute the probability distributions <span class="math inline">\(\mathbb{P}(y)\)</span>, <span class="math inline">\(\mathbb{P}(z)\)</span> and <span class="math inline">\(\mathbb{P}(y,z)\)</span>. Note that these can be thought of as the frequency with which each event occurs.</p></li>
<li><p>Compute the entropies <span class="math inline">\(H(y)\)</span> and <span class="math inline">\(H(z)\)</span> shown in equations (3.3) and (3.4). Use these to compute the normalising factor, <span class="math inline">\(\sqrt{H(y)H(z)}\)</span>.</p></li>
<li><p>Compute the mutual information <span class="math inline">\(I(z,y)\)</span> shown in equation (3.1) and divide by the normalising factor.</p></li>
</ol>
<p>See also <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.normalized_mutual_info_score.html">sklearn.metrics.normalized_mutual_info_score</a>.</p>
</div>
<div id="lst:Ex-npi" class="listing">
<p>Listing D.1: Calculating the normalised prejudice index</p>
<pre><code># Import the necessary classes
import pandas as pd
import scipy.stats as ss

def normalised_mutual_information(x, y):
    &quot;&quot;&quot;normalised mutual information between x and y&quot;&quot;&quot;
    
    # Compute the probability distributions
    px   = x.value_counts(normalize=True)
    py   = y.value_counts(normalize=True)
    pxy  = pd.Series(zip(x,y)).value_counts(normalize=True)
    
    # Compute the normalising factor
    norm = math.sqrt( ss.entropy(px) * ss.entropy(py)  )
    
    # Compute mutual information, divide by the normalising factor
    # and return the result
    return sum([p * math.log(p / (px[xy[0]] * py[xy[1]]))
                for xy, p in p_xy.items()]) / norm</code></pre>
</div>
<div class="lookbox">
<div id="GF_SPDmax">
<p><strong>Proof: Statistical Parity Difference Maximum</strong></p>
</div>
<p><span class="math display">\[d_{\max} = \min\left\{ \frac{\mathbb{P}(\hat{Y}=1)}{\mathbb{P}(Z=1)},
                       \frac{\mathbb{P}(\hat{Y}=0)}{\mathbb{P}(Z=0)} \right\}.\]</span></p>
</div>
<p>We can write statistical parity difference as <span class="math display">\[d = \mathbb{P}(\hat{Y}=1 | Z=1) - \mathbb{P}(\hat{Y}=1 | Z=0).\]</span> Let’s rewrite this with advantaged and disadvantaged outcomes and groups to make it more concrete, <span class="math display">\[d = \mathbb{P}(y^+|z^+) - \mathbb{P}(y^+|z^-)
  = \frac{\mathbb{P}(y^+, z^+)}{\mathbb{P}(z^+)} - \frac{\mathbb{P}(y^+, z^-)}{\mathbb{P}(z^-)}
  \leq \frac{\mathbb{P}(y^+)}{\mathbb{P}(z^+)}.\]</span> This maximal value occurs when <span class="math display">\[\mathbb{P}(y^+, z^+) = \mathbb{P}(y^+) \quad \text{and} \quad \mathbb{P}(y^+, z^-)=0;\]</span> that is, when all members of the advantaged class, receive the advantaged outcome. We can also write, <span class="math display">\[\begin{aligned}
d = \mathbb{P}(y^+|z^+) - \mathbb{P}(y^+|z^-)
  &amp; = \mathbb{P}(y^-|z^-) - \mathbb{P}(y^-|z^+) \\
  &amp; = \frac{\mathbb{P}(y^-, z^-)}{\mathbb{P}(z^-)}
    - \frac{\mathbb{P}(y^-, z^+)}{\mathbb{P}(z^+)} \leq \frac{\mathbb{P}(y^-)}{\mathbb{P}(z^-)}.
\end{aligned}\]</span> Here the maximal value occurs when <span class="math display">\[\mathbb{P}(y^-, z^-) = \mathbb{P}(y^-) \quad \text{and} \quad \mathbb{P}(y^-, z^+)=0;\]</span> that is, when all members of the disadvantaged class, receive the disadvantaged outcome. Thus, <span class="math display">\[d_{max} = \min\left\{ \frac{\mathbb{P}(y^+)}{\mathbb{P}(z^+)},
                      \frac{\mathbb{P}(y^-)}{\mathbb{P}(z^-)} \right\}.\]</span> Note that, <span class="math display">\[\frac{\mathbb{P}(y^+)}{\mathbb{P}(z^+)} = \frac{\mathbb{P}(y^-)}{\mathbb{P}(z^-)} \quad \Leftrightarrow \quad \mathbb{P}(y_+) = \mathbb{P}(z_+);\]</span> that is, when all members of the advantaged class, receive the advantaged outcome and all members of the disadvantaged class, receive the disadvantaged outcome.</p>
</section>
<section id="comparing-errors-2" class="level3" data-number="9.1.2">
<h3 data-number="9.1.2"><span class="header-section-number">D.1.2</span> Comparing Errors</h3>
<div class="lookbox">
<div id="GF_Suff">
<p><strong>Proof: Sufficiency</strong></p>
</div>
<p>Sufficiency is satisfied if and only if the false omission rate and false discovery rate are equal for all groups.</p>
</div>
<p>Sufficiency implies <span class="math display">\[\mathbb{P}(y|\hat{y}, z) = \mathbb{P}(y|\hat{y}).\]</span> For the simplest case of a binary classifier where we have a single sensitive binary feature. We can write this requirement as two conditions, <span class="math display">\[\begin{aligned}
\mathbb{P}(Y=1 | Z=1, \hat{Y}=1) &amp; = \mathbb{P}(Y=1 | Z=0, \hat{Y}=1), \\
\mathbb{P}(Y=1 | Z=1, \hat{Y}=0) &amp; = \mathbb{P}(Y=1 | Z=0, \hat{Y}=0).
\end{aligned}\]</span> Recall that <span class="math inline">\(\mathbb{P}(Y=1 | \hat{Y}=1)\)</span> is the positive predictive value (<span class="math inline">\(PPV\)</span>) of the classifier and <span class="math inline">\(\mathbb{P}(Y=1 | \hat{Y}=0)\)</span> is the false omission rate (<span class="math inline">\(FOR\)</span>). We see then that sufficiency requires the positive predictive value to be the same for all values of the sensitive feature and the false omission rate to be the same for all values of the sensitive feature. Note that the positive predictive value is balanced if and only if the false discovery rate is balanced, so thinking in terms of error metrics only, separation requires the false discovery and false omission rates to be balanced.</p>
</section>
<section id="incompatibility-of-fairness-criteria" class="level3" data-number="9.1.3">
<h3 data-number="9.1.3"><span class="header-section-number">D.1.3</span> Incompatibility of Fairness Criteria</h3>
<section id="separation-versus-sufficiency-1" class="level4" data-number="9.1.3.1">
<h4 data-number="9.1.3.1"><span class="header-section-number">D.1.3.1</span> Separation versus Sufficiency</h4>
<div class="lookbox">
<div id="GF_PredVal">
<p><strong>Proof: Predictive Values</strong></p>
</div>
<p>We can write the positive and negative predictive values in terms of the true and false positive rates as follows, <span class="math display">\[PPV = \frac{p TPR}{p TPR + (1-p)FPR}\]</span> and <span class="math display">\[NPV = \frac{(1-p)(1-FPR)}{p(1-TPR) + (1-p)(1-FPR)}\]</span> where <span class="math inline">\(p=\mathbb{P}(Y=1)\)</span>.</p>
</div>
<p>We start by looking at some relationships between the elements of a confusion matrix shown in Table <a href="#tbl:app_ConfMax" data-reference-type="ref" data-reference="tbl:app_ConfMax">D.1</a>.</p>
<div id="tbl:app_ConfMax">
<table>
<caption>Table D.1: Confusion matrix</caption>
<thead>
<tr class="header">
<th colspan="2" style="text-align: center;"></th>
<th colspan="2" style="text-align: center;">Ground Truth</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td colspan="2" style="text-align: center;"></td>
<td style="text-align: center;"><span class="math inline">\(y=1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(y=0\)</span></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td rowspan="4" style="text-align: center;">Prediction</td>
<td rowspan="2" style="text-align: center;"><span class="math inline">\(\hat{y}=1\)</span></td>
<td rowspan="2" style="text-align: center;">True Positive<br>(<span class="math inline">\(TP\)</span>)</td>
<td rowspan="2" style="text-align: center;">False Positive<br>(<span class="math inline">\(FP\)</span>)</td>
<td rowspan="2" style="text-align: center;"><span class="math inline">\(\displaystyle PPV = \frac{TP}{TP+FP}\)</span></td>
</tr>
<tr class="odd">
</tr>
<tr class="even">
<td rowspan="2" style="text-align: center;"><span class="math inline">\(\hat{y}=0\)</span></td>
<td rowspan="2" style="text-align: center;">False Negative<br>(<span class="math inline">\(FN\)</span>)</td>
<td rowspan="2" style="text-align: center;">True Negative<br>(<span class="math inline">\(TN\)</span>)</td>
<td rowspan="2" style="text-align: center;"><span class="math inline">\(\displaystyle NPV = \frac{TN}{FN+TN}\)</span></td>
</tr>
<tr class="odd">
</tr>
<tr class="even">
<td colspan="2" style="text-align: center;"></td>
<td style="text-align: center;"><span class="math inline">\(\begin{aligned}
  TPR &amp; = \frac{TP}{TP+FN} \\
1-TPR &amp; = \frac{FN}{TP+FN} \\
    p &amp; = \frac{TP+FN}{n}  \end{aligned}\)</span></td>
<td style="text-align: center;"><span><span class="math inline">\(\begin{aligned}
  FPR &amp; = \frac{FP}{FP+TN} \\
1-FPR &amp; = \frac{TN}{FP+TN} \\
  1-p &amp; = \frac{FP+TN}{n}  \end{aligned}\)</span></span></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
</div>
<p>where <span class="math inline">\(n= TP+FP+FN+TN\)</span> denotes the total number of data points. Using the equations in the final row of the table we can write, <span class="math display">\[\begin{aligned}
    p TPR &amp; = \frac{TP}{n}, &amp;     (1-p) FPR &amp; = \frac{FP}{n}, \\
p (1-TPR) &amp; = \frac{FN}{n}, &amp; (1-p) (1-FPR) &amp; = \frac{TP}{n}.
\end{aligned}\]</span> Finally, we can substitute these into our expressions for <span class="math inline">\(PPV\)</span> and <span class="math inline">\(NPV\)</span> in the right hand column of Table <a href="#tbl:app_ConfMax" data-reference-type="ref" data-reference="tbl:app_ConfMax">D.1</a> to find the relationships in equations (3.14) and (3.15). <span class="math display">\[\begin{aligned}
PPV &amp; = \frac{p TPR}{p TPR + (1-p)FPR} \\
NPV &amp; = \frac{(1-p)(1-FPR)}{p(1-TPR) + (1-p)(1-FPR)}.
\end{aligned}\]</span></p>
<div class="lookbox">
<div id="GF_SepVsSuff">
<p><strong>Proof: Separation versus Sufficiency</strong></p>
</div>
<p>For separation and sufficiency to hold we must have <span class="math display">\[FPR (p_a-p_b) TPR = 0\]</span> and <span class="math display">\[(1-FPR) (p_a-p_b) (1-TPR) = 0\]</span> for any pair of groups <span class="math inline">\(Z=a\)</span> and <span class="math inline">\(Z=b\)</span>.</p>
</div>
<p><span class="math display">\[\begin{aligned}
&amp; PPV_a = PPV_b \\
&amp; \Leftrightarrow\quad \frac{p_a TPR}{p_a TPR + (1-p_a)FPR}
                     = \frac{p_b TPR}{p_b TPR + (1-p_b)FPR} \\
&amp; \Leftrightarrow\quad p_b TPR[p_a TPR + (1-p_a)FPR]
                     = p_a TPR[p_b TPR + (1-p_b)FPR] \\
&amp;\Leftrightarrow\quad p_b TPR(1-p_a)FPR
                     = p_a TPR(1-p_b)FPR \\
&amp;\Leftrightarrow\quad TPR(p_b-p_a)FPR = 0.
\end{aligned}\]</span> Similarly, <span class="math display">\[\begin{aligned}
&amp; NPV_a = NPV_b \\
&amp; \Leftrightarrow \quad \frac{(1-p_a)(1-FPR)}{p_a(1-TPR) + (1-p_a)(1-FPR)}
= \frac{(1-p_b)(1-FPR)}{p_b(1-TPR) + (1-p_b)(1-FPR)} \\
&amp; \Leftrightarrow \quad (1-p_b)(1-FPR)[p_a(1-TPR) + (1-p_a)(1-FPR)] \\
&amp; \qquad\qquad = (1-p_a)(1-FPR)[p_b(1-TPR) + (1-p_b)(1-FPR)] \\
&amp; \Leftrightarrow \quad (1-p_b)(1-FPR)p_a(1-TPR) = (1-p_a)(1-FPR)p_b(1-TPR).\\
&amp; \Leftrightarrow \quad (1-FPR)(p_b-p_a)(1-TPR) = 0.
\end{aligned}\]</span></p>
</section>
</section>
</section>
<section id="sec_app_IFSolutions" class="level2" data-number="9.2">
<h2 data-number="9.2"><span class="header-section-number">D.2</span> Individual Fairness</h2>
<div class="lookbox">
<div id="IF_RandPreds">
<p><strong>Code: Randomised predictions</strong></p>
</div>
<p>Write a function which takes the model score from a binary classifier and makes randomised predictions between two thresholds so that the probability of acceptance is a continuous function of the model score:</p>
<ol>
<li><p>Write a function which maps the model score to the probability of acceptance. The function should take a two thresholds, <span class="math inline">\(t_1&lt;t_2\)</span>. The probability of acceptance should be zero if the score is less than <span class="math inline">\(t_1\)</span>, one if the score is greater than <span class="math inline">\(t_2\)</span> and increase linearly from zero to one for model scores between the two thresholds.</p></li>
<li><p>Write a function that takes a probability value <span class="math inline">\(p\)</span> and outputs the value one with probability <span class="math inline">\(p\)</span> and zero with probability <span class="math inline">\(1-p\)</span>.</p></li>
<li><p>Compose the functions above to complete the exercise.</p></li>
</ol>
<p>See section 4.5 of the notebook you downloaded and worked through in the previous chapter.</p>
</div>
<div id="lst:RandPreds" class="listing">
<p>Listing D.2: Randomising predictions between two thresholds</p>
<pre><code>def accept_probability(score, t1=0.45, t2=0.55):
    &quot;&quot;&quot;Probability of acceptance&quot;&quot;&quot;
    # Zero below t1
    if score&lt;=t1: return 0
    # One above t2
    if score&gt;=t2: return 1
    # Linearly increasing from zero to one between t1 and t2
    return (score-t1)/(t2-t1)

def predict(probability):
    &quot;&quot;&quot;Return 1 with probability probability&quot;&quot;&quot;
    return int(random.random()&lt;probability)

def model_prediction(model_score, t1=0.45, t2=0.55):
    &quot;&quot;&quot;Return random prediction given model score and thresholds&quot;&quot;&quot;
    return predict(accept_probability(model_score, t1,t2))</code></pre>
</div>
</section>
<section id="sec_app_IISolutions" class="level2" data-number="9.3">
<h2 data-number="9.3"><span class="header-section-number">D.3</span> Utility as Fairness</h2>
<div class="lookbox">
<p><strong><span class="math inline">\(I_2\)</span> and Relative Standard Deviation</strong></p>
<p><span class="math display">\[\frac{\sigma}{\mu} = \sqrt{2I_2(\boldsymbol{b})}.\]</span></p>
</div>
<p>Recall <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> are the mean and standard deviation respectively, <span class="math display">\[\mu = \frac{1}{n}\sum_{i=1}^n b_i
\qquad\textrm{and}\qquad
\sigma = \sqrt{\frac{1}{n}\sum_{i=1}^n (b_i-\mu)^2}.\]</span></p>
<div class="lookbox">
<div id="II_falphax">
<p><strong>Proof: Behaviour of <span class="math inline">\(f_{\alpha}(x)\)</span></strong></p>
</div>
<ul>
<li><p>For <span class="math inline">\(\alpha&lt;1\)</span>, <span class="math inline">\(f_{\alpha}(x)\)</span> is a strictly decreasing</p></li>
<li><p>For <span class="math inline">\(\alpha=1\)</span>, <span class="math inline">\(f_{\alpha}(x)\)</span> is minimal at <span class="math inline">\(x=e^{-1}\)</span></p></li>
<li><p>For <span class="math inline">\(\alpha&gt;1\)</span>, <span class="math inline">\(f_{\alpha}(x)\)</span> is a strictly increasing</p></li>
</ul>
</div>
<p>For <span class="math inline">\(\alpha=0\)</span>, <span class="math display">\[\begin{aligned}
f_0(x) = -\ln (x) \quad
&amp; \Rightarrow \quad f&#39;_0(x) = -\frac{1}{x} &lt; 0
              \quad \textrm{for} \quad x &gt; 0 \\
&amp; \Rightarrow \quad f_0(x) \textrm{ strictly decreasing for } x &gt; 0 \\
f_0(x) = 0 \quad &amp; \Leftrightarrow\quad x = 1.
\end{aligned}\]</span> For <span class="math inline">\(\alpha=1\)</span>, <span class="math display">\[\begin{aligned}
f_1(x) = x\ln x \quad
&amp; \Rightarrow \quad f&#39;_1(x) = 1 + \ln x = 0 \quad
  \Leftrightarrow\quad x = \frac{1}{e}.\\
&amp; \Rightarrow \quad f&#39;&#39;_1(x) = \frac{1}{x} &gt; 0 \quad\forall\;x &gt; 0 \\
&amp; \Rightarrow \quad f_1(x) \textrm{ is minimal at } x=\frac{1}{e} \\
f_1(x) = 0\quad &amp; \Leftrightarrow\quad x\in\{0,1\}, \\
&amp; \Rightarrow\quad
f_1(x) &gt; 0 \,\textrm{ for }\, x &gt; 1 \quad\mathrm{and}\quad
f_1(x) &lt; 0 \,\textrm{ for }\, x &lt; 1
\end{aligned}\]</span> For <span class="math inline">\(\alpha\in\mathbb{R}\)</span>, <span class="math inline">\(\alpha\notin\{0,1\}\)</span>, <span class="math display">\[\begin{aligned}
f_{\alpha}(x) = \frac{x^{\alpha}-1}{\alpha(\alpha-1)}\quad &amp; \Rightarrow\quad
f&#39;_{\alpha}(x) = \frac{x^{\alpha-1}}{\alpha-1}.\\
&amp; \Rightarrow\quad
f&#39;_1(x) &gt; 0 \,\textrm{ if }\, \alpha &gt; 1 \quad\mathrm{and}\quad
f&#39;_1(x) &lt; 0 \,\textrm{ if }\, \alpha &lt; 1 \\
&amp; \Rightarrow\quad f_{\alpha}(x) \textrm{ strictly decreasing for }\alpha&lt;1 \\
&amp; \Rightarrow\quad f_{\alpha}(x) \textrm{ strictly increasing for } \alpha&gt;1 \\
\end{aligned}\]</span></p>
<div class="lookbox">
<div id="II_GEIdecomp">
<p><strong>Proof: Generalised Entropy Index Decomposition</strong></p>
</div>
<p>For any partition <span class="math inline">\(G\)</span> of the population into subgroups, the generalised entropy index <span class="math inline">\(I\)</span>, is additively decomposable, into a within-group component <span class="math inline">\(I_{\omega}^G\)</span>, and between-group component <span class="math inline">\(I_{\beta}^G\)</span>, <span class="math display">\[\begin{aligned}
I(\boldsymbol{b};\alpha)
= \frac{1}{n}\sum_{i=1}^n f_{\alpha}\left(\frac{b_i}{\mu}\right)
= I_{\omega}^G(\boldsymbol{b};\alpha) + I_{\beta }^G(\boldsymbol{b};\alpha).
\end{aligned}\]</span> The within-group component is the weighted sum of the index measure for each subgroup <span class="math display">\[I_{\omega}^G(\boldsymbol{b};\alpha) = \sum_{g=1}^{|G|} \frac{n_g}{n} \left(\frac{\mu_g}{\mu}\right)^{\alpha} I(\boldsymbol{b}_g;\alpha)
\qquad \forall \, \alpha.\]</span> The between-group component is computed as the value of the index in the case where, each individual is assigned the mean benefit of their subgroup, <span class="math display">\[I_{\beta}^G(\boldsymbol{b};\alpha) = \sum_{g=1}^{|G|}
\frac{n_g}{n} f_{\alpha}\left(\frac{\mu_g}{\mu}\right).\]</span></p>
</div>
<p>We want to show that, for any partition <span class="math inline">\(G\)</span> of the population, we can write <span class="math display">\[I(\boldsymbol{b}) = \underbrace{I_{\omega}^G(\boldsymbol{b})}_{\text{within group component}} + \underbrace{I_{\beta }^G(\boldsymbol{b})}_{\text{between group component}}.\]</span> The within-group component is the weighted sum of the index measure for each subgroup, <span class="math display">\[I_{\omega}^G(\boldsymbol{b}) = \sum_{g=1}^{|G|} \frac{n_g}{n} \left(\frac{\mu_g}{\mu}\right)^{\alpha} I(\boldsymbol{b}_g)
\qquad \forall \, \alpha.\]</span> The between-group component is computed as value of the inequality measure where each individual is assigned the mean benefit of their subgroup.</p>
<section id="case-alpha0" class="level3 unnumbered">
<h3 class="unnumbered">Case: <span class="math inline">\(\alpha=0\)</span></h3>
<p>We follow the hint and isolate the summation over the natural logarithm of the benefits in the index computation, <span class="math display">\[\begin{aligned}
I_0(\boldsymbol{b})
&amp; = \frac{1}{n} \sum_{i=1}^n \ln \frac{\mu}{b_i} \\
\Rightarrow \quad n I_0(\boldsymbol{b})
&amp; = n \ln \mu - \sum_{i=1}^n \ln b_i \\
\Rightarrow \quad \sum_{i=1}^n \ln b_i
&amp; = n [\ln \mu - I_0(\boldsymbol{b})]
\end{aligned}\]</span> We can use this to relate the index values for the subgroups to the index value for the population: <span class="math display">\[\begin{aligned}
n [\ln \mu - I_0(\boldsymbol{b})]
&amp; = \sum_{g=1}^{|G|} n_g [\ln \mu_g - I_0(\boldsymbol{b_g})] \\
\Rightarrow \quad I_0(\boldsymbol{b})
&amp; = \ln \mu -
\sum_{g=1}^{|G|} \frac{n_g}{n} [\ln \mu_g - I_0(\boldsymbol{b_g})] \\
&amp; = \sum_{g=1}^{|G|} \frac{n_g}{n} I_0(\boldsymbol{b_g})
+ \ln \mu - \sum_{g=1}^{|G|} \frac{n_g}{n} \ln \mu_g \\
&amp; = \underbrace{\sum_{g=1}^{|G|} \frac{n_g}{n} I_0(\boldsymbol{b_g})}_{\text{within group component}} +
\underbrace{\sum_{g=1}^{|G|}\frac{n_g}{n}\ln \frac{\mu}{\mu_g} }_{\text{between group component}}
\end{aligned}\]</span></p>
</section>
<section id="case-alpha1" class="level3 unnumbered">
<h3 class="unnumbered">Case: <span class="math inline">\(\alpha=1\)</span></h3>
<p>We isolate the summation over <span class="math inline">\(b_i\)</span> in the index calculation, <span class="math display">\[\begin{aligned}
I_1(\boldsymbol{b})
&amp; = \frac{1}{n}\sum_{i=1}^{n} \frac{b_i}{\mu} \ln \frac{b_i}{\mu} \\
&amp; = \frac{1}{n\mu} \sum_{i=1}^{n} [b_i \ln b_i - b_i \ln \mu] \\
&amp; = \frac{1}{n\mu} \sum_{i=1}^{n} b_i \ln b_i - \ln \mu
\quad \text{since} \quad \frac{1}{n\mu} \sum_{i=1}^{n} b_i = 1 \\
\Rightarrow \quad \sum_{i=1}^{n} b_i \ln b_i
&amp; = n \mu [I_1(\boldsymbol{b}) + \ln\mu].
\end{aligned}\]</span> We can use this to relate the index values for the subgroups to the index value for the population: <span class="math display">\[\begin{aligned}
n \mu [I_1(\boldsymbol{b}) + \ln\mu]
&amp; = \sum_{g=1}^{|G|} n_g \mu_g [I_1(\boldsymbol{b}_g) + \ln\mu_g] \\
\Rightarrow \quad I_1(\boldsymbol{b}) &amp; = \sum_{g=1}^{|G|} \frac{n_g}{n}\frac{\mu_g}{\mu} [I_1(\boldsymbol{b}_g) + \ln\mu_g] - \ln\mu \\
&amp; = \sum_{g=1}^{|G|} \frac{n_g}{n}\frac{\mu_g}{\mu} I_1(\boldsymbol{b}_g) + \frac{1}{n}\sum_{g=1}^{|G|} n_g\frac{\mu_g}{\mu} [\ln\mu_g - \ln\mu] \quad \text{since} \quad \sum_{g=1}^{|G|} \frac{n_g}{n}\frac{\mu_g}{\mu} = 1 \\
&amp; = \underbrace{\sum_{g=1}^{|G|} \frac{n_g}{n}\frac{\mu_g}{\mu} I_1(\boldsymbol{b_g})}_{\text{within group component}} +
\underbrace{\frac{1}{n} \sum_{g=1}^{|G|}
n_g \frac{\mu_g}{\mu} \ln \left(\frac{\mu_g}{\mu}\right)}_{\text{between group component}}.
\end{aligned}\]</span></p>
</section>
<section id="case-alphanotin01" class="level3 unnumbered">
<h3 class="unnumbered">Case: <span class="math inline">\(\alpha\notin\{0,1\}\)</span></h3>
<p>We isolate the summation over <span class="math inline">\(b_i\)</span> in the index calculation, <span class="math display">\[\begin{aligned}
I_{\alpha}(\boldsymbol{b}) &amp; = \frac{1}{n\alpha(\alpha-1)}
\sum_{i=1}^n \left[ \left(\frac{b_i}{\mu}\right)^{\alpha}-1 \right] \\
\Rightarrow \quad n\alpha(\alpha-1) I_{\alpha}(\boldsymbol{b})
&amp; = \sum_{i=1}^n \left(\frac{b_i}{\mu}\right)^{\alpha} - n \\
\Rightarrow \quad \sum_{i=1}^n b_i^{\alpha}
&amp; = n\mu^{\alpha}[\alpha(\alpha-1)I_{\alpha}(\boldsymbol{b})+1]
\end{aligned}\]</span> We can use this to relate the index values for the subgroups to the index value for the population: <span class="math display">\[\begin{aligned}
&amp; n\mu^{\alpha}[\alpha(\alpha-1)I_{\alpha}(\boldsymbol{b})+1] =
\sum_{g=1}^{|G|} n_g\mu_g^{\alpha}
[\alpha(\alpha-1)I_{\alpha}(\boldsymbol{b}_g)+1] \\
\Rightarrow\quad I_{\alpha}(\boldsymbol{b}_g)
&amp; = \frac{1}{\alpha(\alpha-1)} \left[\sum_{g=1}^{|G|}
\frac{n_g}{n}\left(\frac{\mu_g}{\mu}\right)^{\alpha}
[\alpha(\alpha-1)I_{\alpha}(\boldsymbol{b}_g)+1]-1\right]  \\
&amp; = \sum_{g=1}^{|G|}
\frac{n_g}{n}\left(\frac{\mu_g}{\mu}\right)^{\alpha}
\left[I_{\alpha}(\boldsymbol{b}_g) + \frac{1}{\alpha(\alpha-1)}\right]
- \frac{1}{\alpha(\alpha-1)}  \\
&amp; = \sum_{g=1}^{|G|} \frac{n_g}{n}\left(\frac{\mu_g}{\mu}\right)^{\alpha} I_{\alpha}(\boldsymbol{b}_g) + \frac{1}{\alpha(\alpha-1)}\sum_{g=1}^{|G|}
\frac{n_g}{n}\left(\frac{\mu_g}{\mu}\right)^{\alpha}
- \frac{1}{\alpha(\alpha-1)}  \\
&amp; = \underbrace{\sum_{g=1}^{|G|} \frac{n_g}{n}
\left(\frac{\mu_g}{\mu}\right)^{\alpha} I_{\alpha}(\boldsymbol{b}_g)}_{\text{within group component}} + \underbrace{\frac{1}{n\alpha(\alpha-1)}
\sum_{g=1}^{|G|}n_g\left[\left(\frac{\mu_g}{\mu}\right)^{\alpha}-1\right]}_{\text{between group component}}
\end{aligned}\]</span></p>
<div class="lookbox">
<div id="II_GEImax">
<p><strong>Proof: Generalised Entropy Index Maximum</strong></p>
</div>
<p><span class="math display">\[\max_{\boldsymbol{b}}[I_{\alpha}(\boldsymbol{b})] = \left\{
\begin{array}{cl}
\ln n &amp; \textrm{for}\quad\alpha=1 \\
\dfrac{n^{\alpha-1}-1}{\alpha(\alpha-1)}
      &amp; \textrm{for}\quad\alpha&gt;0
\end{array}\right.\]</span></p>
</div>
<p>Recall from equations (5.1) - (5.4), <span class="math display">\[I_{\alpha}(\boldsymbol{b})
= \frac{1}{n}\sum_{i=1}^n f_{\alpha}\left(\frac{b_i}{\mu}\right)
\qquad\textrm{and}\qquad
I_{\alpha}(\boldsymbol{p})
= \mathbb{E}\left[f_{\alpha}(nP)\right]\]</span> where <span class="math display">\[f_{\alpha}(x) = \left\{
\begin{array}{cl}
-\ln x &amp; \textrm{if}\quad \alpha=0 \\
x\ln x &amp; \textrm{if}\quad \alpha=1 \\
\rule{0em}{3.5ex}
\dfrac{x^{\alpha}-1}{\alpha(\alpha-1)}
       &amp; \textrm{if}\quad \alpha\in\mathbb{R}.
\end{array}\right.\]</span></p>
</section>
<section id="case-alpha0-1" class="level3 unnumbered">
<h3 class="unnumbered">Case: <span class="math inline">\(\alpha=0\)</span></h3>
<p>We write the generalised entropy index as, <span class="math display">\[I_0(\boldsymbol{b})
= \frac{1}{n}\sum_{i=1}^n -\ln\left(\frac{b_i}{\mu}\right)
\qquad\textrm{and}\qquad
I_0(\boldsymbol{p}) = \mathbb{E}[-\ln(nP)].\]</span> The index is minimal when <span class="math inline">\(P=1/n\)</span> and unbounded above. Note that for <span class="math inline">\(\alpha=0\)</span> the index is undefined for a benefit of zero. For <span class="math inline">\(\alpha\leq0\)</span>, the index is unbounded.</p>
</section>
<section id="case-alpha1-1" class="level3 unnumbered">
<h3 class="unnumbered">Case: <span class="math inline">\(\alpha=1\)</span></h3>
<section id="proof-1." class="level5 unnumbered">
<h5 class="unnumbered">Proof 1.</h5>
<p>In this case we write the generalised entropy index as, <span class="math display">\[\begin{aligned}
I_1(\boldsymbol{p}) &amp; = \mathbb{E}[nP\ln(nP)] = n\mathbb{E}[P(\ln n + \ln P)] \\
&amp; = \ln n + n\mathbb{E}(P\ln P).
\end{aligned}\]</span> We know from earlier analysis of <span class="math inline">\(f_1(x)=x\ln x\)</span> that <span class="math inline">\(f_1(0)=f_1(1)=0\)</span> and <span class="math inline">\(f_1(x)\leq0\)</span> for <span class="math inline">\(x\in[0,1]\)</span>. Thus <span class="math inline">\(\max[I_1(\boldsymbol{b})]=\ln n\)</span>.</p>
</section>
<section id="proof-2." class="level5 unnumbered">
<h5 class="unnumbered">Proof 2.</h5>
<p>Suppose our benefits array is binary and <span class="math inline">\(m\)</span> of the <span class="math inline">\(n\)</span> elements is one and the remaining <span class="math inline">\(n-m\)</span> elements are zero. Then we have <span class="math inline">\(\mu=m/n\)</span>, <span class="math display">\[\begin{aligned}
I_1(\boldsymbol{b})
&amp; = \frac{1}{n}\sum_{i=1}^n \frac{b_i}{\mu}\ln\left(\frac{b_i}{\mu}\right)
= \frac{1}{n}\sum_{i=1}^{m} \frac{n}{m} \ln \frac{n}{m} = \ln \frac{n}{m} \\
&amp; = \ln n - \ln m
\end{aligned}\]</span> The index is a decreasing function of <span class="math inline">\(m\)</span>. We know it is zero when <span class="math inline">\(m=n\)</span> and maximal at <span class="math inline">\(m=1\)</span>, when, <span class="math inline">\(\max[I_1(\boldsymbol{b})]=\ln n\)</span>.</p>
</section>
</section>
<section id="case-alphanotin01-1" class="level3 unnumbered">
<h3 class="unnumbered">Case: <span class="math inline">\(\alpha\notin\{0,1\}\)</span></h3>
<section id="proof-1.-1" class="level5 unnumbered">
<h5 class="unnumbered">Proof 1.</h5>
<p>We write the generalised entropy index as, <span class="math display">\[I_{\alpha}(\boldsymbol{p})
= \frac{\mathbb{E}[(nP)^{\alpha}]-1}{\alpha(\alpha-1)}
= \frac{n^{\alpha}\mathbb{E}(P^{\alpha})-1}{\alpha(\alpha-1)}.\]</span> For <span class="math inline">\(\alpha\notin\{0,1\}\)</span>, <span class="math inline">\(P^{\alpha}\)</span> is a strictly increasing function of <span class="math inline">\(P\in[0,1]\)</span> and so maximal when <span class="math inline">\(\mathbb{P}(P=1)=1/n\)</span>. It’s straightforward to show that, in this case we have <span class="math inline">\(\max[\mathbb{E}(P^{\alpha})]=1/n\)</span>. Substituting completes the proof.</p>
</section>
<section id="proof-2.-1" class="level5 unnumbered">
<h5 class="unnumbered">Proof 2.</h5>
<p>For a binary array of benefits with <span class="math inline">\(m\)</span> of the <span class="math inline">\(n\)</span> elements being non-zero we can write this as, <span class="math display">\[I_{\alpha}(\boldsymbol{b}) = \frac{1}{n\alpha(\alpha-1)}
\left[ m \left(\frac{n}{m}\right)^{\alpha}-n \right]
= \frac{1}{\alpha(\alpha-1)}
\left[ \left(\frac{n}{m}\right)^{\alpha-1}-1 \right].\]</span> The index is a decreasing function of <span class="math inline">\(m\)</span>, it takes it’s maximal value at <span class="math inline">\(m=1\)</span>. Substituting completes the proof.</p>
<div class="lookbox">
<div id="II_Binary">
<p><strong>Index value for Binary Benefits</strong></p>
</div>
<p>For binary benefits, the value of the index is given by <span class="math display">\[I_{\alpha}(\boldsymbol{b}) = I_{\alpha}(\mu)
= \left\{
\begin{array}{cl}
- \ln\mu &amp; \textrm{for}\quad\alpha=1 \\
\rule{0em}{4ex}
\dfrac{1}{\alpha(\alpha-1)}\left(\dfrac{1}{\mu^{\alpha-1}}-1\right)
&amp; \textrm{for}\quad\alpha&gt;0.
\end{array}\right.\]</span></p>
</div>
<p>Let’s suppose our model makes <span class="math inline">\(n_c\)</span> correct predictions (in which case <span class="math inline">\(b=1\)</span>) and the remaining <span class="math inline">\(n_-=n-n_c\)</span> predictions are errors (in which case <span class="math inline">\(b=0\)</span>). We can write the value of the index as, <span class="math display">\[\begin{aligned}
I_{\alpha}(\boldsymbol{b}) &amp; = \frac{1}{n}\left[(n-n_c)f_{\alpha}(0)
+ n_c f_{\alpha}\left(\frac{1}{\mu}\right)\right]\\
&amp; = (1-\mu) f_{\alpha}(0) + \mu f_{\alpha}\left(\frac{1}{\mu}\right),
\end{aligned}\]</span> since the mean error <span class="math inline">\(\mu=n_c/n\)</span> is exactly the accuracy of our model. From equation (5.2) we know, <span class="math display">\[\begin{aligned}
(1-\mu)f_{\alpha}(0) &amp; = \left\{
\begin{array}{cl}
0                            &amp; \textrm{for}\quad\alpha=1 \\
\rule{0em}{4ex}
\dfrac{\mu-1}{\alpha(\alpha-1)} &amp; \textrm{for}\quad\alpha&gt;0.
\end{array}\right.\\
\mu f_{\alpha}\left(\dfrac{1}{\mu}\right) &amp; = \left\{
\begin{array}{cl}
-\ln\mu
&amp; \textrm{for}\quad\alpha=1 \\ \rule{0em}{4ex}
\dfrac{1}{\alpha(\alpha-1)}\left(\dfrac{1}{\mu^{\alpha-1}}-\mu\right)
&amp; \textrm{for}\quad\alpha&gt;0.
\end{array}\right.
\end{aligned}\]</span> Substituting completes the proof.</p>
<div class="lookbox">
<div id="II_Luck">
<p><strong>Index value for Equal Luck</strong></p>
</div>
<p><span class="math display">\[I_{\alpha}\left(\mu,\lambda\right) = \left\{
\begin{array}{cl}
\left(1-\dfrac{\lambda}{\mu}\right)\ln2-\ln\mu
&amp; \textrm{for}\quad\alpha = 1 \\
\rule{0em}{4.5ex}
\dfrac{1}{\alpha(\alpha-1)}
\left[ \left(\dfrac{2}{\mu}\right)^{\alpha-1}
- \dfrac{(2^{\alpha-1}-1)}{\mu^{\alpha}}\lambda - 1 \right]
&amp; \textrm{for}\quad \alpha&gt;0.
\end{array}\right.\]</span></p>
</div>
<p>Let’s suppose our model makes <span class="math inline">\(n_c\)</span> correct predictions (in which case <span class="math inline">\(b=1\)</span>); <span class="math inline">\(n_+\)</span> false positive predictions (in which case <span class="math inline">\(b=2\)</span>); and the remaining <span class="math inline">\(n-n_c-n_+\)</span> predictions are false negative (in which case <span class="math inline">\(b=0\)</span>). We can write the value of the index as, <span class="math display">\[I_{\alpha}(\boldsymbol{b}) = \frac{1}{n}\left[(n-n_c-n_+)f_{\alpha}(0)
+ n_c f_{\alpha}\left(\frac{1}{\mu}\right)
+ n_+ f_{\alpha}\left(\frac{2}{\mu}\right)\right].\]</span> From equation (5.2) we know, <span class="math display">\[\begin{aligned}
f_{\alpha}(0) &amp; = \left\{
\begin{array}{cl}
0                            &amp; \textrm{for}\quad\alpha=1 \\
\rule{0em}{4ex}
\dfrac{-1}{\alpha(\alpha-1)} &amp; \textrm{for}\quad\alpha&gt;0,
\end{array}\right.\\
f_{\alpha}\left(\dfrac{1}{\mu}\right) &amp; = \left\{
\begin{array}{cl}
-\dfrac{\ln\mu}{\mu}
&amp; \textrm{for}\quad\alpha=1 \\ \rule{0em}{4ex}
\dfrac{1}{\alpha(\alpha-1)}\left(\dfrac{1}{\mu^{\alpha}}-1\right)
&amp; \textrm{for}\quad\alpha&gt;0,
\end{array}\right. \\
f_{\alpha}\left(\dfrac{2}{\mu}\right) &amp; = \left\{
\begin{array}{cl}
\dfrac{2(\ln2-\ln\mu)}{\mu}
&amp; \textrm{for}\quad\alpha=1 \\ \rule{0em}{4ex}
\dfrac{1}{\alpha(\alpha-1)}\left(\dfrac{2^{\alpha}}{\mu^{\alpha}}-1\right)
&amp; \textrm{for}\quad\alpha&gt;0.
\end{array}\right. \\
\Rightarrow\quad I_{\alpha}(\boldsymbol{b}) &amp; = \left\{
\begin{array}{cl}
-\dfrac{(n_c+2n_+)}{n}\dfrac{\ln\mu}{\mu} + \dfrac{2n_+\ln2}{n\mu}
&amp; \textrm{for}\quad\alpha=1 \\ \rule{0em}{4ex}
\dfrac{1}{\alpha(\alpha-1)}\left(\dfrac{n_c+2^{\alpha}n_+}{n\mu^{\alpha}} - 1\right)
&amp; \textrm{for}\quad\alpha&gt;0.
\end{array}\right.
\end{aligned}\]</span> Let us denote the accuracy of our model with <span class="math inline">\(\lambda\)</span>. We have, <span class="math display">\[\lambda = \frac{n_c}{n} \quad\textrm{and}\quad \mu = \frac{n_c+2n_+}{n}
\quad\Rightarrow\quad \frac{2n_+}{n} = \mu-\lambda.\]</span> Substituting completes the proof.</p>
<div class="lookbox">
<div id="II_IndexTpt">
<p><strong>Index turning point</strong></p>
</div>
<p>The index has exactly one turning point (a maxima) for <span class="math inline">\(\alpha&gt;0\)</span>, at <span class="math inline">\(\mu=\tilde{\mu}\)</span> where, <span class="math inline">\(\tilde{\mu} = g(\alpha)\lambda\)</span> and, <span class="math display">\[\quad g(\alpha) = \left\{
\begin{array}{cl}
\ln2 &amp; \textrm{for}\quad\alpha = 1 \\
\rule{0em}{3.8ex}
\dfrac{\alpha(2^{\alpha-1}-1)}{(\alpha-1)2^{\alpha-1}}
&amp; \textrm{for}\quad \alpha&gt;0
\end{array}\right.\]</span></p>
</div>
<p>We wish to find the maximal value of the index for a given accuracy. We start by looking for turning points. Differentiating equation (5.12), <span class="math display">\[\frac{ \partial I_{\alpha} }{ \partial \mu } = \left\{
\begin{array}{cl}
\dfrac{1}{\mu^2} \left( \lambda\ln2 - \mu \right)
&amp; \textrm{for}\quad\alpha = 1 \\
\rule{0em}{4.1ex}
\dfrac{\alpha(2^{\alpha-1}-1)\lambda - (\alpha-1)2^{\alpha-1}\mu}
{\alpha(\alpha-1)\mu^{\alpha+1}}
&amp; \textrm{for}\quad \alpha&gt;0
\end{array}\right.\]</span> <span class="math display">\[\frac{ \partial I_{\alpha} }{ \partial \mu } = 0 \quad\Leftrightarrow\quad
\mu = \tilde{\mu} = g(\alpha)\lambda \quad\textrm{where}\quad
g(\alpha) = \left\{
\begin{array}{cl}
\ln2 &amp; \textrm{for}\quad\alpha = 1 \\
\rule{0em}{3.8ex}
\dfrac{\alpha(2^{\alpha-1}-1)}{(\alpha-1)2^{\alpha-1}}
&amp; \textrm{for}\quad \alpha&gt;0
\end{array}\right.\]</span> <span class="math display">\[\frac{ \partial^2 I_{\alpha} }{ \partial \mu^2 } =\left\{
\begin{array}{cl}
\dfrac{1}{\mu^3} \left[\mu-\lambda2\ln2\right]
&amp; \textrm{for}\quad\alpha = 1 \\
\rule{0em}{4.1ex}
\dfrac{2^{\alpha-1}}{\mu^{\alpha+2}}
\left[\mu-\dfrac{(\alpha+1)(2^{\alpha-1}-1)}{(\alpha-1)2^{\alpha-1}}
      \lambda\right] &amp; \textrm{for}\quad \alpha&gt;0
\end{array}\right.\]</span> <span class="math display">\[\Rightarrow\quad\left.\frac{ \partial^2 I_{\alpha} }{ \partial \mu^2 }\right|_{\mu=\tilde{\mu}} = \left\{
\begin{array}{cl}
-\dfrac{\ln2}{\tilde{\mu}^3} \lambda
&amp; \textrm{for}\quad\alpha = 1 \\
\rule{0em}{4.1ex}
-\dfrac{(2^{\alpha-1}-1)}{\tilde{\mu}^{\alpha+2}(\alpha-1)} \lambda
&amp; \textrm{for}\quad \alpha&gt;0
\end{array}\right\}&lt;0 \quad\forall\,\alpha&gt;0.\]</span></p>
</section>
</section>
</section>
</section>
<section id="app_AIF360" class="level1" data-number="10">
<h1 data-number="10"><span class="header-section-number">E</span> AIF360</h1>
<ol>
<li><p>In this book we will use Python in Jupyter notebooks from the <a href="https://www.anaconda.com//products/individual">Anaconda Python distribution platform</a>. If you don’t already have it download and install it.</p></li>
<li><p>Create an environment named <code>mbml</code>. Using the command line interface (CLI):</p>
<pre><code>\$ conda create --name mbml python=3.7</code></pre></li>
<li><p>Activate your new environment:</p>
<pre><code>$ conda activate mbml</code></pre></li>
<li><p>This book is a work in progress. As part of analysing the metrics and methods it uses code that is not yet available with the library<span class="sidenote-wrapper"><label for="sn-10" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-10" class="margin-toggle"/><span class="sidenote">If you’re interested, here is the <a href="https://github.com/Trusted-AI/AIF360/pull/214">open pull request</a>.<br />
<br />
</span></span>. Once it is merged, you will just be able to just pip install the aif360 library. Until then you must clone this <a href="https://github.com/leenamurgai/AIF360">fork of AIF360</a>:</p>
<pre><code>$ git clone https://github.com/leenamurgai/AIF360.git</code></pre></li>
<li><p>Download the notebook <code>mbml_german.ipynb</code> from <a href="https://git.manning.com/agileauthor/murgai/-/blob/master/code/mbml_german.ipynb">Manning’s GitLab repository</a> and save it in the "AIF360/examples" folder.</p></li>
<li><p>You should now be able to open and run the notebook from the CLI as you usually would:</p>
<pre><code>$ jupyter notebook mbml_german.ipynb</code></pre></li>
</ol>
<section id="app_AIF360_GF" class="level2" data-number="10.1">
<h2 data-number="10.1"><span class="header-section-number">E.1</span> Group Fairness</h2>
<section id="comparing-outcomes-3" class="level3" data-number="10.1.1">
<h3 data-number="10.1.1"><span class="header-section-number">E.1.1</span> Comparing Outcomes</h3>
<p>Now that we have covered some measures of fairness, let’s dive into calculating them. In this book we are going to use IBM’s <a href="https://aif360.mybluemix.net/resources">AI Fairness 360 (AIF360)</a>. AIF360 is currently the most comprehensive open source library available for measuring and mitigating bias in machine learning models. The Python package includes an extensive set of metrics for datasets and models to test for biases, explanations for these metrics, and algorithms to mitigate bias in datasets and models many of which we will cover in this book. The system has been designed to be extensible, adopted software engineering best practices to maintain code quality, and is well <a href="https://aif360.readthedocs.io/en/latest/index.html">documented</a>. The package implements techniques from at-least eight published papers and includes over 71 bias detection metrics and nine bias mitigation algorithms. These techniques can all be called in a standard way, similar to scikit-learn’s fit/transform/predict paradigm.</p>
<p>In this section we’re going to use AIF360 to calculate some of the metrics we’ve talked about in the previous section as a means to get started working with it. For calculating the metrics we’ve talked about so far, using AIF360 might seem to add unnecessary overhead as they are reasonably straightforward to code up directly once you have your data in a Pandas DataFrame. But remember, the library contains implementations of more complicated metrics and bias mitigations algorithms that we’ll cover later on in this book. Before we can use the library, we need to install it. Instructions are provided in Appendix <a href="#app_AIF360" data-reference-type="ref" data-reference="app_AIF360">E</a>.</p>
<section id="statlog-german-credit-data-data-set" class="level4 unnumbered">
<h4 class="unnumbered">Statlog (German Credit Data) Data Set</h4>
<p>The Jupyter Notebook, <a href="https://github.com/leenamurgai/mitigatingbiasml/blob/master/code/source/mbml-german.ipynb"><code>mbml_german.ipynb</code></a>, contains an example calculating some of the above fairness metrics on both a dataset and model output. It uses the <a href="https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)">Statlog (German Credit Data) Data Set</a>, in which one thousand loan applicants are classified as representing ‘good’ or ‘bad’ credit risks based on features such as loan term, loan amount, age, gender, marital status and more.</p>
<div class="lookbox">
<p><strong>Exercise: Statlog (German Credit Data) Data Set</strong></p>
<p>Sections 1-3 in the Jupyter Notebook, <a href="https://github.com/leenamurgai/mitigatingbiasml/blob/master/code/source/mbml-german.ipynb"><code>mbml_german.ipynb</code></a>, load the data and perform some exploratory data analysis (EDA), looking at correlation heat maps (using a variety of different measures of association) and comparing distributions of the target for different values of the features. Open the notebook and run the code up to section four. You should be able to answer the following questions by working through the notebook.</p>
<ol>
<li><p>What proportion of the population is classified as male/female?</p></li>
<li><p>What proportion of the population have good credit vs bad?</p></li>
<li><p>How many continuous variables are there? What are they? Do any of them appear to be related? If so how?</p></li>
<li><p>How many categorical variables are there? What are they? Do any of them appear to be related? If so how?</p></li>
</ol>
</div>
</section>
<section id="calculating-independence-metrics" class="level4 unnumbered">
<h4 class="unnumbered">Calculating Independence Metrics</h4>
<p>In order to calculate our metrics on the data using AIF360, we must have it in the correct format; that is, in a Pandas DataFrame (<code>data_df</code>) containing only numeric data types. In code listing E.1, we calculate the rate at which male and female applicants are classified as being good credit risks (<code>base_rate</code>) along with the difference (<code>mean_difference</code>) and the ratio (<code>disparate_impact</code>) of these rates.</p>
<div id="lst:AIF360metric" class="listing">
<p>Listing E.1: Calculating independence metrics for the data using AIF360</p>
<pre><code># Create a DataFrame to store results in
outcomes_df = pd.DataFrame(columns=[`female&#39;, `male&#39;,
                                    `difference&#39;, `ratio&#39;],
                           index=[`data&#39;, `model&#39;,
                                  `train data&#39;, `train model&#39;,
                                  `test data&#39;, `test model&#39;])

# Define privileged and unprivileged groups
privileged_groups = [{`sex_male&#39;:1}]
unprivileged_groups = [{`sex_male&#39;:0}]

# Create an instance of BinaryLabelDataset
data_ds = BinaryLabelDataset(df = data_df,
    label_names = [`goodcredit&#39;],
    protected_attribute_names = [`sex&#39;])

# Create an instance of BinaryLabelDatasetMetric
data_metric = BinaryLabelDatasetMetric(data_ds,
    privileged_groups = privileged_groups,
    unprivileged_groups = unprivileged_groups)

# Compute the metrics with data_metric and store them in outcomes_df
outcomes_df.at[`data&#39;, `female&#39;] = data_metric.base_rate(privileged=0)
outcomes_df.at[`data&#39;, `male&#39;] = data_metric.base_rate(privileged=1)
outcomes_df.at[`data&#39;, `difference&#39;] = data_metric.mean_difference()
outcomes_df.at[`data&#39;, `ratio&#39;] = data_metric.disparate_impact()</code></pre>
</div>
<p>In the notebook we look at these metrics on both the data and the model output for three different sets of the data (the full dataset, the train set and the test set) with two different models (one trained on the full dataset and another trained only on a subset of the data - the training set). In code listing E.1, we create a DataFrame to display the results in (<code>outcomes_df</code>) and populate the first row of it. First we define our privileged and unprivileged groups.</p>
<div class="lookbox">
<p><strong>Defining privileged and unprivileged groups</strong></p>
<p>The format for these is a list of dictionaries. Each dictionary in the list defines a group, the key being a feature and the value being the value of the feature for members of the group. The key, value pairs in the dictionaries are joined with an intersection (AND operator) and the dictionaries in the list are joined with a union (OR operator). So for example,</p>
<pre><code>[{`sex&#39;: 1, `age&gt;=30&#39;: 1}, {`sex&#39;: 0}]</code></pre>
<p>corresponds to individuals such that,</p>
<pre><code>(data_df[`sex&#39;]==1 AND data_df[`age&gt;=30&#39;]==1)  OR (data_df[`sex&#39;]==0)</code></pre>
</div>
<p>Next we create a <a href="https://aif360.readthedocs.io/en/latest/modules/generated/aif360.datasets.BinaryLabelDataset.html#aif360.datasets.BinaryLabelDataset"><code>BinaryLabelDataset</code></a> object (<code>data_ds</code>) which in turn is used to create a <a href="https://aif360.readthedocs.io/en/latest/modules/generated/aif360.metrics.BinaryLabelDatasetMetric.html#aif360.metrics.BinaryLabelDatasetMetric"><code>BinaryLabelDatasetMetric</code></a> object (<code>data_metric</code>). We then calculate the fairness metrics from <code>data_metric</code> and store the results in <code>outcomes_df</code>.</p>
<div class="lookbox">
<p><strong>Exercise: Multiple sensitive features</strong></p>
<p>Calculate independence metrics (base rates, difference and ratio) for the full dataset in the case where the privileged group is males age 30 and over, and the unprivileged group is females under the age of 30. Do this two ways, using AIF360 and using Pandas. Compare your results to make sure they match.</p>
</div>
<p>Once we have trained a model and made predictions, similar code can be written to calculate independence metrics on the model predictions for the full dataset. Code listing E.2 shows how we do this using the predictions from the trained model <code>clf</code>.</p>
<div id="lst:AIF360metric1" class="listing">
<p>Listing E.2: Calculating independence metrics for the model using AIF360</p>
<pre><code># Create a DataFrame with the features and model predicted target
model_df = pd.concat([X, pd.Series(clf.predict(X), name=`goodcredit&#39;)],
	axis=1)

# Create an instance of BinaryLabelDataset
model_ds = BinaryLabelDataset(df = model_df,
    label_names = [`goodcredit&#39;],
    protected_attribute_names = [`sex_male&#39;])

# Create an instance of BinaryLabelDatasetMetric
model_metric = BinaryLabelDatasetMetric(model_ds,
    privileged_groups = privileged_groups,
    unprivileged_groups = unprivileged_groups)

# Compute the metrics with model_metric and store them in outcomes_df
outcomes_df.at[`model&#39;, `female&#39;] = model_metric.base_rate(privileged=0)
outcomes_df.at[`model&#39;, `male&#39;] = model_metric.base_rate(privileged=1)
outcomes_df.at[`model&#39;, `difference&#39;] = model_metric.mean_difference()
outcomes_df.at[`model&#39;, `ratio&#39;] = model_metric.disparate_impact()</code></pre>
</div>
<p>Table <a href="#tbl:app_IndependenceMetrics" data-reference-type="ref" data-reference="tbl:app_IndependenceMetrics">E.1</a> shows the results of the calculations stored in <code>outcomes_df</code> from the notebook. From Table <a href="#tbl:app_IndependenceMetrics" data-reference-type="ref" data-reference="tbl:app_IndependenceMetrics">E.1</a> we note some variation in the rates at which men and women are predicted to present good credit risks for the model versus the data. In particular, the model acceptance rates are higher for both male and female applicants than those observed in the data. There are particularly big differences when we compare results for the test data versus the model on the test data (test model), which is not surprising since the mean difference and impact ratio for the train data and test data are markedly different. In addition we are aware that our model is overfitting. Without intervention, our model appears to be reducing the bias present in the data for the test set (as measured by our independence metrics).</p>
<div id="tbl:app_IndependenceMetrics">
<table>
<caption>Table E.1: Acceptance rates for the Statlog (German Credit) Data Set.</caption>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">Female</th>
<th style="text-align: right;">Male</th>
<th style="text-align: right;">Difference</th>
<th style="text-align: right;">Ratio</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Data</td>
<td style="text-align: right;">0.648</td>
<td style="text-align: right;">0.723</td>
<td style="text-align: right;">-0.0748</td>
<td style="text-align: right;">0.897</td>
</tr>
<tr class="even">
<td style="text-align: left;">Model<sup>a</sup></td>
<td style="text-align: right;">0.674</td>
<td style="text-align: right;">0.749</td>
<td style="text-align: right;">-0.0751</td>
<td style="text-align: right;">0.900</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Train data</td>
<td style="text-align: right;">0.659</td>
<td style="text-align: right;">0.719</td>
<td style="text-align: right;">-0.0601</td>
<td style="text-align: right;">0.916</td>
</tr>
<tr class="even">
<td style="text-align: left;">Train model<sup>b</sup></td>
<td style="text-align: right;">0.667</td>
<td style="text-align: right;">0.731</td>
<td style="text-align: right;">-0.0647</td>
<td style="text-align: right;">0.911</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Test data</td>
<td style="text-align: right;">0.607</td>
<td style="text-align: right;">0.741</td>
<td style="text-align: right;">-0.1345</td>
<td style="text-align: right;">0.819</td>
</tr>
<tr class="even">
<td style="text-align: left;">Test model<sup>b</sup></td>
<td style="text-align: right;">0.705</td>
<td style="text-align: right;">0.820</td>
<td style="text-align: right;">-0.1152</td>
<td style="text-align: right;">0.860</td>
</tr>
</tbody>
</table>
</div>
<div class="tablenotes">
<p><sup>a</sup>Model trained on the full dataset.</p>
<p><sup>b</sup>Model trained on the train dataset only.</p>
</div>
<div class="lookbox">
<p><strong>Exercise: Twin test</strong></p>
<p>Implement the twin test (described in section <a href="#sec_CondIndep" data-reference-type="ref" data-reference="sec_CondIndep">3.1.2</a>) for the model trained on the full dataset. Calculate the causal mean difference between male and female applicants using 2000 data points (1000 male and 1000 female applicants) i.e. the full dataset together with the ‘twin’ of the opposite gender.</p>
</div>
</section>
</section>
<section id="comparing-errors-3" class="level3" data-number="10.1.2">
<h3 data-number="10.1.2"><span class="header-section-number">E.1.2</span> Comparing Errors</h3>
<p>In order to calculate balanced error metrics with AIF360, we need to create an object of type <a href="https://aif360.readthedocs.io/en/latest/modules/generated/aif360.metrics.ClassificationMetric.html"><code>ClassificationMetric</code></a>. Returning to our example working with the German Credit Data, code listing E.3 calculates a series of balanced error metrics and populates the DataFrame <code>errors_df</code> with them. Note that <code>data_ds</code> and <code>model_ds</code> were created, and <code>privileged_groups</code> and <code>unprivileged_groups</code> were defined in earlier code listings.</p>
<div id="lst:AIF360metric2" class="listing">
<p>Listing E.3: Calculating balanced error metrics with AIF360</p>
<pre><code># Create a DataFrame to store results in
errors_df = pd.DataFrame(columns=[`female&#39;, `male&#39;,
                                  `difference&#39;, `ratio&#39;],
                         index=[`ERR&#39;, `FPR&#39;, `FNR&#39;, `FDR&#39;, `FOR&#39;])

# Create an instance of ClassificationMetric
clf_metric = ClassificationMetric(data_ds,
    model_ds,
    privileged_groups = privileged_groups,
    unprivileged_groups = unprivileged_groups)

# Compute the metrics with clf_metric and store them in errors_df
# Error rates for the unprivileged group
errors_df.at[`ERR&#39;, `female&#39;] = clf_metric.error_rate(privileged=False)
errors_df.at[`FPR&#39;, `female&#39;] =
    clf_metric.false_positive_rate(privileged=False)
errors_df.at[`FNR&#39;, `female&#39;] =
    clf_metric.false_negative_rate(privileged=False)
errors_df.at[`FDR&#39;, `female&#39;] =
    clf_metric.false_discovery_rate(privileged=False)
errors_df.at[`FOR&#39;, `female&#39;] =
    clf_metric.false_omission_rate(privileged=False)

# Error rates for the privileged group
errors_df.at[`ERR&#39;, `male&#39;] = clf_metric.error_rate(privileged=True)
errors_df.at[`FPR&#39;, `male&#39;] =
    clf_metric.false_positive_rate(privileged=True)
errors_df.at[`FNR&#39;, `male&#39;] =
    clf_metric.false_negative_rate(privileged=True)
errors_df.at[`FDR&#39;, `male&#39;] =
    clf_metric.false_discovery_rate(privileged=True)
errors_df.at[`FOR&#39;, `male&#39;] =
    clf_metric.false_omission_rate(privileged=True)

# Differences in error rates
errors_df.at[`ERR&#39;, `difference&#39;] = clf_metric.error_rate_difference()
errors_df.at[`FPR&#39;, `difference&#39;] =
    clf_metric.false_positive_rate_difference()
errors_df.at[`FNR&#39;, `difference&#39;] =
    clf_metric.false_negative_rate_difference()
errors_df.at[`FDR&#39;, `difference&#39;] =
    clf_metric.false_discovery_rate_difference()
errors_df.at[`FOR&#39;, `difference&#39;] =
    clf_metric.false_omission_rate_difference()

# Ratios of error rates
errors_df.at[`ERR&#39;, `ratio&#39;] = clf_metric.error_rate_ratio()
errors_df.at[`FPR&#39;, `ratio&#39;] = clf_metric.false_positive_rate_ratio()
errors_df.at[`FNR&#39;, `ratio&#39;] = clf_metric.false_negative_rate_ratio()
errors_df.at[`FDR&#39;, `ratio&#39;] = clf_metric.false_discovery_rate_ratio()
errors_df.at[`FOR&#39;, `ratio&#39;] = clf_metric.false_omission_rate_ratio()

display(errors_df)</code></pre>
</div>
<p>The DataFrame <code>error_df</code> is shown in Table <a href="#tbl:app_BalancedErrorMetrics" data-reference-type="ref" data-reference="tbl:app_BalancedErrorMetrics">E.2</a>.</p>
<div id="tbl:app_BalancedErrorMetrics">
<table>
<caption>Table E.2: Error metrics for the Statlog (German Credit Data) Data Set.</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Error metric<sup>a</sup></th>
<th style="text-align: right;">Female</th>
<th style="text-align: right;">Male</th>
<th style="text-align: right;">Difference</th>
<th style="text-align: right;">Ratio</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">ERR</td>
<td style="text-align: right;">0.246</td>
<td style="text-align: right;">0.180</td>
<td style="text-align: right;">0.066</td>
<td style="text-align: right;">1.37</td>
</tr>
<tr class="even">
<td style="text-align: left;">FPR</td>
<td style="text-align: right;">0.458</td>
<td style="text-align: right;">0.472</td>
<td style="text-align: right;">-0.014</td>
<td style="text-align: right;">0.97</td>
</tr>
<tr class="odd">
<td style="text-align: left;">FNR</td>
<td style="text-align: right;">0.108</td>
<td style="text-align: right;">0.078</td>
<td style="text-align: right;">0.030</td>
<td style="text-align: right;">1.39</td>
</tr>
<tr class="even">
<td style="text-align: left;">FDR</td>
<td style="text-align: right;">0.250</td>
<td style="text-align: right;">0.152</td>
<td style="text-align: right;">0.098</td>
<td style="text-align: right;">1.65</td>
</tr>
<tr class="odd">
<td style="text-align: left;">FOR</td>
<td style="text-align: right;">0.235</td>
<td style="text-align: right;">0.296</td>
<td style="text-align: right;">-0.061</td>
<td style="text-align: right;">0.79</td>
</tr>
</tbody>
</table>
</div>
<div class="tablenotes">
<p><sup>a</sup>We abbreviate error rate (ERR), false positive rate (FPR), false negative rate (FNR), false discovery rate (FDR) and false omission rate (FOR). See appendix <a href="#app_Metrics" data-reference-type="ref" data-reference="app_Metrics">B</a> for detailed descriptions of confusion matrix metrics.</p>
</div>
<p>This time we just look at the metrics for the model trained on the training set and calculated on the test set. We note that the overall error rate is 37% higher for female applicants. The false negative rate is 39% higher for female applicants, that is for female applicants we more often incorrectly predict that they represent bad credit risks when they are in fact good credit risks. We also note that the false discovery rate is 65% higher for female applicants which means that when we do predict women to be credit worthy they are more often not. The false omission rate is 21% lower for female applicants which means we are more often correct when we predict that they are not credit worthy. Our findings are not surprising given the difference in prevalence of credit worthy male and female applicants between our training and test sets shown in Table <a href="#tbl:app_IndependenceMetrics" data-reference-type="ref" data-reference="tbl:app_IndependenceMetrics">E.1</a>.</p>
<p>Recall that when we compared fairness metrics under the independence criterion, it appeared that our model was reducing the level of bias in the data. Note that comparing balanced error metrics (in addition to independence metrics) gives us a richer understanding of the behaviour of our model in relation to protected groups.</p>
</section>
</section>
<section id="app_AIF360_IF" class="level2" data-number="10.2">
<h2 data-number="10.2"><span class="header-section-number">E.2</span> Individual Fairness</h2>
<section id="consistency" class="level3" data-number="10.2.1">
<h3 data-number="10.2.1"><span class="header-section-number">E.2.1</span> Consistency</h3>
<div class="lookbox">
<p><strong>Exercise: Consistency score</strong></p>
<p>Use AIF360 to calculate consistency for the Statlog (German Credit) data and your model from chapter 3 which classified loan applicants as presenting good or bad credit risks. See section 7 of the jupyter notebook <a href="https://github.com/leenamurgai/mitigatingbiasml/blob/master/code/source/mbml-german.ipynb"><code>mbml_german.ipynb</code></a></p>
</div>
<p>The consistency metric in AIF360 uses Euclidean distance by default, but does allow the user to specify their own distance metric.</p>
</section>
</section>
<section id="app_AIF360_II" class="level2" data-number="10.3">
<h2 data-number="10.3"><span class="header-section-number">E.3</span> Utility as Fairness</h2>
<p>Now that we have some understanding of how inequality indices behave, we return to the German credit dataset. Code listing E.4 shows how to calculate the generalised entropy index with AIF360 for the benefit function in Table <a href="#tbl:app_GEI-bfs" data-reference-type="ref" data-reference="tbl:app_GEI-bfs">E.3</a> corresponding to equal false positive rates.</p>
<div id="lst:AIF360gei" class="listing">
<p>Listing E.4: Calculating the generalised entropy index with AIF360</p>
<pre><code># Import the necessary classes
from aif360.datasets import BinaryLabelDataset
from aif360.metrics import BinaryLabelDatasetMetric
from aif360.metrics import ClassificationMetric
from sklearn.ensemble import GradientBoostingClassifier

# Define the target, sensitive features and advantaged and disadvantaged groups
label_names = [`goodcredit&#39;]
protected_attribute_names = [`male&#39;]
privileged_groups = [{`male&#39;:1}]
unprivileged_groups = [{`male&#39;:0}]

# Define our model
clf = GradientBoostingClassifier(max_depth=7, max_features=`auto&#39;,
    min_samples_leaf=20)

# Fit the model to the training data
clf.fit(X_train, y_train)

# Create a DataFrame with the features and model predicted target
model_df = pd.concat([X_test, pd.Series(clf.predict(X_test),
    name=`goodcredit&#39;, index=X_test.index)], axis=1)
                                
# Create an instance of BinaryLabelDataset for the data
data_ds = BinaryLabelDataset(df=pd.concat([X_test, y_test],
    axis=1), label_names=label_names,
    protected_attribute_names=protected_attribute_names)

# Create an instance of BinaryLabelDatasetMetric for the data
data_metric = BinaryLabelDatasetMetric(data_ds,
    privileged_groups=privileged_groups,
    unprivileged_groups=unprivileged_groups)

# Create an instance of BinaryLabelDataset for the model
model_ds = BinaryLabelDataset(df=model_df,
    label_names=label_names,
    protected_attribute_names=protected_attribute_names)

# Create an instance of BinaryLabelDatasetMetric for the model
model_metric = BinaryLabelDatasetMetric(model_ds,
    privileged_groups=privileged_groups,
    unprivileged_groups=unprivileged_groups)

# Create an instance of ClassificationMetric
clf_metric = ClassificationMetric(data_ds, model_ds,
    privileged_groups=privileged_groups,
    unprivileged_groups=unprivileged_groups)

# Define the benefit function
FPR_bf = {`TN&#39;:1, `FP&#39;:0} # equal false positive rate benefit

# Calculate the generalised entropy index for our chosen benefit function
gei = clf_metric.generalized_entropy_index(benefit_function=FPR_bf)</code></pre>
</div>
<div class="lookbox">
<p><strong>Exercise: Benefits array sizes</strong></p>
<p>Calculate the size of the benefits arrays for each of the benefit functions corresponding to balanced error group fairness metrics in Table <a href="#tbl:clfBenefitFuncs" data-reference-type="ref" data-reference="tbl:clfBenefitFuncs">5.1</a>. Why are they not the the same size for all the benefit functions?</p>
</div>
<p>We compute the generalised entropy index and its between group component (with only 2 groups, male and females) for a range of benefit functions shown in Table <a href="#tbl:clfBenefitFuncs" data-reference-type="ref" data-reference="tbl:clfBenefitFuncs">5.1</a>. The results are displayed in Table <a href="#tbl:app_GEI-bfs" data-reference-type="ref" data-reference="tbl:app_GEI-bfs">E.3</a>.</p>
<div id="tbl:app_GEI-bfs">
<table>
<caption>Table E.3: Generalised entropy index and the between group component with <span class="math inline">\(\alpha=2\)</span> computed for the German credit dataset with two groups (male and female) for a variety of benefit functions.</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Type</th>
<th style="text-align: left;">Benefit function<sup>a</sup></th>
<th style="text-align: right;"><span class="math inline">\(I(\boldsymbol{b})\)</span></th>
<th style="text-align: right;"><span class="math inline">\(I_{\beta}(\boldsymbol{b})\)</span></th>
<th style="text-align: right;"><span class="math inline">\(I_{\beta}(\boldsymbol{b})/I(\boldsymbol{b})\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td rowspan="2" style="text-align: left;">Balanced outcomes</td>
<td style="text-align: left;">Equal ACR (data)</td>
<td style="text-align: right;">0.215</td>
<td style="text-align: right;">3.87 e-03</td>
<td style="text-align: right;">1.80 e-02</td>
</tr>
<tr class="even">
<td style="text-align: left;">Equal ACR (model)</td>
<td style="text-align: right;">0.117</td>
<td style="text-align: right;">2.60 e-03</td>
<td style="text-align: right;">2.21 e-02</td>
</tr>
<tr class="odd">
<td rowspan="5" style="text-align: left;">Balanced errors</td>
<td style="text-align: left;">Equal ERR</td>
<td style="text-align: right;">0.133</td>
<td style="text-align: right;">8.81 e-05</td>
<td style="text-align: right;">6.63 e-04</td>
</tr>
<tr class="even">
<td style="text-align: left;">Equal FPR</td>
<td style="text-align: right;">0.571</td>
<td style="text-align: right;">2.87 e-02</td>
<td style="text-align: right;">5.02 e-02</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Equal FNR</td>
<td style="text-align: right;">0.038</td>
<td style="text-align: right;">7.88 e-06</td>
<td style="text-align: right;">2.05 e-04</td>
</tr>
<tr class="even">
<td style="text-align: left;">Equal FDR</td>
<td style="text-align: right;">0.123</td>
<td style="text-align: right;">8.02 e-05</td>
<td style="text-align: right;">6.52 e-04</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Equal FOR</td>
<td style="text-align: right;">0.179</td>
<td style="text-align: right;">1.01 e-02</td>
<td style="text-align: right;">5.67 e-02</td>
</tr>
<tr class="even">
<td style="text-align: left;">Balanced benefits</td>
<td style="text-align: left;">Unified approach</td>
<td style="text-align: right;">0.080</td>
<td style="text-align: right;">4.13 e-06</td>
<td style="text-align: right;">5.15 e-05</td>
</tr>
</tbody>
</table>
</div>
<div class="tablenotes">
<p><sup>a</sup>See Table <a href="#tbl:clfBenefitFuncs" data-reference-type="ref" data-reference="tbl:clfBenefitFuncs">5.1</a> for benefit function definitions.</p>
</div>
<p>Given the variability of the value of the index and its between group component, we also look at the between group component as a proportion of the index. We note that in all cases, the between group component is a relatively small part of the overall unfairness (5% or less) - this makes sense given that there are only two groups (males and females) and one thousand data points.</p>
</section>
</section>
<section id="bibliography" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body" role="doc-bibliography">
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">D. Ingold and S. Soper, <span>“Amazon doesn’t consider the race of its customers. Should it?”</span> <em>Bloomberg</em>, 2016.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">J. Rawls, <em>Justice as fairness: A restatement</em>. Cambridge, Mass.: Harvard University Press, 2001.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">P. L. B. Johnson, <span>“Speech to a joint session of congress on march 15, 1965,”</span> <em>Public Papers of the Presidents of the United States</em>, vol. I, entry 107, pp. 281–287, 1965.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">S. Barocas and A. D. Selbst, <span>“Big data’s disparate impact,”</span> <em>Calif Law Rev.</em>, vol. 104, pp. 671–732, 2016.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline"><span>“<span class="nocase">Ricci v. DeStefano, 557 U.S. 557</span>.”</span> 2009.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline"><span>“<span class="nocase">Griggs v. Duke Power Co., 401 U.S. 424</span>.”</span> 1971.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline"><span>“<span class="nocase">Wards Cove Packing Co. v. Atonio, 490 U.S. 642</span>.”</span> 1989.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[8] </div><div class="csl-right-inline"><span>“<span class="nocase">Grutter v. Bollinger, 539 U.S. 306</span>.”</span> 2003.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[9] </div><div class="csl-right-inline"><span>“<span>General Data Protection Regulation (GDPR): (EU) 2016/679 Recital 71</span>.”</span> 2016.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[10] </div><div class="csl-right-inline"><span>“<span class="nocase">Europe fit for the Digital Age: Commission proposes new rules and actions for excellence and trust in Artificial Intelligence</span>.”</span> 2021.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[11] </div><div class="csl-right-inline">P. J. Bickel, E. A. Hammel, and J. W. O’Connell, <span>“Sex bias in graduate admissions: Data from berkeley,”</span> <em>Science</em>, vol. 187, Issue 4175, pp. 398–404, 1975.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[12] </div><div class="csl-right-inline">E. Simpson, <span>“The interpretation of interaction in contingency tables,”</span> <em>Journal of the Royal Statistical Society</em>, vol. Series B, 13, pp. 238–241, 1951.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[13] </div><div class="csl-right-inline">J. Angwin, J. Larson, S. Mattu, and L. Kirchner, <span>“Machine bias,”</span> <em>ProPublica</em>, 2016.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[14] </div><div class="csl-right-inline">X. Wu and X. Zhang, <span>“Automated inference on criminality using face images.”</span> 2017.Available: <a href="https://arxiv.org/abs/1611.04135">https://arxiv.org/abs/1611.04135</a></div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[15] </div><div class="csl-right-inline">Y. Wang and M. Kosinski, <span>“Deep neural networks are more accurate than humans at detecting sexual orientation from facial images,”</span> <em>Journal of Personality and Social Psychology</em>, 2018.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[16] </div><div class="csl-right-inline">C. Wang, Q. Zhang, W. Liu, Y. Liu, and L. Miao, <span>“Facial feature discovery for ethnicity recognition,”</span> <em>Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</em>, 2018.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[17] </div><div class="csl-right-inline">C. Cadwalladr, <em>Facebook’s role in <span>Brexit</span> - and the threat to democracy</em>. TED, 2019.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[18] </div><div class="csl-right-inline">O. Varol, E. Ferrara, C. A. Davis, F. Menczer, and A. Flammini, <span>“Online human-bot interactions: Detection, estimation, and characterization.”</span> 2017.Available: <a href="https://arxiv.org/abs/1703.03107">https://arxiv.org/abs/1703.03107</a></div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[19] </div><div class="csl-right-inline">B. Allyn, <span>“Researchers: Nearly half of accounts tweeting about coronavirus are likely bots,”</span> <em>NPR</em>, May 2020.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[20] </div><div class="csl-right-inline">J. Nicas, <span>“Does facebook really know how many fake accounts it has?”</span> <em>The New York Times</em>, 2019.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[21] </div><div class="csl-right-inline">E. O’Toole, <span>“A dictionary entry citing <span>‘rabid feminist’</span> doesn’t just reflect prejudice, it reinforces it,”</span> <em>The Guardian</em>, 2016.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[22] </div><div class="csl-right-inline">D. Shariatmadari, <span>“Eight words that reveal the sexism at the heart of the english language,”</span> <em>The Guardian</em>, 2016.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[23] </div><div class="csl-right-inline">T. Bolukbasi, K.-W. Chang, J. Zou, V. Saligrama, and A. Kalai, <span>“Man is to computer programmer as woman is to homemaker? Debiasing word embeddings.”</span> 2016.Available: <a href="https://arxiv.org/abs/1607.06520">https://arxiv.org/abs/1607.06520</a></div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[24] </div><div class="csl-right-inline">A. Caliskan, J. J. Bryson, and A. Narayanan, <span>“Semantics derived automatically from language corpora contain human-like biases,”</span> <em>Science</em>, vol. 356, pp. 183–186, 2017.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[25] </div><div class="csl-right-inline">J. Buolamwini and T. Gerbru, <em>Gender shades: Intersectional accuracy disparities in commercial gender classification</em>, vol. 81. Proceedings of Machine Learning Research, 2018, pp. 1–15.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[26] </div><div class="csl-right-inline">V. U. Prabhu and A. Birhane, <span>“Large image datasets: A pyrrhic win for computer vision?”</span> 2020.Available: <a href="https://arxiv.org/abs/2006.16923">https://arxiv.org/abs/2006.16923</a></div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[27] </div><div class="csl-right-inline">L. Sweeney, <span>“Discrimination in online ad delivery,”</span> <em>SSRN</em>, 2013.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[28] </div><div class="csl-right-inline">M. Kay, C. Matuszek, and S. A. Munson, <span>“Unequal representation and gender stereotypes in image search results for occupations,”</span> <em>ACM</em>, 2015.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[29] </div><div class="csl-right-inline"><span>“Rates of drug use and sales, by race; rates of drug related criminal justice measures, by race.”</span> The Hamilton Project, 2015.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[30] </div><div class="csl-right-inline">J. Larson, S. Mattu, L. Kirchner, and J. Angwin, <span>“How we analyzed the COMPAS recidivism algorithm,”</span> <em>ProPublica</em>, 2016.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[31] </div><div class="csl-right-inline">Northpointe, <em>Practitioners guide to COMPAS core</em>. 2015.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[32] </div><div class="csl-right-inline">J. Larson, <span>“ProPublica analysis of data from broward county, fla.”</span> ProPublica, 2016.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[33] </div><div class="csl-right-inline">C. Jarrett, <span>“How prison changes people,”</span> <em>BBC Future</em>, May 2018.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[34] </div><div class="csl-right-inline">D. S. Nagin, <span>“Deterrence in the twenty-first century: A review of the evidence,”</span> <em>Crime and Justice</em>, vol. 42, May 2018.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[35] </div><div class="csl-right-inline">M. Mauer, <span>“Long-term sentences: Time to reconsider the scale of punishment,”</span> <em>The Sentencing Project</em>, 2018.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[36] </div><div class="csl-right-inline">P. Wagner and W. Sawyer, <span>“States of incarceration: The global context,”</span> <em>Prison Policy Initiative</em>, 2018.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[37] </div><div class="csl-right-inline">B. Lufkin, <span>“The myth behind long prison sentences,”</span> <em>BBC Future</em>, May 2018.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[38] </div><div class="csl-right-inline">A. D. Selbst, D. Boyd, S. A. Friedler, S. Venkatasubramanian, and J. Vertesi, <span>“Fairness and abstraction in sociotechnical systems,”</span> in <em>Proceedings of the conference on fairness, accountability, and transparency</em>, 2019, pp. 59–68. doi: <a href="https://doi.org/10.1145/3287560.3287598">10.1145/3287560.3287598</a>.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[39] </div><div class="csl-right-inline">Z. Obermeyer, B. Powers, C. Vogeli, and S. Mullainathan, <span>“Dissecting racial bias in an algorithm used to manage the health of populations,”</span> <em>Science</em>, vol. 366, pp. 447–453, Oct. 2019, doi: <a href="https://doi.org/10.1126/science.aax2342">10.1126/science.aax2342</a>.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[40] </div><div class="csl-right-inline">B. d’Alessandro, C. O’Neil, and T. LaGatta, <span>“Conscientious classification: A data scientist’s guide to discrimination-aware classification,”</span> <em>Big Data</em>, vol. 5, no. 2, pp. 120–134, 2017, doi: <a href="https://doi.org/10.1089/big.2016.0048">10.1089/big.2016.0048</a>.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[41] </div><div class="csl-right-inline">H. Suresh and J. Guttag, <span>“A framework for understanding sources of harm throughout the machine learning life cycle,”</span> 2021.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[42] </div><div class="csl-right-inline">T. Gebru <em>et al.</em>, <span>“Datasheets for datasets.”</span> 2020.Available: <a href="https://arxiv.org/abs/1803.09010">https://arxiv.org/abs/1803.09010</a></div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[43] </div><div class="csl-right-inline">M. Mitchell <em>et al.</em>, <span>“Model cards for model reporting,”</span> <em>Proceedings of the Conference on Fairness, Accountability, and Transparency</em>, 2019, doi: <a href="https://doi.org/10.1145/3287560.3287596">10.1145/3287560.3287596</a>.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[44] </div><div class="csl-right-inline">K. L. Calderone, <span>“The influence of gender on the frequency of pain and sedative medication administered to postoperative patients,”</span> <em>Sex Roles</em>, vol. 23, pp. 713–725, 1990, doi: <a href="https://doi.org/10.1007/BF00289259">https://doi.org/10.1007/BF00289259</a>.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[45] </div><div class="csl-right-inline">E. H. C. MD <em>et al.</em>, <span>“Gender disparity in analgesic treatment of emergency department patients with acute abdominal pain,”</span> <em>Academic Emergency Medicine</em>, vol. 15, pp. 414–418, May 2008, doi: <a href="https://doi.org/10.1111/j.1553-2712.2008.00100.x">https://doi.org/10.1111/j.1553-2712.2008.00100.x</a>.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[46] </div><div class="csl-right-inline">D. E. Hoffmann and A. J. Tarzian, <span>“The girl who cried pain: A bias against women in the treatment of pain,”</span> <em>SSRN</em>, 2001, doi: <a href="http://dx.doi.org/10.2139/ssrn.383803">http://dx.doi.org/10.2139/ssrn.383803</a>.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[47] </div><div class="csl-right-inline">K. M. Hoffman, S. Trawalter, J. R. Axt, and M. N. Oliver, <span>“Racial bias in pain assessment and treatment recommendations, and false beliefs about biological differences between blacks and whites,”</span> <em>Proceedings of the National Academy of Sciences</em>, vol. 113, no. 16, pp. 4296–4301, 2016, doi: <a href="https://doi.org/10.1073/pnas.1516047113">10.1073/pnas.1516047113</a>.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[48] </div><div class="csl-right-inline"><span>“The voice of 12,000 patients: Experiences and expectations of rare disease patients on diagnosis and care in europe.”</span> 2009.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[49] </div><div class="csl-right-inline">M. B. Zafar, I. Valera, M. Gomez Rodriguez, and K. P. Gummadi, <span>“Fairness beyond disparate treatment &amp; disparate impact,”</span> <em>Proceedings of the 26th International Conference on World Wide Web</em>, 2017, doi: <a href="https://doi.org/10.1145/3038912.3052660">10.1145/3038912.3052660</a>.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[50] </div><div class="csl-right-inline">R. Binns, <span>“On the apparent conflict between individual and group fairness.”</span> arXiv, 2019. doi: <a href="https://doi.org/10.48550/ARXIV.1912.06883">10.48550/ARXIV.1912.06883</a>.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[51] </div><div class="csl-right-inline">K. Fukuchi, J. Sakuma, and T. Kamishima, <span>“Prediction with model-based neutrality,”</span> <em>IEICE TRANS. INF. &amp; SYS.</em>, vol. E98–D, no. 8, 2015, doi: <a href="https://doi.org/10.1587/transinf.2014EDP7367">10.1587/transinf.2014EDP7367</a>.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[52] </div><div class="csl-right-inline">I. Zliobaite, <span>“On the relation between accuracy and fairness in binary classification.”</span> 2015.Available: <a href="https://arxiv.org/abs/1505.05723">https://arxiv.org/abs/1505.05723</a></div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[53] </div><div class="csl-right-inline">U. S. E. E. O. Commission, <span>“Questions and answers to clarify and provide a common interpretation of the uniform guidelines on employee selection procedures,”</span> <em>Federal Register</em>, vol. 44, no. 43, 1979.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[54] </div><div class="csl-right-inline">D. Pedreschi, S. Ruggieri, and F. Turini, <span>“Discrimination-aware data mining,”</span> in <em>Proceedings of the ACM SIGKDD international conference on knowledge discovery and data mining</em>, 2008, pp. 560–568. doi: <a href="https://doi.org/10.1145/1401890.1401959">10.1145/1401890.1401959</a>.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[55] </div><div class="csl-right-inline">T. Calders, A. Karim, F. Kamiran, W. Ali, and X. Zhang, <span>“Controlling attribute effect in linear regression,”</span> 2013. doi: <a href="https://doi.org/10.1109/ICDM.2013.114">10.1109/ICDM.2013.114</a>.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[56] </div><div class="csl-right-inline">M. Hardt, E. Price, and N. Srebro, <span>“Equality of opportunity in supervised learning.”</span> 2016.Available: <a href="https://arxiv.org/abs/1610.02413">https://arxiv.org/abs/1610.02413</a></div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[57] </div><div class="csl-right-inline">L. T. Liu, M. Simchowitz, and M. Hardt, <span>“The implicit fairness criterion of unconstrained learning.”</span> 2019.Available: <a href="https://arxiv.org/abs/1808.10013">https://arxiv.org/abs/1808.10013</a></div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[58] </div><div class="csl-right-inline">J. Dressel and H. Farid, <span>“The accuracy, fairness, and limits of predicting recidivism,”</span> <em>Science Advances</em>, vol. 4, no. 1, p. eaao5580, 2018, doi: <a href="https://doi.org/10.1126/sciadv.aao5580">10.1126/sciadv.aao5580</a>.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[59] </div><div class="csl-right-inline">C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel, <span>“Fairness through awareness.”</span> 2011.Available: <a href="https://arxiv.org/abs/1104.3913">https://arxiv.org/abs/1104.3913</a></div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[60] </div><div class="csl-right-inline">A. V. Dicey, <span>“The law of the constitution.”</span> 1978.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[61] </div><div class="csl-right-inline">R. Dworkin, <span>“No right answer.”</span> 1978.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[62] </div><div class="csl-right-inline">R. Zemel, Y. Wu, K. Swersky, T. Pitassi, and C. Dwork, <span>“Learning fair representations,”</span> in <em>Proceedings of the 30th international conference on machine learning</em>, 2013, vol. 28, pp. 325–333.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[63] </div><div class="csl-right-inline">T. Speicher <em>et al.</em>, <span>“A unified approach to quantifying algorithmic unfairness,”</span> <em>Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em>, 2018, doi: <a href="https://doi.org/10.1145/3219819.3220046">10.1145/3219819.3220046</a>.</div>
</div>
<div class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[64] </div><div class="csl-right-inline">A. F. Shorrocks, <span>“The class of additively decomposable inequality measures,”</span> <em>Econometrica: Journal of the Econometric Society</em>, vol. 48, no. 613–625, 1980.</div>
</div>
</div>
</section>
</article>
<script>
var coll = document.getElementsByClassName("collapsible");
var i;
for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var content = this.nextElementSibling;
    if (content.style.display === "block") {
      content.style.display = "none";
    } else {
      content.style.display = "block";
    }
  });
}
</script>
</body>
</html>
